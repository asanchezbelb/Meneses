{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75d80f3d",
   "metadata": {
    "papermill": {
     "duration": 0.026388,
     "end_time": "2024-06-10T15:36:47.936731",
     "exception": false,
     "start_time": "2024-06-10T15:36:47.910343",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"toc\"></a>\n",
    "# **Twitter Network Analysis: Mentions & Semantics**\n",
    "\n",
    "Table of Contents:\n",
    "* [About the Project](#1)\n",
    "* [About the Data](#2)\n",
    "* [Set Up](#3)\n",
    "* [Twitter Mentions Network](#4)\n",
    "    - [Viewing the Data](#4a)\n",
    "    - [Identifying Unique Users](#4b)\n",
    "    - [Filtering Out Users](#4c)\n",
    "    - [Setting Up Helper Functions](#4d)\n",
    "    - [Creating the Graph](#4e)\n",
    "    - [Creating Subgraphs](#4f)\n",
    "    - [Saving and Plotting the Graphs](#4g)\n",
    "* [Semantic Network](#5)\n",
    "    - [Setting Up Helper Functions](#5a)\n",
    "    - [Identifying Unique Words](#5b)\n",
    "    - [Creating the Graph](#5c)\n",
    "    - [Creating the Subgraphs](#5d)\n",
    "    - [Saving and Plotting the Graphs](#5e)\n",
    "* [Conclusion/Analysis](#6)\n",
    "    - [Twitter Mentions Network](#6a)\n",
    "    - [Semantic Network](#6b)\n",
    "* [References](#r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3550ac1",
   "metadata": {
    "papermill": {
     "duration": 0.025486,
     "end_time": "2024-06-10T15:36:47.988295",
     "exception": false,
     "start_time": "2024-06-10T15:36:47.962809",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "<a id=\"1\"></a>\n",
    "# **About the Project**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e17363a",
   "metadata": {
    "papermill": {
     "duration": 0.026318,
     "end_time": "2024-06-10T15:36:48.042485",
     "exception": false,
     "start_time": "2024-06-10T15:36:48.016167",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This project was completed as the final project for CU Boulder's DTSA 5800 Network Analysis for Marketing Analytics course in the MS-DS program. The overarching goal of the project is to leverage network graphs to gain insight into how consumers interact with and talk about three competing brands: Nike, Adidas, and Lululemon. More specifically, by constructing network graphs based upon Twitter tweets, this project will first analyze the connections created from Twitter mentions. These mentions, e.g., @nike, @adidas, @lululemon, are typically from consumers of the brands interacting with either the brands themselves or adjacent entities. These mentions graphs will assist in identifying the users that are most centrally related to the brand. Next, this project will also create semantic network graphs, which will reveal what words in the tweets are most commonly associated with eachother. This may provide further insight into what consumers are saying about each brand. Overall, this project provided a great opportunity to learn about the development of network graphs, namely by using [NetworkX](https://networkx.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f221990",
   "metadata": {
    "papermill": {
     "duration": 0.025585,
     "end_time": "2024-06-10T15:36:48.093949",
     "exception": false,
     "start_time": "2024-06-10T15:36:48.068364",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This project was originally completed in Google Colab. In order to reduce the runtime of this notebook, I have opted to comment out several code blocks, such as the ones that plot and save the network graphs. Instead, I have directly inserted several examples of the generated images. If you would like to see my full work, all project files, including the original Colab notebook, the dataset used (in .jsonl and .jsonl.gz formats), and all created graphs, please follow this link to [Google Drive](https://drive.google.com/drive/folders/1rrRiAegl6A-P6BVP3oRLgpZQmyP8J-jR?usp=sharing)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7436a52f",
   "metadata": {
    "papermill": {
     "duration": 0.025561,
     "end_time": "2024-06-10T15:36:48.145514",
     "exception": false,
     "start_time": "2024-06-10T15:36:48.119953",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<a id=\"toc\"></a>\n",
    "\n",
    "<a href=\"#toc\" style=\"background-color:blue; color:white; padding: 7px 10px; text-decoration: none; border-radius: 50px;\">Back to top</a><a id=\"toc\"></a>\n",
    "\n",
    "<a id=\"toc\"></a>\n",
    "\n",
    "<a href=\"#3\" style=\"background-color:blue; color:white; padding: 7px 10px; text-decoration: none; border-radius: 50px;\">Next Section</a><a id=\"toc\"></a>\n",
    "<a id=\"2\"></a>\n",
    "\n",
    "---\n",
    "# **About the Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f142b7c8",
   "metadata": {
    "papermill": {
     "duration": 0.02628,
     "end_time": "2024-06-10T15:36:48.197643",
     "exception": false,
     "start_time": "2024-06-10T15:36:48.171363",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The data used in this project consists of 175,077 tweets that mention one or more of the following brands: @nike, @adidas, @lululemon. The data was originally retrieved from Twitter's [Standard search API](https://developer.x.com/en/docs/twitter-api/v1/tweets/search/api-reference/get-search-tweets). All tweets were sent from the US and are in English. The data comes in .jsonl format (JSON lines). I have uploaded this data to Kaggle specifically for this notebook, but if you would like to access the .json file, please follow the Google Drive link provided above. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6543f7e",
   "metadata": {
    "papermill": {
     "duration": 0.027001,
     "end_time": "2024-06-10T15:36:48.250695",
     "exception": false,
     "start_time": "2024-06-10T15:36:48.223694",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"toc\"></a>\n",
    "\n",
    "<a href=\"#toc\" style=\"background-color:blue; color:white; padding: 7px 10px; text-decoration: none; border-radius: 50px;\">Back to top</a><a id=\"toc\"></a>\n",
    "\n",
    "<a id=\"toc\"></a>\n",
    "\n",
    "<a href=\"#4\" style=\"background-color:blue; color:white; padding: 7px 10px; text-decoration: none; border-radius: 50px;\">Next Section</a><a id=\"toc\"></a>\n",
    "\n",
    "<a id=\"3\"></a>\n",
    "\n",
    "---\n",
    "# **Set Up**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7acc552c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T15:36:48.307635Z",
     "iopub.status.busy": "2024-06-10T15:36:48.307169Z",
     "iopub.status.idle": "2024-06-10T15:36:48.315723Z",
     "shell.execute_reply": "2024-06-10T15:36:48.314083Z"
    },
    "papermill": {
     "duration": 0.041173,
     "end_time": "2024-06-10T15:36:48.318893",
     "exception": false,
     "start_time": "2024-06-10T15:36:48.277720",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mounting to Google Drive and setting filepaths\n",
    "# ONLY NECESSARY FOR THE ORIGINAL PROJECT IN COLAB\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# WORKING_DIR = \"/content/drive/MyDrive/MSDS_marketing_text_analytics/master_files/3_network_analysis\"\n",
    "# FILE_PATH = '%s/nikelululemonadidas_tweets.jsonl' % WORKING_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "013caf70",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-06-10T15:36:48.374670Z",
     "iopub.status.busy": "2024-06-10T15:36:48.374191Z",
     "iopub.status.idle": "2024-06-10T15:36:53.701639Z",
     "shell.execute_reply": "2024-06-10T15:36:53.700118Z"
    },
    "papermill": {
     "duration": 5.360603,
     "end_time": "2024-06-10T15:36:53.706277",
     "exception": false,
     "start_time": "2024-06-10T15:36:48.345674",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-] Importing packages...\n",
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /usr/share/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading necessary packages\n",
    "print('[-] Importing packages...')\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "from time import sleep\n",
    "import re\n",
    "import string\n",
    "import itertools\n",
    "import datetime\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3da2b167",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-06-10T15:36:53.781576Z",
     "iopub.status.busy": "2024-06-10T15:36:53.780545Z",
     "iopub.status.idle": "2024-06-10T15:36:55.277871Z",
     "shell.execute_reply": "2024-06-10T15:36:55.276134Z"
    },
    "papermill": {
     "duration": 1.539596,
     "end_time": "2024-06-10T15:36:55.281116",
     "exception": false,
     "start_time": "2024-06-10T15:36:53.741520",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /usr/share/nltk_data/corpora/wordnet.zip\r\n",
      "   creating: /usr/share/nltk_data/corpora/wordnet/\r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/lexnames  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/data.verb  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/index.adv  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/adv.exc  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/index.verb  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/cntlist.rev  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/data.adj  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/index.adj  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/LICENSE  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/citation.bib  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/noun.exc  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/verb.exc  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/README  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/index.sense  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/data.noun  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/data.adv  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/index.noun  \r\n",
      "  inflating: /usr/share/nltk_data/corpora/wordnet/adj.exc  \r\n"
     ]
    }
   ],
   "source": [
    "# Unzipping 'wordnet', as Kaggle isn't able to access it when it is compressed\n",
    "!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ee254f9",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-06-10T15:36:55.339943Z",
     "iopub.status.busy": "2024-06-10T15:36:55.339380Z",
     "iopub.status.idle": "2024-06-10T15:37:13.483623Z",
     "shell.execute_reply": "2024-06-10T15:37:13.482035Z"
    },
    "papermill": {
     "duration": 18.178075,
     "end_time": "2024-06-10T15:37:13.487175",
     "exception": false,
     "start_time": "2024-06-10T15:36:55.309100",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-] Importing packages...\n",
      "Collecting pyvis\r\n",
      "  Downloading pyvis-0.3.2-py3-none-any.whl.metadata (1.7 kB)\r\n",
      "Requirement already satisfied: ipython>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from pyvis) (8.20.0)\r\n",
      "Requirement already satisfied: jinja2>=2.9.6 in /opt/conda/lib/python3.10/site-packages (from pyvis) (3.1.2)\r\n",
      "Collecting jsonpickle>=1.4.1 (from pyvis)\r\n",
      "  Downloading jsonpickle-3.2.1-py3-none-any.whl.metadata (7.2 kB)\r\n",
      "Requirement already satisfied: networkx>=1.11 in /opt/conda/lib/python3.10/site-packages (from pyvis) (3.2.1)\r\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipython>=5.3.0->pyvis) (5.1.1)\r\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython>=5.3.0->pyvis) (0.19.1)\r\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.10/site-packages (from ipython>=5.3.0->pyvis) (0.1.6)\r\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.10/site-packages (from ipython>=5.3.0->pyvis) (3.0.42)\r\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from ipython>=5.3.0->pyvis) (2.17.2)\r\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython>=5.3.0->pyvis) (0.6.2)\r\n",
      "Requirement already satisfied: traitlets>=5 in /opt/conda/lib/python3.10/site-packages (from ipython>=5.3.0->pyvis) (5.9.0)\r\n",
      "Requirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from ipython>=5.3.0->pyvis) (1.2.0)\r\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython>=5.3.0->pyvis) (4.8.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2>=2.9.6->pyvis) (2.1.3)\r\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython>=5.3.0->pyvis) (0.8.3)\r\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->ipython>=5.3.0->pyvis) (0.7.0)\r\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=5.3.0->pyvis) (0.2.13)\r\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=5.3.0->pyvis) (2.0.1)\r\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=5.3.0->pyvis) (2.4.1)\r\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=5.3.0->pyvis) (0.2.2)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from asttokens>=2.1.0->stack-data->ipython>=5.3.0->pyvis) (1.16.0)\r\n",
      "Downloading pyvis-0.3.2-py3-none-any.whl (756 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading jsonpickle-3.2.1-py3-none-any.whl (41 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: jsonpickle, pyvis\r\n",
      "Successfully installed jsonpickle-3.2.1 pyvis-0.3.2\r\n"
     ]
    }
   ],
   "source": [
    "# Loading necessary packages from pyvis\n",
    "print('[-] Importing packages...')\n",
    "try:\n",
    "  import pyvis\n",
    "  from pyvis.network import Network\n",
    "except:\n",
    "  !pip install pyvis\n",
    "  import pyvis\n",
    "  from pyvis.network import Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68f1e3b",
   "metadata": {
    "papermill": {
     "duration": 0.032724,
     "end_time": "2024-06-10T15:37:13.613397",
     "exception": false,
     "start_time": "2024-06-10T15:37:13.580673",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"toc\"></a>\n",
    "\n",
    "<a href=\"#toc\" style=\"background-color:blue; color:white; padding: 7px 10px; text-decoration: none; border-radius: 50px;\">Back to top</a><a id=\"toc\"></a>\n",
    "\n",
    "<a id=\"toc\"></a>\n",
    "\n",
    "<a href=\"#5\" style=\"background-color:blue; color:white; padding: 7px 10px; text-decoration: none; border-radius: 50px;\">Next Section</a><a id=\"toc\"></a>\n",
    "\n",
    "<a id=\"4\"></a>\n",
    "\n",
    "---\n",
    "# **Twitter Mentions Network**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417ab455",
   "metadata": {
    "papermill": {
     "duration": 0.030129,
     "end_time": "2024-06-10T15:37:13.673074",
     "exception": false,
     "start_time": "2024-06-10T15:37:13.642945",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"4a\"></a>\n",
    "## **Viewing the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c502836",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T15:37:13.740908Z",
     "iopub.status.busy": "2024-06-10T15:37:13.739757Z",
     "iopub.status.idle": "2024-06-10T15:37:13.752032Z",
     "shell.execute_reply": "2024-06-10T15:37:13.749232Z"
    },
    "papermill": {
     "duration": 0.053606,
     "end_time": "2024-06-10T15:37:13.756108",
     "exception": false,
     "start_time": "2024-06-10T15:37:13.702502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Opening 'nikelululemonadidas_tweets.jsonl' file and saving as variable 'json_file'\n",
    "# jsonl = json line, each line of the text file corresponds to a json entry\n",
    "json_file = open('/kaggle/input/nikelululemonadidas-tweets-jsonl/nikelululemonadidas_tweets.jsonl', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a22f9034",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T15:37:13.831168Z",
     "iopub.status.busy": "2024-06-10T15:37:13.830715Z",
     "iopub.status.idle": "2024-06-10T15:37:27.488028Z",
     "shell.execute_reply": "2024-06-10T15:37:27.486353Z"
    },
    "papermill": {
     "duration": 13.69648,
     "end_time": "2024-06-10T15:37:27.491343",
     "exception": false,
     "start_time": "2024-06-10T15:37:13.794863",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Via Nike⁠ SNKRS: can I get a W ⁦@Nike⁩ ⁦@nikebasketball⁩ #snkrs  https://t.co/lQ6zKN1Oq6\n",
      "@Kaya_Alexander5 @nikestore @Nike @SneakerAdmirals Jelly!   Awesome pair https://t.co/L2Kefg2fUP\n",
      "RT @WALionsFB: Game Recap from #MondayNightFootball‼️🏈 @jumpman23 @usnikefootball @wacad @larryblustein @RiddellSports @Nike @FlaHSFootball…\n",
      "@somaliboxer @AlisSistersClub @nikelondon @Nike Fists up ❤️\n",
      "RT @NiaOnAir: Identity is not as simple as black and white. ☯ On episode 5 of @nike's #ComeThru, I get into it with @lisa_asano, @whoisumi,…\n",
      "RT @WALionsFB: Game Recap from #MondayNightFootball‼️🏈 @jumpman23 @usnikefootball @wacad @larryblustein @RiddellSports @Nike @FlaHSFootball…\n",
      "Liquid3_6 Sneaker Of The Day @nike \n",
      "\n",
      "Saquon Barkley x Nike Air Trainer 3\n",
      "Color: Pearl White/Neptune Green-Sail\n",
      "Style Code: DA5403-200\n",
      "Release Date: October 8, 2021\n",
      "Price: $140\n",
      "#sneakerhead #style #fashion #sneakeroftheday #sneakers #footwear #brandmarketing #Nike #Liquid3_6🛸 https://t.co/kwIROtsn1k\n"
     ]
    }
   ],
   "source": [
    "# Iterating through 'json_file' to see what the data looks like\n",
    "\n",
    "# Iterating through file, if less than 50 and involves 'nike', print out the tweet\n",
    "for i, atweet in enumerate(json_file):\n",
    "    if i < 50:\n",
    "      tweetjson = json.loads(atweet)\n",
    "      text = tweetjson['full_text']\n",
    "      if \"nike\" in text:\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94651075",
   "metadata": {
    "papermill": {
     "duration": 0.029121,
     "end_time": "2024-06-10T15:37:27.550279",
     "exception": false,
     "start_time": "2024-06-10T15:37:27.521158",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "To start, we can look at some examples of the tweets in the dataset! Above, we can see one such example: \"Via Nike SNKRS: can I get a W @Nike @nikebasketball #snkrs https://t.co/lQ6zKN1Oq6\". Immediately, we can identify the mentions, e.g. @Nike and @nikebasketball. As we move through this project, these tweets will ultimately need to be cleaned up. For example, the https: link will need to be removed, as it does not provide us with any useful information. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40403bf7",
   "metadata": {
    "papermill": {
     "duration": 0.028539,
     "end_time": "2024-06-10T15:37:27.607718",
     "exception": false,
     "start_time": "2024-06-10T15:37:27.579179",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"4b\"></a>\n",
    "## **Identifying Unique Users**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4b8283e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T15:37:27.668793Z",
     "iopub.status.busy": "2024-06-10T15:37:27.668387Z",
     "iopub.status.idle": "2024-06-10T15:37:27.674915Z",
     "shell.execute_reply": "2024-06-10T15:37:27.673464Z"
    },
    "papermill": {
     "duration": 0.040219,
     "end_time": "2024-06-10T15:37:27.677933",
     "exception": false,
     "start_time": "2024-06-10T15:37:27.637714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reopening 'nikelululemonadidas_tweets.jsonl' file in order to iterate through it again\n",
    "json_file = open('/kaggle/input/nikelululemonadidas-tweets-jsonl/nikelululemonadidas_tweets.jsonl', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7876ebb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T15:37:27.739183Z",
     "iopub.status.busy": "2024-06-10T15:37:27.738728Z",
     "iopub.status.idle": "2024-06-10T15:37:52.165991Z",
     "shell.execute_reply": "2024-06-10T15:37:52.164688Z"
    },
    "papermill": {
     "duration": 24.461682,
     "end_time": "2024-06-10T15:37:52.169052",
     "exception": false,
     "start_time": "2024-06-10T15:37:27.707370",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tweets iterated\n",
      "10000 tweets iterated\n",
      "20000 tweets iterated\n",
      "30000 tweets iterated\n",
      "40000 tweets iterated\n",
      "50000 tweets iterated\n",
      "60000 tweets iterated\n",
      "70000 tweets iterated\n",
      "80000 tweets iterated\n",
      "90000 tweets iterated\n",
      "100000 tweets iterated\n",
      "110000 tweets iterated\n",
      "120000 tweets iterated\n",
      "130000 tweets iterated\n",
      "140000 tweets iterated\n",
      "150000 tweets iterated\n",
      "160000 tweets iterated\n",
      "170000 tweets iterated\n"
     ]
    }
   ],
   "source": [
    "# Identifying unique users in the mention network\n",
    "\n",
    "# Creating a dictionary of all unique users\n",
    "unique_users = {}\n",
    "\n",
    "# Iterating through 'json_file'\n",
    "for i, atweet in enumerate(json_file):\n",
    "    # Creating counter to show iteration progress\n",
    "    if i % 10000 == 0:\n",
    "      print(\"%s tweets iterated\" % i)\n",
    "    # Loading in file as string with 'json.loads'\n",
    "    tweet_json = json.loads(atweet)\n",
    "    # Parsing out user, id, and follower count from tweets\n",
    "    user_who_tweeted = tweet_json['user']['screen_name']\n",
    "    id_who_tweeted = tweet_json['user']['id']\n",
    "    follower_count = tweet_json['user']['followers_count']\n",
    "    # Counting number of tweets by a given user\n",
    "    if id_who_tweeted in unique_users:\n",
    "      unique_users[id_who_tweeted]['tweet_count'] += 1\n",
    "      #if unique_users[id_who_tweeted]['followers_count'] == 0:\n",
    "      unique_users[id_who_tweeted]['followers_count'] = follower_count\n",
    "      #unique_users[id_who_tweeted]['screen_name'] = user_who_tweeted.lower()\n",
    "    # Adding new ids to dictionary, including tweet count and follower count\n",
    "    if id_who_tweeted not in unique_users:\n",
    "      unique_users[id_who_tweeted] = {}\n",
    "      unique_users[id_who_tweeted]['tweet_count'] = 1\n",
    "      unique_users[id_who_tweeted]['mention_count'] = 0\n",
    "      unique_users[id_who_tweeted]['id'] = id_who_tweeted\n",
    "      unique_users[id_who_tweeted]['followers_count'] = follower_count\n",
    "      unique_users[id_who_tweeted]['screen_name'] = user_who_tweeted.lower()\n",
    "    # Adding in mentioned users\n",
    "    users_mentioned = tweet_json['entities']['user_mentions']\n",
    "    # If the tweet mentions other users in the tweet\n",
    "    if len(users_mentioned) > 0:\n",
    "      # Iterating through each mention in the tweet\n",
    "      for user_mentioned in users_mentioned:\n",
    "        # Extracting details about user\n",
    "        screen_name_mentioned = user_mentioned['screen_name']\n",
    "        id_mentioned = user_mentioned['id']\n",
    "        # Increasing mention count\n",
    "        if id_mentioned in unique_users:\n",
    "          unique_users[id_mentioned]['mention_count'] += 1\n",
    "        # Extracting details about mentioned user\n",
    "        if id_mentioned not in unique_users:\n",
    "          unique_users[id_mentioned] = {}\n",
    "          unique_users[id_mentioned]['tweet_count'] = 0\n",
    "          unique_users[id_mentioned]['mention_count'] = 1\n",
    "          unique_users[id_mentioned]['id'] = id_mentioned\n",
    "          unique_users[id_mentioned]['followers_count'] = 0\n",
    "          unique_users[id_mentioned]['screen_name'] = screen_name_mentioned.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9237327",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T15:37:52.232701Z",
     "iopub.status.busy": "2024-06-10T15:37:52.232290Z",
     "iopub.status.idle": "2024-06-10T15:37:52.240115Z",
     "shell.execute_reply": "2024-06-10T15:37:52.238800Z"
    },
    "papermill": {
     "duration": 0.042664,
     "end_time": "2024-06-10T15:37:52.242537",
     "exception": false,
     "start_time": "2024-06-10T15:37:52.199873",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175077"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the total number of tweets\n",
    "i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dff883",
   "metadata": {
    "papermill": {
     "duration": 0.030431,
     "end_time": "2024-06-10T15:37:52.304849",
     "exception": false,
     "start_time": "2024-06-10T15:37:52.274418",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Above, we can confirm the total number of tweets, which is roughly 175,000! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6edfa8c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T15:37:52.369175Z",
     "iopub.status.busy": "2024-06-10T15:37:52.367817Z",
     "iopub.status.idle": "2024-06-10T15:37:52.375789Z",
     "shell.execute_reply": "2024-06-10T15:37:52.374681Z"
    },
    "papermill": {
     "duration": 0.043209,
     "end_time": "2024-06-10T15:37:52.378699",
     "exception": false,
     "start_time": "2024-06-10T15:37:52.335490",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131663"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the number of unique users\n",
    "len(unique_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41c894d",
   "metadata": {
    "papermill": {
     "duration": 0.030419,
     "end_time": "2024-06-10T15:37:52.440022",
     "exception": false,
     "start_time": "2024-06-10T15:37:52.409603",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Here, we learn that there are 131,663 unique users in the dataset. In order to construct a network of users, we will need to reduce this number. Over 100k users would result in way too many nodes and thus the network graph may be unreadable/overwhelming. To remedy this, we will select only the users who have tweeted at least 2 times and have at least 100,000 followers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca55e5cd",
   "metadata": {
    "papermill": {
     "duration": 0.030408,
     "end_time": "2024-06-10T15:37:52.501291",
     "exception": false,
     "start_time": "2024-06-10T15:37:52.470883",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"4c\"></a>\n",
    "## **Filtering Out Users**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9536df9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T15:37:52.564460Z",
     "iopub.status.busy": "2024-06-10T15:37:52.564067Z",
     "iopub.status.idle": "2024-06-10T15:37:52.723085Z",
     "shell.execute_reply": "2024-06-10T15:37:52.721573Z"
    },
    "papermill": {
     "duration": 0.194065,
     "end_time": "2024-06-10T15:37:52.726071",
     "exception": false,
     "start_time": "2024-06-10T15:37:52.532006",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 415859364 \tscreen_name: nike\n",
      "id: 300114634 \tscreen_name: adidas\n",
      "id: 16252784 \tscreen_name: lululemon\n",
      "198\n"
     ]
    }
   ],
   "source": [
    "# We can't really have 131,663 unique nodes, we need to filter down!\n",
    "\n",
    "# Creating a set of users to include\n",
    "users_to_include = set()\n",
    "\n",
    "# Creating a list of brand users\n",
    "brand_users = ['nike', 'lululemon', 'adidas']\n",
    "\n",
    "# Filtering down users to 2+ tweets and > 100,000 followers\n",
    "user_count = 0\n",
    "for auser in unique_users:\n",
    "  if unique_users[auser]['screen_name'] in brand_users:\n",
    "      print('id:', auser, '\\tscreen_name:', unique_users[auser]['screen_name'])\n",
    "      user_count += 1\n",
    "      users_to_include.add(auser)\n",
    "  elif unique_users[auser]['tweet_count'] >= 2:\n",
    "    if unique_users[auser]['followers_count'] >= 100000:\n",
    "      user_count += 1\n",
    "      users_to_include.add(auser)\n",
    "\n",
    "print(len(users_to_include))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3956e64",
   "metadata": {
    "papermill": {
     "duration": 0.030917,
     "end_time": "2024-06-10T15:37:52.788827",
     "exception": false,
     "start_time": "2024-06-10T15:37:52.757910",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "After selecting only the users with at least 2 tweets and at least 100,000 followers, we retain only 198 unique users! This is a major reduction and will likely lead to the creation of a more intuitive network graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46ae9acf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T15:37:52.854955Z",
     "iopub.status.busy": "2024-06-10T15:37:52.853954Z",
     "iopub.status.idle": "2024-06-10T15:37:52.861549Z",
     "shell.execute_reply": "2024-06-10T15:37:52.860370Z"
    },
    "papermill": {
     "duration": 0.044023,
     "end_time": "2024-06-10T15:37:52.864117",
     "exception": false,
     "start_time": "2024-06-10T15:37:52.820094",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001503839347424865"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comparing length of original number of unique users and filtered users, over 99% reduction!\n",
    "len(users_to_include) / len(unique_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d913bd42",
   "metadata": {
    "papermill": {
     "duration": 0.03106,
     "end_time": "2024-06-10T15:37:52.926720",
     "exception": false,
     "start_time": "2024-06-10T15:37:52.895660",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "To confirm the effect of the reduction above, we can see that the number of 'users_to_include' is only 0.1% of the size of the original group! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6de87235",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T15:37:52.992469Z",
     "iopub.status.busy": "2024-06-10T15:37:52.992050Z",
     "iopub.status.idle": "2024-06-10T15:37:52.998522Z",
     "shell.execute_reply": "2024-06-10T15:37:52.997233Z"
    },
    "papermill": {
     "duration": 0.042133,
     "end_time": "2024-06-10T15:37:53.001716",
     "exception": false,
     "start_time": "2024-06-10T15:37:52.959583",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nike: {'tweet_count': 0, 'mention_count': 120125, 'id': 415859364, 'followers_count': 0, 'screen_name': 'nike'}\n",
      "Adidas: {'tweet_count': 3, 'mention_count': 36654, 'id': 300114634, 'followers_count': 4082910, 'screen_name': 'adidas'}\n",
      "Lululemon: {'tweet_count': 0, 'mention_count': 6294, 'id': 16252784, 'followers_count': 0, 'screen_name': 'lululemon'}\n"
     ]
    }
   ],
   "source": [
    "# Checking the counts for the major brands\n",
    "print(\"Nike:\", unique_users[415859364])\n",
    "print(\"Adidas:\", unique_users[300114634])\n",
    "print(\"Lululemon:\", unique_users[16252784])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a60d24",
   "metadata": {
    "papermill": {
     "duration": 0.031063,
     "end_time": "2024-06-10T15:37:53.064165",
     "exception": false,
     "start_time": "2024-06-10T15:37:53.033102",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Above, we can gain some quick insight into the three major brands. Here, we can see the number of mentions for each brand. Notably, Nike appears to have the vast majority with 120,125, while Lululemon has the least mentions with only 6,294. Also, there appears to be an issue with the 'followers_count', as Nike and Lululemon show 0 followers. Fortunately, this metric will not impact the network graphs we will create. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfa9269",
   "metadata": {
    "papermill": {
     "duration": 0.030994,
     "end_time": "2024-06-10T15:37:53.127581",
     "exception": false,
     "start_time": "2024-06-10T15:37:53.096587",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"4d\"></a>\n",
    "## **Setting up Helper Functions for Graph Analysis and Plotting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55590362",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T15:37:53.193745Z",
     "iopub.status.busy": "2024-06-10T15:37:53.193336Z",
     "iopub.status.idle": "2024-06-10T15:37:53.202850Z",
     "shell.execute_reply": "2024-06-10T15:37:53.201560Z"
    },
    "papermill": {
     "duration": 0.045254,
     "end_time": "2024-06-10T15:37:53.205678",
     "exception": false,
     "start_time": "2024-06-10T15:37:53.160424",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating helper function for graph analysis\n",
    "def graph_summary_stats(G, title='Graph Summary'):\n",
    "  # Display a summary of the graph object created\n",
    "  # https://networkx.org/documentation/stable/reference/functions.html\n",
    "  print('----------------------------------------')\n",
    "  print('#####', title, '#####')\n",
    "  print('number of nodes:', nx.number_of_nodes(G))\n",
    "  print('number of edges:', nx.number_of_edges(G))\n",
    "  print()\n",
    "  print('nodes:', nx.nodes(G))\n",
    "  print()\n",
    "  if G.has_node('adidas'):\n",
    "    print('neighbors of adidas:', list(nx.all_neighbors(G, 'adidas')))\n",
    "  if G.has_node('nike'):\n",
    "    print('neighbors of nike:', list(nx.all_neighbors(G, 'nike')))\n",
    "  if G.has_node('lululemon'):\n",
    "   print('neighbors of lululemon:', list(nx.all_neighbors(G, 'lululemon')))\n",
    "  print('----------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a936ad7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T15:37:53.271496Z",
     "iopub.status.busy": "2024-06-10T15:37:53.271071Z",
     "iopub.status.idle": "2024-06-10T15:37:53.279007Z",
     "shell.execute_reply": "2024-06-10T15:37:53.277817Z"
    },
    "papermill": {
     "duration": 0.043908,
     "end_time": "2024-06-10T15:37:53.281798",
     "exception": false,
     "start_time": "2024-06-10T15:37:53.237890",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating helper function to plot graphs and save to png\n",
    "# THIS CODE BLOCK HAS BEEN COMMENTED OUT\n",
    "# THESE WOULD BE INCLUDED FOR SAVING THE GRAPHS IN A GOOGLE DRIVE DIRECTORY\n",
    "\n",
    "# def plot_graph(G, file_path='temp_file', use_edge_weight=True, plot_size='large'):\n",
    "\n",
    "  # Defining node colors\n",
    "#  default_color = 'blue'\n",
    "#  highlight_color = 'red'\n",
    "#  brand_users = ['nike', 'lululemon', 'adidas']\n",
    "#  node_colors = [highlight_color if node in brand_users else default_color for node in G.nodes()]\n",
    "\n",
    "  # Setting plot sizes\n",
    "#  if plot_size == 'medium-large':\n",
    "#    p_figsize = (150, 150)\n",
    "#    p_font_size = 20\n",
    "#    p_edge_width_scale = 2\n",
    "#    p_node_size = 5000\n",
    "#    p_arrow_size = 50\n",
    "#    p_k = None\n",
    "#  if plot_size == 'medium':\n",
    "#    p_figsize = (25, 25)\n",
    "#    p_font_size = 12\n",
    "#    p_edge_width_scale = 2\n",
    "#    p_node_size = 3000\n",
    "#    p_arrow_size = 50\n",
    "#    p_k = None\n",
    "#  elif plot_size == 'small':\n",
    "#    p_figsize = (50, 50)\n",
    "#    p_font_size = 20\n",
    "#    p_edge_width_scale = 2\n",
    "#    p_node_size = 30000\n",
    "#    p_arrow_size = 100\n",
    "#    p_k = None\n",
    "#  elif plot_size == 'x-small':\n",
    "#    p_figsize = (12, 12)\n",
    "#    p_font_size = 20\n",
    "#    p_edge_width_scale = 2\n",
    "#    p_node_size = 5000\n",
    "#    p_arrow_size = 100\n",
    "#    p_k = None\n",
    "#  else:\n",
    "#    p_figsize = (300, 300)\n",
    "#    p_font_size = 20\n",
    "#    p_edge_width_scale = 2\n",
    "#    p_node_size = 3000\n",
    "#    p_arrow_size = 100\n",
    "#    p_k = None\n",
    "\n",
    "  # Generating spring layout for graphs\n",
    "#  positions = nx.spring_layout(G, k=p_k)\n",
    "\n",
    "  # Extracting edge weights for drawing\n",
    "#  if use_edge_weight == True:\n",
    "#    p_edge_weights = p_edge_width_scale*[G[u][v]['weight'] for u, v in G.edges()]\n",
    "#  else:\n",
    "#    p_edge_weights = p_edge_width_scale\n",
    "\n",
    "  # Creating graph\n",
    "#  fig, ax = plt.subplots(1, 1, figsize=p_figsize)\n",
    "#  nx.draw_networkx(G, pos=positions, ax=ax, node_color=node_colors,\n",
    "#                   font_color=\"#FFFFFF\", font_size=p_font_size,\n",
    "#                   node_size=p_node_size, width=p_edge_weights,\n",
    "#                   arrows=True, arrowsize=p_arrow_size)\n",
    "  # Saving the graph as a png to the working directory\n",
    "#  print_file_path = '%s/%s.png' % (WORKING_DIR, file_path)\n",
    "#  plt.savefig(print_file_path, format='PNG')\n",
    "#  plt.close('all')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b3292d",
   "metadata": {
    "papermill": {
     "duration": 0.032454,
     "end_time": "2024-06-10T15:37:53.346252",
     "exception": false,
     "start_time": "2024-06-10T15:37:53.313798",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As mentioned previously, this project was originally created in Google Colab. I have opted to comment out several sections, such as the one above, as they interact with a Google Drive folder. Again, if you would like to view my original work and all of its files, please check it out [here](https://drive.google.com/drive/folders/1rrRiAegl6A-P6BVP3oRLgpZQmyP8J-jR?usp=sharing)! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08db1422",
   "metadata": {
    "papermill": {
     "duration": 0.031897,
     "end_time": "2024-06-10T15:37:53.410387",
     "exception": false,
     "start_time": "2024-06-10T15:37:53.378490",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"4e\"></a>\n",
    "## **Creating the Graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87a9f1fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T15:37:53.475342Z",
     "iopub.status.busy": "2024-06-10T15:37:53.474930Z",
     "iopub.status.idle": "2024-06-10T15:37:53.481317Z",
     "shell.execute_reply": "2024-06-10T15:37:53.480048Z"
    },
    "papermill": {
     "duration": 0.042211,
     "end_time": "2024-06-10T15:37:53.484103",
     "exception": false,
     "start_time": "2024-06-10T15:37:53.441892",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reopening 'nikelululemonadidas_tweets.jsonl' file in order to iterate through it again\n",
    "json_file = open('/kaggle/input/nikelululemonadidas-tweets-jsonl/nikelululemonadidas_tweets.jsonl', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35955ea0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T15:37:53.548353Z",
     "iopub.status.busy": "2024-06-10T15:37:53.547871Z",
     "iopub.status.idle": "2024-06-10T15:37:53.553326Z",
     "shell.execute_reply": "2024-06-10T15:37:53.552073Z"
    },
    "papermill": {
     "duration": 0.040676,
     "end_time": "2024-06-10T15:37:53.555999",
     "exception": false,
     "start_time": "2024-06-10T15:37:53.515323",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preparing directional graph\n",
    "Mentions_Graph = nx.DiGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36a4fe6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T15:37:53.624974Z",
     "iopub.status.busy": "2024-06-10T15:37:53.624583Z",
     "iopub.status.idle": "2024-06-10T15:38:16.837941Z",
     "shell.execute_reply": "2024-06-10T15:38:16.836604Z"
    },
    "papermill": {
     "duration": 23.253148,
     "end_time": "2024-06-10T15:38:16.841021",
     "exception": false,
     "start_time": "2024-06-10T15:37:53.587873",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tweets iterated\n",
      "10000 tweets iterated\n",
      "20000 tweets iterated\n",
      "30000 tweets iterated\n",
      "40000 tweets iterated\n",
      "50000 tweets iterated\n",
      "60000 tweets iterated\n",
      "70000 tweets iterated\n",
      "80000 tweets iterated\n",
      "90000 tweets iterated\n",
      "100000 tweets iterated\n",
      "110000 tweets iterated\n",
      "120000 tweets iterated\n",
      "130000 tweets iterated\n",
      "140000 tweets iterated\n",
      "150000 tweets iterated\n",
      "160000 tweets iterated\n",
      "170000 tweets iterated\n"
     ]
    }
   ],
   "source": [
    "# Identifying unique users in the mention network\n",
    "\n",
    "# Similar steps as before, iterating through the file, extracting screen name, id, and follower count\n",
    "for i, atweet in enumerate(json_file):\n",
    "    # Creating counter to track progress\n",
    "    if i % 10000 == 0:\n",
    "      print(\"%s tweets iterated\" % i)\n",
    "    tweet_json = json.loads(atweet)\n",
    "    # Extracting screen name, id, follower count\n",
    "    user_who_tweeted = tweet_json['user']['screen_name'].lower()\n",
    "    id_who_tweeted = tweet_json['user']['id']\n",
    "    follower_count = tweet_json['user']['followers_count']\n",
    "    # If id is in filtered user list, we pull out the users they mention in a tweet\n",
    "    if id_who_tweeted in users_to_include:\n",
    "      users = tweet_json['entities']['user_mentions']\n",
    "      if len(users) > 0:\n",
    "          # Iterating through users being mentioned, extracting their screen name and id\n",
    "          for auser in users:\n",
    "              screen_name = auser['screen_name'].lower()\n",
    "              mention_id = auser['id']\n",
    "              # Appending this as an edge in the graph\n",
    "              if mention_id in users_to_include:\n",
    "                if user_who_tweeted != screen_name:\n",
    "                  if Mentions_Graph.has_edge(user_who_tweeted, screen_name):\n",
    "                    # If the edge exists, increment its weight\n",
    "                    Mentions_Graph[user_who_tweeted][screen_name]['weight'] += 1\n",
    "                  else:\n",
    "                    # If the edge doesn't exist, add it with weight = 1\n",
    "                    Mentions_Graph.add_edge(user_who_tweeted, screen_name, weight=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9c8a3d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T15:38:16.913145Z",
     "iopub.status.busy": "2024-06-10T15:38:16.912717Z",
     "iopub.status.idle": "2024-06-10T15:38:16.919876Z",
     "shell.execute_reply": "2024-06-10T15:38:16.918313Z"
    },
    "papermill": {
     "duration": 0.046365,
     "end_time": "2024-06-10T15:38:16.923215",
     "exception": false,
     "start_time": "2024-06-10T15:38:16.876850",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "##### Graph Summary #####\n",
      "number of nodes: 194\n",
      "number of edges: 339\n",
      "\n",
      "nodes: ['kiganyi_', 'adidas', 'undefeatedinc', 'uniwatch', 'nike', 'atari_jones', 'adidasoriginals', 'solefed', 'jumpman23', 'bajabiri', 'golfdigest', 'lululemon', 'jonahlupton', 'nikestore', 'jermainedupri', 'finishline', 'wwd', 'hiphopwired', 'xboxwire', 'aarongreenberg', 'xbox', 'xboxp3', 'predsnhl', 'lakings', 'dashiexp', 'fastcompany', 'reignofapril', 'nrarmour', 'sbjsbd', 'barcaacademy', 'khou', 'oakley', 'marshablackburn', 'senrickscott', 'schuh', 'dezeen', 'lebatardshow', 'billiejeanking', 'barrysanders', 'complex', 'brooksrunning', 'adidasrunning', 'wiedenkennedy', 'bottom2thatop', 'candace_parker', 'snkr_twitr', 'katgraham', 'joshog', 'adweek', 'kingjames', 'jamesgunn', 'pomklementieff', 'loyalty360', 'jasonlacanfora', 'kohls', '7newsdc', 'realrclark25', 'adidashoops', 'wnba', 'barondavis', 'metropolismag', 'legiqn', 'fatkiddeals', 'jack_p', 'nyctsubway', 'nikebasketball', 'slamonline', 'rga', 'stockx', 'gladstein', 'msmelchen', 'sproutsocial', 'orioles', 'natbrunell', 'fousey', 'okayplayer', 'sacramentokings', 'yahoofinance', 'enesfreedom', 'redsteeze', 'sethamandel', 'slimjim', 'tengbiao', 'hughhewitt', 'muslimmatters', 'nedryun', 'ericmmatheny', 'lifeatpurdue', 'epochtimes', 'suns', 'donaldjtrumpjr', 'joshrogin', 'yesnicksearcy', 'highsnobiety', 'lasparks', 'realunogame', 'giannis_an34', 'deezefi', 'mrleonardkim', 'evankirstel', 'mattel', 'reebok', 'pennstatefball', 'stevedeaceshow', 'girlsintech', 'uoregon', 'mystic7', 'ericpmusselman', 'qrich', 'cointelegraph', 'houstondynamo', 'threadreaderapp', 'prestonpysh', 'rwang0', 'tandfn', 'thebussypleaser', 'techinsider', 'jdofficial', 'xlr8r', 'onsoranje', 'liekemartens1', 'knvb', 'realshellyannfp', 'kicksdeals', 'lafc', 'thegrovela', 'lagalaxy', 'adamsconsulting', 'coindesk', 'adidasfootball', 'spicer', 'kieraplease', 'jkylebass', 'kfile', 'sportsiren', 'nicekicks', 'scottwarner18', 'complexstyle', 'tinashe', 'rosgo21', 'roblox', 'bloxy_news', 'insideroblox', 'atmos_usa', 'nwsl', 'adamjacksonsf', 'novambb', 'solomonyue', 'angelaruggiero', 'bauerhockey', 'jenniferwalcott', 'road_2_ft_worth', 'vampybitme', 'iamwellandgood', 'tropofarmer', 'boredelonmusk', 'mattsteffanina', 'mediapost', 'brooklynnets', 'vidcon', 'uninterrupted', 'hunterheather', 'misstabstevens', 'chainlinkgod', '1djfirstclass', 'chsommers', 'tonipayne', 'cowhercbs', 'detroitpistons', 'cory_shoff', 'usa_lacrosse', 'xxxcrypt0', 'ijustine', 'hackapreneur', 'burgerking', 'namecheap', 'campaignbrands', 'elmaaelmoo', 'colethereum', 'nebraskancrypto', 'vaynermedia', 'schmittnyc', 'mpinoe', 'seanmdav', 'hunterw', 'realvision', 'reallisariley', 'rexchapman', 'qiasomar', 'kingofthecrane', 'cnbc', 'ericvdunn', 'trustlessstate', 'lopp']\n",
      "\n",
      "neighbors of adidas: ['kiganyi_', 'undefeatedinc', 'atari_jones', 'bajabiri', 'jermainedupri', 'xboxwire', 'aarongreenberg', 'xboxp3', 'lakings', 'dashiexp', 'xbox', 'wwd', 'hiphopwired', 'billiejeanking', 'complex', 'bottom2thatop', 'katgraham', 'jamesgunn', 'pomklementieff', 'loyalty360', 'kohls', 'uniwatch', 'adidashoops', 'metropolismag', 'adidasrunning', 'legiqn', 'fatkiddeals', 'jack_p', 'nyctsubway', 'snkr_twitr', 'stockx', 'adweek', 'houstondynamo', 'reebok', 'finishline', 'sbjsbd', 'techinsider', 'kicksdeals', 'lafc', 'lagalaxy', 'adidasfootball', 'spicer', 'kfile', 'sportsiren', 'highsnobiety', 'nicekicks', 'adidasoriginals', 'coindesk', 'jenniferwalcott', 'thebussypleaser', 'vampybitme', 'tropofarmer', 'boredelonmusk', 'mattsteffanina', 'brooklynnets', 'hunterheather', 'jdofficial', 'misstabstevens', 'complexstyle', 'chainlinkgod', 'wnba', 'tonipayne', 'predsnhl', 'slamonline', 'schuh', 'xxxcrypt0', 'candace_parker', 'burgerking', 'namecheap', 'iamwellandgood', 'deezefi', 'dezeen', 'hunterw', 'cointelegraph', 'ericvdunn', 'burgerking', 'xbox', 'nike']\n",
      "neighbors of nike: ['uniwatch', 'solefed', 'nikestore', 'undefeatedinc', 'finishline', 'wwd', 'hiphopwired', 'fastcompany', 'reignofapril', 'nrarmour', 'sbjsbd', 'barcaacademy', 'marshablackburn', 'senrickscott', 'schuh', 'dezeen', 'lebatardshow', 'barrysanders', 'brooksrunning', 'wiedenkennedy', 'snkr_twitr', 'jumpman23', 'kingjames', 'jasonlacanfora', '7newsdc', 'barondavis', 'nikebasketball', 'slamonline', 'rga', 'msmelchen', 'sproutsocial', 'orioles', 'natbrunell', 'fousey', 'okayplayer', 'sacramentokings', 'realrclark25', 'enesfreedom', 'redsteeze', 'sethamandel', 'slimjim', 'tengbiao', 'gladstein', 'hughhewitt', 'muslimmatters', 'nedryun', 'ericmmatheny', 'lifeatpurdue', 'epochtimes', 'suns', 'donaldjtrumpjr', 'joshrogin', 'yesnicksearcy', 'highsnobiety', 'lasparks', 'realunogame', 'mrleonardkim', 'evankirstel', 'mattel', 'reebok', 'pennstatefball', 'stevedeaceshow', 'girlsintech', 'uoregon', 'ericpmusselman', 'qrich', 'wnba', 'cointelegraph', 'threadreaderapp', 'prestonpysh', 'rwang0', 'tandfn', 'thebussypleaser', 'techinsider', 'jdofficial', 'xlr8r', 'onsoranje', 'knvb', 'liekemartens1', 'realshellyannfp', 'thegrovela', 'adamsconsulting', 'coindesk', 'giannis_an34', 'kieraplease', 'jkylebass', 'khou', 'scottwarner18', 'kicksdeals', 'nicekicks', 'complexstyle', 'rosgo21', 'roblox', 'bloxy_news', 'insideroblox', 'nwsl', 'adamjacksonsf', 'novambb', 'solomonyue', 'angelaruggiero', 'boredelonmusk', 'vidcon', 'uninterrupted', '1djfirstclass', 'chsommers', 'tonipayne', 'cowhercbs', 'detroitpistons', 'cory_shoff', 'usa_lacrosse', 'hackapreneur', 'tropofarmer', 'burgerking', 'deezefi', 'namecheap', 'mattsteffanina', 'campaignbrands', 'elmaaelmoo', 'mediapost', 'adidas', 'colethereum', 'nebraskancrypto', 'vaynermedia', 'tinashe', 'schmittnyc', 'mpinoe', 'seanmdav', 'realvision', 'xxxcrypt0', 'ijustine', 'bajabiri', 'rexchapman', 'qiasomar', 'kingofthecrane', 'trustlessstate', 'lopp']\n",
      "neighbors of lululemon: ['golfdigest', 'jonahlupton', 'predsnhl', 'khou', 'oakley', 'brooksrunning', 'joshog', 'adweek', 'realrclark25', 'yahoofinance', 'uniwatch', 'deezefi', 'mrleonardkim', 'mystic7', 'evankirstel', 'bauerhockey', 'thegrovela', 'road_2_ft_worth', 'iamwellandgood', 'mediapost', 'ijustine', 'reallisariley', 'wwd', 'cnbc']\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking the summary of the mentions graph\n",
    "graph_summary_stats(G = Mentions_Graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ed696f",
   "metadata": {
    "papermill": {
     "duration": 0.033862,
     "end_time": "2024-06-10T15:38:16.990818",
     "exception": false,
     "start_time": "2024-06-10T15:38:16.956956",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Above, we can see the summary of the Mentions network graph! Here, we can see that we have 194 nodes (users) with 339 edges (connections). Furthermore, we can see several examples of the neighbors for each brand. For example, some of Adidas' neighbors include: 'kiganyi_', 'undefeatedinc', and 'atari_jones'. Although we have already reduced the total number of nodes, it would still be beneficial to create smaller subgraphs. These subgraphs (created below), will provide insight into which users are interacting with all three brands, and which users are common amongst combinations of two brands. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911c5258",
   "metadata": {
    "papermill": {
     "duration": 0.034693,
     "end_time": "2024-06-10T15:38:17.059726",
     "exception": false,
     "start_time": "2024-06-10T15:38:17.025033",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"4f\"></a>\n",
    "## **Creating Subgraphs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c45c1af7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T15:38:17.128851Z",
     "iopub.status.busy": "2024-06-10T15:38:17.128444Z",
     "iopub.status.idle": "2024-06-10T15:38:17.142275Z",
     "shell.execute_reply": "2024-06-10T15:38:17.140899Z"
    },
    "papermill": {
     "duration": 0.051739,
     "end_time": "2024-06-10T15:38:17.145107",
     "exception": false,
     "start_time": "2024-06-10T15:38:17.093368",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating and defining node interactions and connections\n",
    "# Kudos to 'Chiuchiyin'! The following code chunk has been adapted from their work\n",
    "\n",
    "# Defining key nodes\n",
    "key_nodes_all = ['nike', 'lululemon', 'adidas']\n",
    "key_nodes_nl = ['nike', 'lululemon']\n",
    "key_nodes_na = ['nike', 'adidas']\n",
    "key_nodes_al = ['adidas', 'lululemon']\n",
    "\n",
    "# Finding neighbors of the key nodes by themselves\n",
    "neighbors_sets_n = set(nx.all_neighbors(Mentions_Graph, 'nike'))\n",
    "neighbors_sets_a = set(nx.all_neighbors(Mentions_Graph, 'adidas'))\n",
    "neighbors_sets_l = set(nx.all_neighbors(Mentions_Graph, 'lululemon'))\n",
    "\n",
    "# Finding neighbors of the key nodes\n",
    "neighbors_sets_all = [set(nx.all_neighbors(Mentions_Graph, node)) for node in key_nodes_all]\n",
    "neighbors_sets_nl = [set(nx.all_neighbors(Mentions_Graph, node)) for node in key_nodes_nl]\n",
    "neighbors_sets_na = [set(nx.all_neighbors(Mentions_Graph, node)) for node in key_nodes_na]\n",
    "neighbors_sets_al = [set(nx.all_neighbors(Mentions_Graph, node)) for node in key_nodes_al]\n",
    "\n",
    "# Intersecting the sets to get nodes connected to all key nodes\n",
    "common_neighbors_all = set.intersection(*neighbors_sets_all)\n",
    "\n",
    "# Intersecting the sets to get nodes connected to only 2 key nodes\n",
    "common_neighbors_nl = set.intersection(*neighbors_sets_nl) - common_neighbors_all - set(key_nodes_al)\n",
    "common_neighbors_na = set.intersection(*neighbors_sets_na) - common_neighbors_all - set(key_nodes_nl)\n",
    "common_neighbors_al = set.intersection(*neighbors_sets_al) - common_neighbors_all - set(key_nodes_na)\n",
    "\n",
    "# Getting nodes connected to any one of the key nodes but not all of them\n",
    "union_neighbors = set.union(*neighbors_sets_all)\n",
    "\n",
    "# Getting nodes connected to only 1 brand\n",
    "exclusive_neighbors = (union_neighbors - common_neighbors_all\n",
    "                       - common_neighbors_nl - common_neighbors_na - common_neighbors_al)\n",
    "\n",
    "# Getting nodes connected to each specific brand\n",
    "exclusive_neighbors_n = neighbors_sets_n - common_neighbors_all - common_neighbors_nl - common_neighbors_na - set(key_nodes_al)\n",
    "exclusive_neighbors_a = neighbors_sets_a - common_neighbors_all - common_neighbors_na - common_neighbors_nl - set(key_nodes_nl)\n",
    "exclusive_neighbors_l = neighbors_sets_l - common_neighbors_all - common_neighbors_nl - common_neighbors_al - set(key_nodes_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8cb25c72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T15:38:17.215151Z",
     "iopub.status.busy": "2024-06-10T15:38:17.214723Z",
     "iopub.status.idle": "2024-06-10T15:38:17.222971Z",
     "shell.execute_reply": "2024-06-10T15:38:17.221599Z"
    },
    "papermill": {
     "duration": 0.046874,
     "end_time": "2024-06-10T15:38:17.225698",
     "exception": false,
     "start_time": "2024-06-10T15:38:17.178824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating subgraphs\n",
    "\n",
    "# Creating subgraph of bridges between all brand nodes\n",
    "nodes_to_keep_all = list(common_neighbors_all) + key_nodes_all\n",
    "Mentions_Graph_bridge_all = Mentions_Graph.subgraph(nodes_to_keep_all)\n",
    "\n",
    "# Creating subgraph of bridges between Nike & Adidas\n",
    "nodes_to_keep_na = list(common_neighbors_na) + key_nodes_na\n",
    "Mentions_Graph_bridge_na = Mentions_Graph.subgraph(nodes_to_keep_na)\n",
    "\n",
    "# Creating subgraph of bridges between Nike & Lululemon\n",
    "nodes_to_keep_nl = list(common_neighbors_nl) + key_nodes_nl\n",
    "Mentions_Graph_bridge_nl = Mentions_Graph.subgraph(nodes_to_keep_nl)\n",
    "\n",
    "# Creating subgraph of bridges between Adidas & Lululemon\n",
    "nodes_to_keep_al = list(common_neighbors_al) + key_nodes_al\n",
    "Mentions_Graph_bridge_al = Mentions_Graph.subgraph(nodes_to_keep_al)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "743b8c7a",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-06-10T15:38:17.294539Z",
     "iopub.status.busy": "2024-06-10T15:38:17.294106Z",
     "iopub.status.idle": "2024-06-10T15:38:17.304479Z",
     "shell.execute_reply": "2024-06-10T15:38:17.303028Z"
    },
    "papermill": {
     "duration": 0.048756,
     "end_time": "2024-06-10T15:38:17.308083",
     "exception": false,
     "start_time": "2024-06-10T15:38:17.259327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "##### Bridges Between All Brand Nodes #####\n",
      "number of nodes: 6\n",
      "number of edges: 10\n",
      "\n",
      "nodes: ['nike', 'adidas', 'uniwatch', 'wwd', 'deezefi', 'lululemon']\n",
      "\n",
      "neighbors of adidas: ['wwd', 'uniwatch', 'deezefi', 'nike']\n",
      "neighbors of nike: ['uniwatch', 'wwd', 'deezefi', 'adidas']\n",
      "neighbors of lululemon: ['uniwatch', 'deezefi', 'wwd']\n",
      "----------------------------------------\n",
      "\n",
      "----------------------------------------\n",
      "##### Nike & Adidas Bridges #####\n",
      "number of nodes: 29\n",
      "number of edges: 57\n",
      "\n",
      "nodes: ['namecheap', 'adidas', 'snkr_twitr', 'bajabiri', 'dezeen', 'jdofficial', 'mattsteffanina', 'complexstyle', 'tropofarmer', 'undefeatedinc', 'highsnobiety', 'sbjsbd', 'xxxcrypt0', 'cointelegraph', 'boredelonmusk', 'reebok', 'schuh', 'nicekicks', 'slamonline', 'finishline', 'burgerking', 'nike', 'thebussypleaser', 'techinsider', 'tonipayne', 'kicksdeals', 'hiphopwired', 'wnba', 'coindesk']\n",
      "\n",
      "neighbors of adidas: ['undefeatedinc', 'bajabiri', 'hiphopwired', 'snkr_twitr', 'reebok', 'finishline', 'sbjsbd', 'techinsider', 'kicksdeals', 'highsnobiety', 'nicekicks', 'coindesk', 'thebussypleaser', 'tropofarmer', 'boredelonmusk', 'mattsteffanina', 'jdofficial', 'complexstyle', 'wnba', 'tonipayne', 'slamonline', 'schuh', 'xxxcrypt0', 'burgerking', 'namecheap', 'dezeen', 'cointelegraph', 'burgerking', 'nike']\n",
      "neighbors of nike: ['undefeatedinc', 'finishline', 'hiphopwired', 'sbjsbd', 'schuh', 'dezeen', 'snkr_twitr', 'slamonline', 'highsnobiety', 'reebok', 'wnba', 'cointelegraph', 'thebussypleaser', 'techinsider', 'jdofficial', 'coindesk', 'kicksdeals', 'nicekicks', 'complexstyle', 'boredelonmusk', 'tonipayne', 'tropofarmer', 'burgerking', 'namecheap', 'mattsteffanina', 'adidas', 'xxxcrypt0', 'bajabiri']\n",
      "----------------------------------------\n",
      "\n",
      "----------------------------------------\n",
      "##### Nike & Lululemon Bridges #####\n",
      "number of nodes: 10\n",
      "number of edges: 16\n",
      "\n",
      "nodes: ['nike', 'thegrovela', 'mediapost', 'ijustine', 'evankirstel', 'lululemon', 'brooksrunning', 'mrleonardkim', 'khou', 'realrclark25']\n",
      "\n",
      "neighbors of nike: ['brooksrunning', 'realrclark25', 'mrleonardkim', 'evankirstel', 'thegrovela', 'khou', 'mediapost', 'ijustine']\n",
      "neighbors of lululemon: ['khou', 'brooksrunning', 'realrclark25', 'mrleonardkim', 'evankirstel', 'thegrovela', 'mediapost', 'ijustine']\n",
      "----------------------------------------\n",
      "\n",
      "----------------------------------------\n",
      "##### Adidas & Lululemon Bridges #####\n",
      "number of nodes: 5\n",
      "number of edges: 6\n",
      "\n",
      "nodes: ['adidas', 'iamwellandgood', 'adweek', 'lululemon', 'predsnhl']\n",
      "\n",
      "neighbors of adidas: ['adweek', 'predsnhl', 'iamwellandgood']\n",
      "neighbors of lululemon: ['predsnhl', 'adweek', 'iamwellandgood']\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking the summaries of the new subgraphs\n",
    "graph_summary_stats(G = Mentions_Graph_bridge_all, title='Bridges Between All Brand Nodes')\n",
    "graph_summary_stats(G = Mentions_Graph_bridge_na, title='Nike & Adidas Bridges')\n",
    "graph_summary_stats(G = Mentions_Graph_bridge_nl, title='Nike & Lululemon Bridges')\n",
    "graph_summary_stats(G = Mentions_Graph_bridge_al, title='Adidas & Lululemon Bridges')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1fc49a",
   "metadata": {
    "papermill": {
     "duration": 0.033037,
     "end_time": "2024-06-10T15:38:17.375495",
     "exception": false,
     "start_time": "2024-06-10T15:38:17.342458",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Above, we can see our new subgraphs! At a glance, these appear to be much more interpretable. For example, the graph 'Bridges Between All Brand Nodes', show the users that are connected to (mentioning) all three brands. In this particular network, there are only 6 nodes and 10 edges. These subgraphs will be shown and explored in more detail in the Conclusion & Analysis section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c980af",
   "metadata": {
    "papermill": {
     "duration": 0.033621,
     "end_time": "2024-06-10T15:38:17.443860",
     "exception": false,
     "start_time": "2024-06-10T15:38:17.410239",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"4g\"></a>\n",
    "## **Saving and Plotting the Graphs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70c44c2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T15:38:17.512806Z",
     "iopub.status.busy": "2024-06-10T15:38:17.512332Z",
     "iopub.status.idle": "2024-06-10T15:38:17.520089Z",
     "shell.execute_reply": "2024-06-10T15:38:17.518890Z"
    },
    "papermill": {
     "duration": 0.045814,
     "end_time": "2024-06-10T15:38:17.522922",
     "exception": false,
     "start_time": "2024-06-10T15:38:17.477108",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This code chunk saves the graphs to the set Google Drive filepath\n",
    "\n",
    "# Saving and plotting the full graph\n",
    "# plot_graph(G = Mentions_Graph, file_path='mentions_network', plot_size='large')\n",
    "\n",
    "# Saving and plotting the subgraphs\n",
    "# plot_graph(G = Mentions_Graph_bridge_all, file_path='mentions_network_bridge_all', plot_size='small')\n",
    "# plot_graph(G = Mentions_Graph_bridge_na, file_path='mentions_network_bridge_nike_adidas', plot_size='small')\n",
    "# plot_graph(G = Mentions_Graph_bridge_nl, file_path='mentions_network_bridge_nike_lululemon', plot_size='small')\n",
    "# plot_graph(G = Mentions_Graph_bridge_al, file_path='mentions_network_bridge_adidas_lululemon', plot_size='small')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f6910b",
   "metadata": {
    "papermill": {
     "duration": 0.033172,
     "end_time": "2024-06-10T15:38:17.589662",
     "exception": false,
     "start_time": "2024-06-10T15:38:17.556490",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"toc\"></a>\n",
    "\n",
    "<a href=\"#toc\" style=\"background-color:blue; color:white; padding: 7px 10px; text-decoration: none; border-radius: 50px;\">Back to top</a><a id=\"toc\"></a>\n",
    "\n",
    "<a id=\"toc\"></a>\n",
    "\n",
    "<a href=\"#6\" style=\"background-color:blue; color:white; padding: 7px 10px; text-decoration: none; border-radius: 50px;\">Next Section</a><a id=\"toc\"></a>\n",
    "\n",
    "<a id=\"5\"></a>\n",
    "\n",
    "---\n",
    "# **Semantic Network**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81b06ef",
   "metadata": {
    "papermill": {
     "duration": 0.032808,
     "end_time": "2024-06-10T15:38:17.655811",
     "exception": false,
     "start_time": "2024-06-10T15:38:17.623003",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"5a\"></a>\n",
    "## **Setting Up Helper Functions for Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5cf96d99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T15:38:17.727392Z",
     "iopub.status.busy": "2024-06-10T15:38:17.727017Z",
     "iopub.status.idle": "2024-06-10T15:38:17.903767Z",
     "shell.execute_reply": "2024-06-10T15:38:17.902448Z"
    },
    "papermill": {
     "duration": 0.216273,
     "end_time": "2024-06-10T15:38:17.906693",
     "exception": false,
     "start_time": "2024-06-10T15:38:17.690420",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating helper functions for data preprocessing\n",
    "\n",
    "# Creating variable names for nltk functions\n",
    "from nltk.corpus import wordnet\n",
    "TWEET_TOKENIZER = nltk.TweetTokenizer().tokenize\n",
    "WORD_TOKENIZER = nltk.tokenize.word_tokenize\n",
    "STEMMER = nltk.PorterStemmer()\n",
    "LEMMATIZER = nltk.WordNetLemmatizer()\n",
    "\n",
    "# Removing URLs\n",
    "def removeURL(tokens):\n",
    "  return [t for t in tokens\n",
    "          if not t.startswith(\"http://\")\n",
    "          and not t.startswith(\"https://\")\n",
    "        ]\n",
    "\n",
    "# Tokenizing the text\n",
    "def tokenize(text, lowercase=True, tweet=False):\n",
    "  if lowercase:\n",
    "        text = text.lower()\n",
    "  if tweet:\n",
    "        return TWEET_TOKENIZER(text)\n",
    "  else:\n",
    "        return WORD_TOKENIZER(text)\n",
    "\n",
    "# Reducing the number of repeated words\n",
    "def stem(tokens):\n",
    "  return [STEMMER.stem(token) for token in tokens]\n",
    "\n",
    "# Removing stopwords\n",
    "def remove_stopwords(tokens, stopwords=None):\n",
    "  if stopwords is None:\n",
    "    stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "  return [ token for token in tokens if token not in stopwords]\n",
    "\n",
    "# Feature reduction, graphing only the root words\n",
    "def lemmatize(tokens):\n",
    "    lemmas = []\n",
    "    for token in tokens:\n",
    "        if isinstance(token, str):\n",
    "            lemmas.append(LEMMATIZER.lemmatize(token))\n",
    "        else:\n",
    "            lemmas.append(LEMMATIZER.lemmatize(*token))\n",
    "    return lemmas\n",
    "\n",
    "# Removing punctuation\n",
    "def remove_punctuation(tokens,\n",
    "                       strip_mentions=False,\n",
    "                       strip_hashtags=False,\n",
    "                       strict=False):\n",
    "    tokens = [t for t in tokens if t not in string.punctuation]\n",
    "    if strip_mentions:\n",
    "        tokens = [t.lstrip('@') for t in tokens]\n",
    "    if strip_hashtags:\n",
    "        tokens = [t.lstrip('#') for t in tokens]\n",
    "    if strict:\n",
    "        cleaned = []\n",
    "        for t in tokens:\n",
    "            cleaned.append(\n",
    "                t.translate(str.maketrans('', '', string.punctuation)).strip())\n",
    "        tokens = [t for t in cleaned if t]\n",
    "    return tokens\n",
    "\n",
    "# Removing words less than 2 characters\n",
    "def remove_single_words(tokens):\n",
    "  goodwords = []\n",
    "  for a_feature in tokens:\n",
    "    if len(a_feature) > 1:\n",
    "      goodwords.append(a_feature)\n",
    "  return goodwords\n",
    "\n",
    "# Filtering out certain parts of speech\n",
    "def filter_part_of_speech(tokens, tagger=nltk.tag.PerceptronTagger().tag, parts_of_speech=None):\n",
    "  words = tokens\n",
    "  tags = tagger(words)\n",
    "  tokens = []\n",
    "  for tag in tags:\n",
    "      if parts_of_speech is None or tag[1] in parts_of_speech:\n",
    "        if tag[0] not in tokens:\n",
    "          tokens.append(tag[0])\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf5654cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T15:38:17.976843Z",
     "iopub.status.busy": "2024-06-10T15:38:17.975803Z",
     "iopub.status.idle": "2024-06-10T15:38:17.984337Z",
     "shell.execute_reply": "2024-06-10T15:38:17.982959Z"
    },
    "papermill": {
     "duration": 0.046571,
     "end_time": "2024-06-10T15:38:17.987104",
     "exception": false,
     "start_time": "2024-06-10T15:38:17.940533",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating 'text_preprocessing' function to hold all helper functions above\n",
    "def text_preprocessing(text):\n",
    "  tokens = tokenize(text, lowercase=True, tweet=True)\n",
    "  tokens = filter_part_of_speech(tokens, parts_of_speech=['NNP', 'NN', 'NNS', 'NNPS', # Nouns\n",
    "                                                            'JJ', 'JJR', 'JJS', # Adjectives\n",
    "                                                            'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']) #Verbs\n",
    "  tokens = removeURL(tokens)\n",
    "  tokens = remove_stopwords(tokens, stopwords = stopwords_set)\n",
    "  tokens = remove_punctuation(tokens, strip_mentions=True, strip_hashtags=True)\n",
    "  tokens = lemmatize(tokens)\n",
    "  tokens = remove_single_words(tokens)\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1fd65476",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T15:38:18.058240Z",
     "iopub.status.busy": "2024-06-10T15:38:18.057817Z",
     "iopub.status.idle": "2024-06-10T15:38:18.068545Z",
     "shell.execute_reply": "2024-06-10T15:38:18.067018Z"
    },
    "papermill": {
     "duration": 0.050742,
     "end_time": "2024-06-10T15:38:18.071563",
     "exception": false,
     "start_time": "2024-06-10T15:38:18.020821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setting and expanding the list of stopwords\n",
    "stopwords_set = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stopwords_set.add('rt')\n",
    "stopwords_set.add(\"'s\")\n",
    "stopwords_set.add('...')\n",
    "stopwords_set.add('..')\n",
    "stopwords_set.add(':/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585bffd6",
   "metadata": {
    "papermill": {
     "duration": 0.033349,
     "end_time": "2024-06-10T15:38:18.139190",
     "exception": false,
     "start_time": "2024-06-10T15:38:18.105841",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The three code blocks above set up several functions that preprocess the tweet text in ways that are common for natural language processing. The first code chunk sets up these basic cleaning functions, such as removing URLs, tokenizing, lemmatizing, and removing punctuation. The second code block creates a function called 'text_preprocessing', which effectively takes the text from a given tweet and then applies all of the cleaning functions previously defined. The third code block simply creates and adds to the set of stopwords that will be removed from the text. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99b1bd9",
   "metadata": {
    "papermill": {
     "duration": 0.033178,
     "end_time": "2024-06-10T15:38:18.207200",
     "exception": false,
     "start_time": "2024-06-10T15:38:18.174022",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"5b\"></a>\n",
    "## **Identifying Unique Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab6fa569",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T15:38:18.276899Z",
     "iopub.status.busy": "2024-06-10T15:38:18.276503Z",
     "iopub.status.idle": "2024-06-10T15:38:18.283641Z",
     "shell.execute_reply": "2024-06-10T15:38:18.282539Z"
    },
    "papermill": {
     "duration": 0.045462,
     "end_time": "2024-06-10T15:38:18.286498",
     "exception": false,
     "start_time": "2024-06-10T15:38:18.241036",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reopening 'nikelululemonadidas_tweets.jsonl' file in order to iterate through it again\n",
    "json_file = open('/kaggle/input/nikelululemonadidas-tweets-jsonl/nikelululemonadidas_tweets.jsonl', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c1f90685",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T15:38:18.356234Z",
     "iopub.status.busy": "2024-06-10T15:38:18.355809Z",
     "iopub.status.idle": "2024-06-10T15:44:18.949327Z",
     "shell.execute_reply": "2024-06-10T15:44:18.948050Z"
    },
    "papermill": {
     "duration": 360.632157,
     "end_time": "2024-06-10T15:44:18.952629",
     "exception": false,
     "start_time": "2024-06-10T15:38:18.320472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n"
     ]
    }
   ],
   "source": [
    "# Creating a dictionary of unique words, retrieved after processing the data\n",
    "\n",
    "# Creating empty dictionary\n",
    "unique_words = {}\n",
    "\n",
    "# Iterating through the json file\n",
    "for i, atweet in enumerate(json_file):\n",
    "    # Counting progress\n",
    "    if i % 10000 == 0:\n",
    "      print(i)\n",
    "    tweet_json = json.loads(atweet)\n",
    "    text = tweet_json['full_text']\n",
    "    # Natural language preprocessing, using the helper functions from above\n",
    "    tokens = text_preprocessing(text)\n",
    "    # Adding processed words to 'unique_words' dictionary\n",
    "    for aword in tokens:\n",
    "        if aword in unique_words:\n",
    "            unique_words[aword] += 1\n",
    "        if aword not in unique_words:\n",
    "            unique_words[aword] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b492195",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T15:44:19.025960Z",
     "iopub.status.busy": "2024-06-10T15:44:19.025559Z",
     "iopub.status.idle": "2024-06-10T15:44:19.033193Z",
     "shell.execute_reply": "2024-06-10T15:44:19.031949Z"
    },
    "papermill": {
     "duration": 0.047246,
     "end_time": "2024-06-10T15:44:19.035563",
     "exception": false,
     "start_time": "2024-06-10T15:44:18.988317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77718"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking out the length of 'unique_words'\n",
    "len(unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52560073",
   "metadata": {
    "papermill": {
     "duration": 0.035958,
     "end_time": "2024-06-10T15:44:19.106454",
     "exception": false,
     "start_time": "2024-06-10T15:44:19.070496",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "After cleaning, we end up with roughly 78,000 unique words. This is still a lot and will likely not translate to an intuitive network graph. To remedy this, I have opted to select only the words that appear at least 250 times (further below). In a sense, these words will represent the most common words used in the tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "08e6dbce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T15:44:19.179611Z",
     "iopub.status.busy": "2024-06-10T15:44:19.179119Z",
     "iopub.status.idle": "2024-06-10T15:44:19.228561Z",
     "shell.execute_reply": "2024-06-10T15:44:19.227364Z"
    },
    "papermill": {
     "duration": 0.088507,
     "end_time": "2024-06-10T15:44:19.231152",
     "exception": false,
     "start_time": "2024-06-10T15:44:19.142645",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nike',\n",
       " 'adidas',\n",
       " 'sneakerscouts',\n",
       " 'eneskanter',\n",
       " 'xbox',\n",
       " 'available',\n",
       " 'day',\n",
       " 'air',\n",
       " 'china',\n",
       " 'kingjames']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a sorted list of words\n",
    "sorted_counts = sorted(unique_words.items(), key=lambda item: item[1], reverse=True)\n",
    "sorted_words = [word for word, count in sorted_counts]\n",
    "# Checking the top 10 words in the unique word list\n",
    "sorted_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0410d619",
   "metadata": {
    "papermill": {
     "duration": 0.034928,
     "end_time": "2024-06-10T15:44:19.301186",
     "exception": false,
     "start_time": "2024-06-10T15:44:19.266258",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Above, we can see the top 10 most common words. Unsurprisingly, Nike is at the top of the list. This shouldn't be shocking, as we already know that Nike has the most mentions/interactions in this dataset. One thing to note is that Lululemon is the only major brand that is not found in the top 10 words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f1d6150a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T15:44:19.374351Z",
     "iopub.status.busy": "2024-06-10T15:44:19.373912Z",
     "iopub.status.idle": "2024-06-10T15:44:19.380528Z",
     "shell.execute_reply": "2024-06-10T15:44:19.379227Z"
    },
    "papermill": {
     "duration": 0.047721,
     "end_time": "2024-06-10T15:44:19.383926",
     "exception": false,
     "start_time": "2024-06-10T15:44:19.336205",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nike: 102733\n",
      "Adidas: 36256\n",
      "Lululemon: 6226\n"
     ]
    }
   ],
   "source": [
    "# Checking the word counts for the major brands\n",
    "print(\"Nike:\", unique_words[\"nike\"])\n",
    "print(\"Adidas:\", unique_words[\"adidas\"])\n",
    "print(\"Lululemon:\", unique_words[\"lululemon\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216f478e",
   "metadata": {
    "papermill": {
     "duration": 0.03566,
     "end_time": "2024-06-10T15:44:19.455576",
     "exception": false,
     "start_time": "2024-06-10T15:44:19.419916",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Above, we can see the total word counts for each brand. Again, Nike appears to have the highest word count, while Lululemon has the lowest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ad56250c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T15:44:19.528786Z",
     "iopub.status.busy": "2024-06-10T15:44:19.528356Z",
     "iopub.status.idle": "2024-06-10T15:44:19.560355Z",
     "shell.execute_reply": "2024-06-10T15:44:19.558915Z"
    },
    "papermill": {
     "duration": 0.072011,
     "end_time": "2024-06-10T15:44:19.563193",
     "exception": false,
     "start_time": "2024-06-10T15:44:19.491182",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating a set of words to include in the semantic network map\n",
    "words_to_include = set()\n",
    "word_count = 0\n",
    "\n",
    "# Selecting words used over 250 times to add to total number of unique words\n",
    "for aword in unique_words:\n",
    "    if unique_words[aword] > 250:\n",
    "        word_count += 1\n",
    "        words_to_include.add(aword)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db9ea24",
   "metadata": {
    "papermill": {
     "duration": 0.03745,
     "end_time": "2024-06-10T15:44:19.636156",
     "exception": false,
     "start_time": "2024-06-10T15:44:19.598706",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As mentioned above, we need to reduce the total number of words in order to create a better network graph. By including the most common words, i.e. words that appear over 250 times, we are left with roughly 900 words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb607e00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T15:44:19.710504Z",
     "iopub.status.busy": "2024-06-10T15:44:19.710076Z",
     "iopub.status.idle": "2024-06-10T15:44:19.716949Z",
     "shell.execute_reply": "2024-06-10T15:44:19.715425Z"
    },
    "papermill": {
     "duration": 0.046378,
     "end_time": "2024-06-10T15:44:19.719422",
     "exception": false,
     "start_time": "2024-06-10T15:44:19.673044",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "908\n"
     ]
    }
   ],
   "source": [
    "# Checking the number of filtered words to include\n",
    "print(len(words_to_include))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9286aa4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T15:44:19.792191Z",
     "iopub.status.busy": "2024-06-10T15:44:19.791775Z",
     "iopub.status.idle": "2024-06-10T15:44:19.799349Z",
     "shell.execute_reply": "2024-06-10T15:44:19.798059Z"
    },
    "papermill": {
     "duration": 0.047004,
     "end_time": "2024-06-10T15:44:19.801973",
     "exception": false,
     "start_time": "2024-06-10T15:44:19.754969",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.011683265138063254"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comparing number of words to include to intial number of unique words\n",
    "len(words_to_include)/len(unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc71d13",
   "metadata": {
    "papermill": {
     "duration": 0.036568,
     "end_time": "2024-06-10T15:44:19.875203",
     "exception": false,
     "start_time": "2024-06-10T15:44:19.838635",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "By comparing our filtered word list with the total number of unique words, we can see that we have reduced the original number to roughly 1%. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c59b70",
   "metadata": {
    "papermill": {
     "duration": 0.035346,
     "end_time": "2024-06-10T15:44:19.946187",
     "exception": false,
     "start_time": "2024-06-10T15:44:19.910841",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"5c\"></a>\n",
    "## **Creating the Graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d5cf1ca8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T15:44:20.019056Z",
     "iopub.status.busy": "2024-06-10T15:44:20.018623Z",
     "iopub.status.idle": "2024-06-10T15:44:20.024968Z",
     "shell.execute_reply": "2024-06-10T15:44:20.023682Z"
    },
    "papermill": {
     "duration": 0.046156,
     "end_time": "2024-06-10T15:44:20.027592",
     "exception": false,
     "start_time": "2024-06-10T15:44:19.981436",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Reopening 'nikelululemonadidas_tweets.jsonl' file in order to iterate through it again\n",
    "json_file = open('/kaggle/input/nikelululemonadidas-tweets-jsonl/nikelululemonadidas_tweets.jsonl', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "63415849",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T15:44:20.103057Z",
     "iopub.status.busy": "2024-06-10T15:44:20.102647Z",
     "iopub.status.idle": "2024-06-10T15:44:20.107970Z",
     "shell.execute_reply": "2024-06-10T15:44:20.106587Z"
    },
    "papermill": {
     "duration": 0.047215,
     "end_time": "2024-06-10T15:44:20.110542",
     "exception": false,
     "start_time": "2024-06-10T15:44:20.063327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepraring a graph\n",
    "Semantic_Graph = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "888a3466",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T15:44:20.184659Z",
     "iopub.status.busy": "2024-06-10T15:44:20.184260Z",
     "iopub.status.idle": "2024-06-10T15:50:33.288148Z",
     "shell.execute_reply": "2024-06-10T15:50:33.286885Z"
    },
    "papermill": {
     "duration": 373.144016,
     "end_time": "2024-06-10T15:50:33.291110",
     "exception": false,
     "start_time": "2024-06-10T15:44:20.147094",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n"
     ]
    }
   ],
   "source": [
    "# Creating the graph, this is roughly the same as for the Twitter mentions graph\n",
    "\n",
    "# Iterating through the json file\n",
    "for i, atweet in enumerate(json_file):\n",
    "    # Tracking progress\n",
    "    if i % 10000 == 0:\n",
    "      print(i)\n",
    "    tweet_json = json.loads(atweet)\n",
    "    text = tweet_json['full_text']\n",
    "    # Cleaning the text with the helper functions\n",
    "    tokens = text_preprocessing(text)\n",
    "    nodes = [t for t in tokens if t in words_to_include]\n",
    "    if len(nodes) > 0:\n",
    "      # Looking for cooccurences of 2\n",
    "      cooccurrences = itertools.combinations(nodes, 2)\n",
    "      # Iterating through all the combinations\n",
    "      for c in cooccurrences:\n",
    "        if c[0] != c[1]:\n",
    "          # Adding the tuples of words as edges to the graph\n",
    "          if Semantic_Graph.has_edge(c[0], c[1]):\n",
    "            # Incrementing the weight if the edge exists\n",
    "            Semantic_Graph[c[0]][c[1]]['weight'] += 1\n",
    "          else:\n",
    "            # Adjusting the weight if the edge doesn't exist\n",
    "            Semantic_Graph.add_edge(c[0], c[1], weight=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "adcfc4b0",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-06-10T15:50:33.371173Z",
     "iopub.status.busy": "2024-06-10T15:50:33.370639Z",
     "iopub.status.idle": "2024-06-10T15:50:33.380208Z",
     "shell.execute_reply": "2024-06-10T15:50:33.378901Z"
    },
    "papermill": {
     "duration": 0.053977,
     "end_time": "2024-06-10T15:50:33.384844",
     "exception": false,
     "start_time": "2024-06-10T15:50:33.330867",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "##### Graph Summary #####\n",
      "number of nodes: 908\n",
      "number of edges: 207401\n",
      "\n",
      "nodes: ['nike', \"women's\", 'air', 'uptempo', 'white', 'yellow', 'available', 'footlocker', 'sneakerscouts', 'adidas', 'lasership', 'stealing', 'work', 'home', 'alert', 'next', 'collab', 'dropping', 'ad', 'space', 'low', 'snipes_usa', 'snkrs', 'get', 'nikebasketball', 'puma', 'stock', 'partnership', 'helped', 'grow', 'etnow', 'team', 'release', 'jordan', 'real', 'support', 'friend', 'family', 'sick', 'kaya_alexander5', 'nikestore', 'sneakeradmirals', 'awesome', 'pair', 'lot', 'wait', 'dress', 'game', 'jumpman23', 'usnikefootball', 'usarmy', 'britisharmy', 'cnn', 'wheeloffortune', 'bloombergradio', 'wbpictures', 'disneystudios', 'wgci', 'hot97', 'v103', 'tmz', 'harveylevintmz', 'instagram', 'ebay', 'amazon', 'abc', 'abc7chicago', 'chicago_police', 'nypdnews', 'usairforce', 'usmc', 'usnavy', 'royalnavy', 'royalairforce', 'jack', 'finkd', 'priscillachanz', 'business', 'fbi', 'beccadiamondxx', 'potus', 'vp', 'drbiden44', 'bet', 'max', 'japan', 'ht', 'dunk', 'beautiful', 'fuck', 'working', 'sizeofficial', 'adidasus', 'adidasoriginals', 'look', 'problem', 'wearing', 'say', 'drop', 'time', 'vapormax', 'flyknit', 'royal', 'finishline', 'young', 'thetraeyoung', 'much', 'people', 'like', 'nicekicks', 'snkr_twitr', 'iamtmcii', 'good', 'edition', 'global', 'sport', 'week', 'take', 'place', 'black', 'lol', 'apple', 'think', 'corporation', 'care', 'latest', 'underarmour', 'deal', 'run', 'company', 'jersey', 'win', 'guy', 'see', 'way', 'keep', 'play', 'top', 'going', 'year', 'thank', 'great', 'getting', 'started', 'moment', 'connected', 'show', 'understand', \"they're\", 'making', 'tweet', 'want', 'others', 'word', 'kingjames', 'start', 'dude', 'blazer', 'suede', 'red', 'need', 'sneaker', 'day', 'color', 'style', 'code', 'date', 'price', 'sneakerhead', 'fashion', 'yes', 'know', 'free', 'used', 'celebrating', 'birthday', 'discount', 'item', 'thanks', 'given', 'something', 'trying', 'seen', 'issue', 'hand', 'let', 'make', 'sure', 'love', 'school', 'student', 'buy', 'quality', 'everyone', 'job', 'theestallion', 'thee', 'app', 'morning', 'history', 'step', 'go', 'shoe', 'part', 'facility', 'forum', 'mid', 'bot', 'sold', 'summit', 'bright', 'wolf', 'grey', 'hey', 'cool', 'pay', 'dropped', 'exclusive', 'access', 'long', 'member', 'got', 'account', 'brand', 'new', 'customer', 'bought', 'blue', 'month', 'light', 'cocacola', 'turn', 'join', 'please', 'city', 'lime', 'glow', 'kdtrey5', 'coach', 'sponsor', 'use', 'steal', 'package', 'delivery', 'put', 'left', 'le', 'minute', 'delivered', 'lululemon', 'bill', 'happy', 'friday', 'strong', 'weekend', 'jacket', 'deep', \"we're\", 'giving', 'store', 'head', 'wanted', 'hi', 'running', 'force', 'email', 'thing', \"i'm\", 'woman', 'god', 'service', 'idea', 'night', 'gonna', 'waiting', 'old', 'son', 'first', 'tv', 'espn', 'appreciate', 'wnba', 'help', 'change', 'move', 'kicksonfire', 'bro', 'best', 'custom', 'candace_parker', 'leader', 'human', 'right', 'order', 'congrats', 'manager', 'justdoit', 'pixel', 'se', 'gear', 'partner', 'wild', 'retail', 'http', 'second', 'season', 'yeah', 'bad', 'one', 'ice', 'soccer', 'ask', 'thought', 'kit', 'lfc', 'line', 'match', 'sock', 'york', 'looking', 'man', 'many', 'size', 'last', 'chance', 'said', 'wow', 'club', 'shop', 'shirt', 'law', 'tell', \"he's\", 'remember', 'decided', 'campaign', 'link', 'read', 'story', 'today', 'nikenyc', 'girl', 'point', 'smh', 'ni', 'continue', 'stay', 'boot', 'camp', 'swoosh', 'medium', 'em', 'hit', 'nice', 'add', 'tried', 'different', 'product', 'stop', 'adidasrunning', 'adidasfballus', 'adidashoops', 'htt', 'talking', 'shit', 'several', 'money', 'actual', 'virtual', 'event', 'fly', 'israel', 'glad', 'snkrfrkrmag', 'atmos_usa', 'theshoegame', 'ticket', 'sale', 'la', 'tomorrow', 'gatorade', 'ordered', 'came', 'site', 'thesocialstatus', \"that's\", 'starting', 'dick', 'detail', 'yall', 'big', 'wtf', 'incredible', 'hippie', 'foot', 'fontanka', 'pink', 'find', 'kotd', 'yoursneakersaredope', 'kind', 'safe', 'exploring', 'lebron', 'james', 'college', 'wish', 'athlete', 'early', 'video', 'neutral', 'olive', 'travis', 'kick', 'fix', 'fan', 'wear', 'total', 'classic', 'green', 'selling', 'special', 'check', 'icon', 'able', 'follow', 'twitter', 'sb', 'box', 'football', 'believe', 'done', 'community', 'chicago', 'try', 'amazing', 'luck', 'life', 'short', 'holiday', 'face', 'talk', 'buying', 'pant', 'boy', 'pushed', 'send', 'ig', 'academy', 'woke', 'hoodie', 'favorite', 'orange', 'congratulation', 'feel', 'social', 'message', 'supply', 'crazy', 'tho', 'corporate', 'become', 'model', 'rest', 'world', 'pepsi', 'online', 'paid', 'shipping', 'stuff', 'found', 'bring', 'back', 'christmas', 'uniform', 'report', 'high', 'forced', 'chinese', 'shut', 'guess', 'fire', 'watch', 'nikegolf', 'ball', 'nikefootball', 'pathetic', 'made', 'court', 'retro', 'modern', 'purchase', 'seeing', 'program', 'phil', 'knight', 'street', 'university', 'name', 'image', 'rule', 'official', 'clean', 'chicksinkicks2', 'shot', 'hour', 'soleguru', 'welcome', 'converse', 'slave', 'labor', 'clothing', \"can't\", 'number', 'gift', 'suck', 'biggest', 'person', 'innovation', 'fam', 'beiberlove69', 'stand', 'mean', 'wrong', 'type', 'question', 'original', 'come', 'wonder', 'speak', 'update', 'cause', 'website', 'triple', 'better', 'sell', 'im', 'purple', 'dawn', 'fedex', 'worst', 'fit', 'profit', 'called', 'consumersfirst', 'dm', 'took', 'design', 'example', 'kanyewest', 'cop', 'collection', 'content', 'super', 'dope', 'lost', 'ups', 'ship', 'walmart', 'refund', 'mind', 'logo', 'pack', 'heard', 'give', 'card', 'fun', 'true', 'mine', \"i've\", 'hope', 'meet', 'pick', 'gotta', 'dollar', 'american', 'fresh', 'state', 'boost', 'tech', 'side', 'griffey', 'future', 'controlled', 'return', 'netflix', 'present', 'nikeservice', 'saying', 'draw', 'passing', 'post', 'someone', 'lexxdaturtle', 'charger', 'thinking', 'commercial', 'vadriano2000', 'full', 'happen', 'damn', 'wanna', 'whole', 'anyone', 'baby', 'kid', 'hard', 'huge', 'perfect', 'hell', 'prize', 'taking', 'sorry', 'unlvfootball', 'uniswag', 'marketing', 'asking', 'nfl', 'chose', 'fact', 'nothing', 'raffle', 'leading', 'player', \"let's\", 'men', 'dear', 'clothes', 'missing', 'little', 'course', 'opening', 'ok', 'public', 'photo', 'end', 'child', 'hold', 'seems', 'sound', 'using', 'jamarrobrown', 'book', 'navy', 'nikesb', 'saw', 'loved', 'hear', 'ya', 'dream', 'as', 'nft', 'gold', 'collaboration', 'anything', 'sent', 'hate', 'playing', 'exec', 'call', 'reason', 'celebrate', 'newbalance', 'supporting', 'boston', 'least', 'share', 'mcdonalds', 'trash', 'went', 'yo', 'live', 'set', 'coming', 'solefed', 'apparel', 'anniversary', 'excited', 'midnight', 'china', 'employee', 'told', 'pic', 'jadendaly', 'target', 'fucking', 'worker', 'received', 'opportunity', 'boycott', 'tonight', 'listen', 'bit', 'matter', 'michael', 'trvisxx', 'nba', 'celtic', 'sign', 'country', 'meeting', 'proud', 'including', 'goal', 'canada', 'final', 'happened', 'lsoshipping', 'yesterday', 'agree', 'launch', 'basketball', 'ready', 'dirtydetty9381', 'nfts', 'phone', 'yeezy', 'list', 'impossibleisnothing', 'rock', 'speaking', 'donaldjtrumpjr', 'comment', 'maniere_usa', 'fast', 'kaepernick7', 'joshuajhan', 'xbox', 'december', 'bag', 'oh', 'dark', 'cosmic', 'unity', 'everything', 'solelinks', 'miss', 'introducing', 'experience', 'announced', 'act', 'foamposite', 'calling', '_talkswithtj', 'adizero', 'runningshoesgur', 'runningshoes', 'giveaway', 'festive', 'signed', 'contract', 'fuel', 'credit', 'major', 'holy', 'inspire', 'reebok', 'teamcanada', 'impossible', 'slavery', 'inspired', 'open', 'project', 'olympics', 'create', 'spidadmitchell', 'respect', 'lmao', 'league', 'patta_nl', 'news', 'recent', 'america', 'jdsports', 'culture', 'enter', 'owner', 'market', 'eliudkipchoge', 'grail', 'boardroom', 'action', 'brilliant', 'blocked', 'djbluiz', 'changing', 'baseball', 'weareivypark', 'turtlepace5', 'shame', 'november', 'uno', 'region', 'deezle148', '210gotkickz', 'voice', 'on-court', 'repjimbanks', 'joebiden', 'billboard', 'digital', 'copping', 'xinjiang', 'david', 'xbox20', 'metaverse', 'analyst', 'entire', 'aaron', 'justintrudeau', 'relationship', 'king', 'de', 'dashiexp', 'yooo', 'imagine', 'import', 'banned', 'web', 'twotimesprime', 'uyghur', 'mindless_bmd', 'ayeeeee', 'postseason', 'statefarm', 'respond', 'ban', 'sneakerhd84', 'senator', 'centre', 'entering', 'beyonce', 'nov', 'backpack', 'solesavy', 'giannis_an34', 'twelve', 'thunder', 'hero', 'roblox', 'crater', 'donovan', 'mitchell', 'austinekeler', 'peo', 'michael_fabiano', 'slam', 'yardrunner', 'generation', 'easportsfifa', 'lied', 'heartbroken', 'acquired', 'duffel', 'taeyong', 'lazylionsnft', 'sephora', 'xboxsweepstakes', 'hyperkin', 'auxgod_', 'stern', 'bred', 'realunogame', 'plane', 'ingloriousguido', 'eneskanter', 'kanter', 'malcolmjackso20', 'recognizes', 'rtfktstudios', 'ene', 'meta', 'enduyghurforcedlabor', 'eth', 'uninterrupted', 'prevention', 'boredapeyc', 'garcons', 'brooklynsown90', 'graduation', 'scctradingcards', 'topps', 'virgil', 'lagalaxy', 'psg_english', 'elizabeth_that', 'rodgers', 'richsignorelli', 'techinsider', \"abloh's\", 'sen', 'nikeland', 'funneled', 'throne', 'nba_newyork', 'pre-day', 'enesfreedom', 'clonex', 'rtfkt', 'invsblefriends', 'footydotcom_', \"footy's\", 'zaptio', 'benitopagotto', 'spacerunnersnft', 'ronwyden', 'septe', 'maybes']\n",
      "\n",
      "neighbors of adidas: ['available', 'puma', 'stock', 'partnership', 'helped', 'grow', 'nike', 'real', 'get', 'support', 'friend', 'family', 'adidasus', 'adidasoriginals', 'look', 'problem', 'wearing', 'say', 'drop', 'young', 'thetraeyoung', 'much', 'people', 'like', 'lol', 'deal', 'black', 'run', 'company', 'thank', 'great', 'team', 'getting', 'started', 'dude', 'red', 'need', 'jersey', 'yes', 'start', 'know', 'free', 'used', 'history', 'step', 'play', 'forum', 'mid', 'bot', 'sold', 'bought', 'pair', 'blue', 'month', 'light', 'please', 'next', 'city', 'long', 'beautiful', 'sneaker', 'love', 'new', 'grey', 'year', 'wanted', 'thanks', 'service', 'shoe', 'keep', 'idea', 'night', 'gonna', 'human', 'right', 'brand', 'make', 'sure', 'gear', 'see', 'got', 'one', 'good', 'day', 'top', 'trying', 'law', 'tell', 'let', \"he's\", 'think', 'dropping', 'today', 'way', 'smh', 'snkr_twitr', 'adidasrunning', 'adidasfballus', 'adidashoops', \"we're\", 'glad', 'em', 'want', 'collab', 'tomorrow', 'man', 'show', 'site', 'wish', 'shop', 'underarmour', 'apple', 'awesome', 'first', 'athlete', 'size', 'fix', 'check', 'icon', 'able', 'believe', 'early', 'done', 'home', 'kit', 'follow', 'instagram', 'http', 'morning', 'try', 'amazing', 'luck', 'help', 'life', 'fan', 'buying', 'academy', 'shirt', 'woke', 'order', 'wait', 'online', 'fire', 'work', 'watch', 'boy', 'looking', 'fly', 'court', 'season', 'retro', 'modern', 'time', 'store', 'purchase', 'official', 'face', 'win', 'guess', 'total', 'shot', 'tweet', 'hour', 'sale', 'price', 'clothing', 'question', 'original', 'program', 'point', 'world', 'better', 'hey', 'going', 'im', 'big', 'thought', 'dm', 'welcome', 'many', 'design', 'example', 'kanyewest', 'mind', 'ordered', 'mine', \"i've\", \"i'm\", 'old', 'head', 'hope', 'meet', 'pick', 'take', 'care', 'gotta', 'find', 'state', 'white', 'boost', 'tech', 'side', 'give', 'future', 'controlled', 'return', 'passing', 'post', 'thinking', 'commercial', 'guy', 'weekend', 'wanna', 'ask', 'girl', 'wild', 'card', 'prize', 'working', 'seen', 'fun', 'collection', \"that's\", 'nice', 'sell', 'college', 'move', 'several', 'style', 'feel', 'sock', 'dress', 'hoodie', \"women's\", 'pant', 'lasership', 'delivered', 'package', 'customer', 'saying', 'money', 'waiting', 'actual', 'cocacola', 'nft', 'favorite', 'exec', 'sport', 'school', 'hard', 'bring', 'happy', 'birthday', 'last', 'newbalance', 'converse', 'supporting', 'boston', 'understand', 'sorry', 'everyone', 'put', 'dope', 'jacket', 'excited', 'share', 'coach', 'clean', \"let's\", 'buy', 'stop', 'video', 'something', 'thing', 'missing', 'made', 'green', 'player', 'tonight', 'super', 'high', 'talking', 'message', 'code', 'app', 'gold', 'collaboration', 'china', 'campaign', 'ad', 'asking', 'photo', 'ice', 'rule', 'sneakerhead', 'snipes_usa', 'running', 'fit', 'whole', 'color', 'minute', 'saw', 'release', 'bro', 'walmart', 'target', 'set', 'le', 'football', 'short', 'taking', 'stay', 'ready', 'using', 'reason', 'seems', 'nfts', 'pay', 'join', 'kick', 'impossibleisnothing', 'employee', 'best', 'found', 'different', 'jumpman23', 'nfl', 'nba', 'bag', 'full', 'stand', 'said', 'corporation', \"can't\", 'wear', 'product', 'second', 'game', 'ups', 'paid', 'pack', 'phone', 'lost', 'hell', 'website', 'pic', 'send', 'retail', 'kid', 'fucking', 'sick', 'mean', 'perfect', 'happen', 'shipping', 'worst', 'shit', 'suck', 'speak', 'uniform', 'someone', 'crazy', 'announced', 'cool', 'live', 'street', 'classic', 'huge', 'cause', 'adizero', 'runningshoesgur', 'runningshoes', 'giveaway', 'week', 'back', 'part', 'canada', 'public', 'come', 'opportunity', 'god', 'kind', 'box', 'call', 'matter', 'line', 'bit', 'others', 'twitter', 'inspire', 'reebok', 'gatorade', 'kotd', 'yoursneakersaredope', 'safe', 'took', 'congrats', 'email', 'use', 'lot', 'discount', 'go', 'bad', 'celebrate', 'change', 'impossible', 'nothing', 'remember', 'name', 'anything', 'match', 'orange', 'member', 'experience', 'exclusive', 'went', 'goal', 'congratulation', 'turn', 'amazon', 'proud', 'add', 'number', 'damn', 'true', 'basketball', 'giving', 'sponsor', 'clothes', 'soccer', 'link', 'ok', 'course', 'open', 'nikebasketball', 'woman', 'enter', 'corporate', 'global', 'fedex', 'happened', 'delivery', 'yeezy', 'steal', 'stuff', 'hand', 'refund', 'wtf', 'nikestore', 'kdtrey5', 'owner', 'date', 'detail', 'ig', 'social', 'medium', 'low', 'quality', 'apparel', 'blocked', 'edition', 'making', 'hi', 'marketing', 'tv', 'business', 'mcdonalds', 'selling', 'foot', 'virtual', 'seeing', 'yo', 'lmao', 'called', 'content', 'dream', 'pink', 'rock', 'everything', 'comment', 'ball', 'november', 'footlocker', 'story', 'ni', 'partner', 'gift', 'son', 'unity', 'uno', 'xbox', 'tho', 'news', 'alert', 'pepsi', 'market', 'recent', 'country', 'tried', 'trash', 'read', 'community', 'club', 'book', 'holiday', 'type', 'little', 'become', 'account', 'as', 'innovation', 'boot', 'brilliant', 'special', 'report', 'sneakeradmirals', 'end', 'sound', 'calling', 'chose', '_talkswithtj', 'told', 'nikenyc', 'respect', 'item', 'issue', 'came', 'lululemon', 'anyone', 'james', 'vp', 'fuck', 'xbox20', 'anniversary', 'ebay', \"they're\", 'neutral', 'celebrating', 'coming', 'cop', 'inspired', 'launch', 'wrong', 'list', 'aaron', 'wow', 'fresh', 'la', 'king', 'ticket', 'signed', 'nicekicks', 'iamtmcii', 'beiberlove69', 'including', 'fact', 'word', 'air', 'event', 'dashiexp', 'yooo', 'sent', 'culture', 'baby', 'yall', 'logo', 'lime', 'yeah', 'jack', 'fam', 'candace_parker', 'least', 'model', 'hold', 'holy', 'credit', 'america', 'left', 'jordan', 'force', 'american', 'changing', 'fashion', 'wnba', 'michael', 'latest', 'hit', 'job', 'sizeofficial', 'banned', 'received', 'israel', 'men', 'uyghur', 'slave', 'labor', 'region', 'wonder', 'miss', 'person', 'appreciate', 'billboard', 'ban', 'dropped', 'exploring', 'dear', 'netflix', 'navy', 'espn', 'royal', 'purple', 'draw', 'space', 'ship', 'summit', 'chance', 'child', 'place', 'given', 'olympics', 'worker', 'playing', 'starting', 'weareivypark', 'beyonce', 'forced', 'supply', 'sign', 'entering', 'bet', 'oh', 'introducing', 'heard', 'contract', 'baseball', 'dick', 'update', 'yesterday', 'nov', 'continue', 'decided', 'solelinks', 'league', 'theestallion', 'cnn', 'moment', 'talk', 'final', 'stealing', 'major', 'max', 'statefarm', 'christmas', 'student', 'fast', 'snkrs', 'image', 'digital', 'eliudkipchoge', 'dark', 'hate', 'create', 'sb', 'rest', 'dunk', 'knight', 'chicago', 'triple', 'spidadmitchell', 'phil', 'dollar', 'relationship', 'ya', 'pathetic', 'leader', 'pushed', 'hear', 'raffle', 'access', 'agree', 'university', 'action', 'donovan', 'mitchell', 'friday', 'austinekeler', 'respond', 'peo', 'charger', 'michael_fabiano', 'shut', 'atmos_usa', 'leading', 'deep', 'chinese', 'kicksonfire', 'bill', 'strong', 'solefed', 'imagine', 'incredible', 'generation', 'easportsfifa', 'kingjames', 'entire', 'biggest', 'custom', 'present', 'japan', 'opening', 'project', 'taeyong', 'se', 'hero', 'xboxsweepstakes', 'hyperkin', 'glow', 'act', 'travis', 'roblox', 'plane', 'nikefootball', 'bright', 'speaking', 'turtlepace5', 'grail', 'boycott', 'ingloriousguido', 'shame', 'loved', 'lexxdaturtle', 'deezle148', 'finishline', 'celtic', 'blazer', 'heartbroken', 'meeting', 'ht', 'usnikefootball', 'slam', 'swoosh', 'recognizes', 'soleguru', 'jdsports', 'manager', 'htt', 'eneskanter', 'nikesb', 'nikeservice', 'justdoit', 'kanter', 'realunogame', 'giannis_an34', 'solesavy', 'sephora', 'lfc', 'uniswag', 'lagalaxy', 'topps', 'profit', 'listen', 'boredapeyc', 'kaepernick7', 'york', 'pixel', 'december', 'fbi', 'rodgers', 'yellow', 'meta', 'backpack', 'tmz', 'copping', 'dawn', 'suede', 'trvisxx', 'centre', 'richsignorelli', 'lied', 'techinsider', 'analyst', 'david', 'web', 'camp', 'lebron', 'usnavy', 'usmc', 'usarmy', 'usairforce', '210gotkickz', 'metaverse', 'eth', 'twotimesprime', 'flyknit', 'acquired', 'rtfktstudios', 'thunder', 'kaya_alexander5', 'facility', 'jadendaly', 'abc', 'vadriano2000', 'senator', 'midnight', 'voice', 'britisharmy', 'wheeloffortune', 'bloombergradio', 'wbpictures', 'disneystudios', 'wgci', 'hot97', 'v103', 'harveylevintmz', 'etnow', 'abc7chicago', 'chicago_police', 'nypdnews', 'royalnavy', 'royalairforce', 'finkd', 'priscillachanz', 'beccadiamondxx', 'potus', 'connected', 'drbiden44', 'import', 'olive', 'foamposite', 'lazylionsnft', 'de', 'nikeland', 'clonex', 'nikegolf', 'boardroom', 'joebiden', 'scctradingcards', 'psg_english', 'sen', 'snkrfrkrmag', 'invsblefriends', 'maniere_usa', 'thee', 'stern', 'duffel', 'benitopagotto', 'rtfkt', 'funneled', 'fuel', 'bred', 'graduation', 'slavery', 'maybes', 'on-court', 'jamarrobrown', 'thesocialstatus']\n",
      "neighbors of nike: [\"women's\", 'air', 'uptempo', 'white', 'yellow', 'available', 'footlocker', 'sneakerscouts', 'lasership', 'stealing', 'work', 'home', 'alert', 'next', 'collab', 'dropping', 'snkrs', 'get', 'nikebasketball', 'puma', 'stock', 'partnership', 'helped', 'grow', 'adidas', 'etnow', 'team', 'release', 'jordan', 'sick', 'lot', 'wait', 'dress', 'usarmy', 'britisharmy', 'cnn', 'wheeloffortune', 'bloombergradio', 'wbpictures', 'disneystudios', 'wgci', 'hot97', 'v103', 'tmz', 'harveylevintmz', 'instagram', 'ebay', 'amazon', 'abc', 'abc7chicago', 'chicago_police', 'nypdnews', 'usairforce', 'usmc', 'usnavy', 'royalnavy', 'royalairforce', 'jack', 'finkd', 'priscillachanz', 'business', 'fbi', 'beccadiamondxx', 'potus', 'vp', 'drbiden44', 'bet', 'ad', 'max', 'japan', 'snipes_usa', 'ht', 'dunk', 'beautiful', 'sizeofficial', 'time', 'vapormax', 'flyknit', 'royal', 'finishline', 'jumpman23', 'sneakeradmirals', 'nicekicks', 'snkr_twitr', 'iamtmcii', 'look', 'good', 'black', 'apple', 'think', 'people', 'corporation', 'care', 'latest', 'underarmour', 'working', 'win', 'guy', 'see', 'way', 'keep', 'play', 'say', 'top', 'going', 'year', 'moment', 'connected', 'show', 'understand', \"they're\", 'making', 'tweet', 'want', 'others', 'space', 'low', 'kingjames', 'edition', 'start', 'blazer', 'suede', 'getting', 'celebrating', 'birthday', 'discount', 'item', 'thanks', 'given', 'something', 'trying', 'seen', 'issue', 'hand', 'go', 'shoe', 'part', 'facility', 'support', 'summit', 'bright', 'wolf', 'grey', 'cocacola', 'turn', 'join', 'problem', 'lime', 'glow', 'kdtrey5', 'new', 'coach', 'sponsor', 'use', 'company', 'steal', 'package', 'delivery', 'put', 'left', 'le', 'minute', 'delivered', 'week', 'deep', 'know', 'thing', \"i'm\", 'woman', 'day', 'god', 'waiting', 'red', 'let', 'order', 'congrats', 'customer', 'manager', 'justdoit', 'force', 'pixel', 'se', 'happy', 'partner', 'wild', 'retail', 'http', 'second', 'season', 'yeah', 'bad', 'lol', 'ice', 'soccer', 'store', 'ask', 'thought', 'kit', 'lfc', 'real', 'line', 'match', 'sock', 'york', 'game', 'custom', 'jersey', 'man', 'many', 'pair', 'size', 'last', 'chance', 'said', 'sold', 'wow', 'club', 'shop', 'shirt', 'please', 'remember', 'app', 'nikenyc', 'girl', 'point', 'blue', 'night', 'ni', 'love', 'swoosh', 'htt', 'today', 'several', 'job', 'make', 'money', 'actual', 'virtual', 'event', 'take', 'link', 'israel', 'right', 'snkrfrkrmag', 'atmos_usa', 'theshoegame', 'great', 'nikestore', 'gatorade', 'dick', 'sure', 'detail', 'incredible', 'global', 'brand', 'hippie', 'light', 'fontanka', 'pink', 'need', 'medium', 'city', 'exploring', 'neutral', 'olive', 'stop', 'giving', 'travis', 'kick', 'buy', 'total', 'classic', 'green', 'college', 'started', 'sport', 'story', 'check', 'sb', 'box', 'sneaker', 'thank', 'pant', 'boy', 'got', 'pushed', 'send', 'hoodie', 'favorite', 'color', 'orange', 'product', 'corporate', 'become', 'model', 'rest', 'world', 'pepsi', 'shipping', 'found', 'bring', 'back', 'christmas', 'uniform', 'report', 'high', 'price', 'forced', 'chinese', 'shut', 'woke', 'best', 'nikegolf', 'ball', 'nikefootball', 'pathetic', 'made', 'big', 'able', 'seeing', 'program', 'purchase', 'school', 'luck', 'phil', 'knight', 'street', 'university', 'athlete', 'name', 'image', 'rule', 'chicksinkicks2', 'much', 'kaya_alexander5', 'soleguru', 'welcome', 'converse', 'holiday', 'slave', 'labor', \"can't\", 'number', 'gift', 'suck', 'innovation', 'fam', 'beiberlove69', 'quality', 'stand', 'mean', 'wrong', 'wish', 'type', 'come', 'em', 'wonder', 'cause', 'website', 'selling', 'lebron', 'triple', 'sell', 'run', 'purple', 'dawn', 'fan', 'fedex', 'worst', 'help', 'foot', 'fit', 'profit', 'called', 'cop', 'believe', 'collection', 'content', 'super', 'dope', 'bro', 'lost', 'ups', 'service', 'ship', 'walmart', 'refund', 'awesome', 'logo', 'pack', 'fun', 'true', 'watch', 'looking', 'give', 'fuck', 'dollar', 'american', 'buying', 'shit', 'fresh', 'griffey', 'tell', 'netflix', 'campaign', 'present', 'add', 'draw', 'lexxdaturtle', 'gonna', 'charger', 'vadriano2000', 'full', 'community', 'one', 'ordered', 'happen', 'ig', 'nice', 'damn', 'paid', 'whole', 'appreciate', 'free', 'anyone', 'word', 'strong', 'baby', 'amazing', 'hell', 'mine', 'weekend', 'tried', 'unlvfootball', 'usnikefootball', 'uniswag', 'hi', 'marketing', 'month', 'done', 'theestallion', 'raffle', 'long', 'player', 'im', 'clothes', 'nothing', 'missing', 'speak', 'retro', 'little', 'fashion', 'sale', 'online', 'photo', 'follow', 'end', 'child', 'question', 'gear', 'nfl', 'first', 'place', 'seems', 'fire', 'icon', 'yes', 'nikeservice', 'book', 'navy', 'cool', 'nikesb', 'saw', 'loved', 'old', 'bought', 'talk', 'social', 'glad', 'pay', 'hard', 'try', 'fix', 'gold', 'wearing', 'anything', 'sent', 'hate', 'playing', 'call', 'yall', 'gotta', 'video', 'feel', 'deal', \"that's\", 'newbalance', 'took', 'hit', 'least', \"we're\", 'life', 'share', 'design', 'hey', 'mcdonalds', 'trash', 'dude', 'wear', 'went', 'yo', 'face', 'person', 'used', 'talking', 'better', 'reason', 'stuff', 'friday', 'set', 'coming', 'solefed', 'kicksonfire', 'son', 'apparel', 'anniversary', 'kid', 'excited', 'change', 'live', 'midnight', 'everyone', 'yoursneakersaredope', 'kotd', 'tomorrow', 'head', 'wanted', 'china', 'employee', 'told', 'crazy', 'came', 'shot', 'dream', 'clean', 'jamarrobrown', 'jadendaly', 'target', 'bill', 'fucking', 'find', 'style', 'opportunity', 'boycott', 'tv', 'commercial', 'running', 'state', 'fact', 'modern', 'football', 'listen', 'saying', 'michael', 'trvisxx', 'nba', 'celtic', 'including', 'meet', 'goal', 'sneakerhead', 'canada', 'someone', 'worker', 'move', 'young', 'final', 'opening', 'dropped', 'country', 'hope', 'site', 'men', 'agree', 'original', 'future', 'jacket', 'different', 'launch', 'exclusive', 'sign', 'course', 'twitter', 'basketball', 'academy', 'family', 'bot', 'taking', 'dirtydetty9381', 'ok', 'post', 'sorry', 'as', 'phone', 'member', 'thinking', 'guess', 'list', 'using', 'hour', 'supporting', 'drop', 'yesterday', 'donaldjtrumpjr', 'comment', 'maniere_usa', 'chicago', 'fast', 'kaepernick7', 'morning', 'ready', 'idea', 'joshuajhan', 'xbox', 'tho', 'adidasus', 'perfect', 'hear', 'december', 'kind', 'example', 'return', 'huge', 'dear', 'dark', 'wanna', 'asking', 'friend', 'cosmic', 'unity', 'official', 'miss', 'stay', 'special', 'dm', 'introducing', 'experience', 'thee', 'decided', 'short', 'access', 'everything', 'act', \"i've\", 'foamposite', 'date', 'calling', '_talkswithtj', 'like', 'festive', 'continue', 'read', 'fuel', 'credit', 'oh', 'hold', 'major', 'card', 'message', 'congratulation', 'holy', 'inspire', 'reebok', 'james', 'boston', 'student', 'email', 'espn', 'safe', 'biggest', 'nfts', \"he's\", 'camp', 'prize', 'la', 'inspired', \"let's\", 'wnba', 'open', 'court', 'project', 'olympics', 'pick', 'spidadmitchell', 'contract', 'leading', 'clothing', 'league', 'patta_nl', 'news', 'ya', 'early', 'meeting', 'boot', 'step', 'america', 'jdsports', 'culture', 'mid', 'side', 'create', 'announced', 'owner', 'market', 'wtf', 'eliudkipchoge', 'grail', 'received', 'solelinks', 'pic', 'tech', 'collaboration', 'brilliant', 'respect', 'adidasoriginals', 'update', 'djbluiz', 'changing', 'giveaway', 'smh', 'baseball', 'weareivypark', 'bit', 'turtlepace5', 'account', 'celebrate', 'sound', 'mind', 'shame', 'bag', 'november', 'matter', 'region', 'happened', 'nft', 'uno', 'starting', 'blocked', 'recent', 'lmao', 'heard', 'signed', 'voice', 'on-court', 'repjimbanks', 'supply', 'joebiden', 'proud', 'billboard', 'lsoshipping', 'rock', 'boardroom', '210gotkickz', 'xinjiang', 'david', 'fly', 'controlled', 'history', 'lululemon', 'analyst', 'law', 'enter', 'deezle148', 'justintrudeau', 'digital', 'relationship', 'de', 'action', 'import', 'human', 'dashiexp', 'web', 'banned', 'twotimesprime', 'king', 'entire', 'tonight', 'kanyewest', 'uyghur', 'mindless_bmd', 'ayeeeee', 'boost', 'respond', 'copping', 'imagine', 'passing', 'consumersfirst', 'thesocialstatus', 'code', 'slavery', 'senator', 'entering', 'speaking', 'public', 'sneakerhd84', 'adidasrunning', 'impossible', 'leader', 'backpack', 'candace_parker', 'solesavy', 'exec', 'giannis_an34', 'twelve', 'ticket', 'hero', 'metaverse', 'roblox', 'yeezy', 'crater', 'yooo', 'adidashoops', 'slam', 'yardrunner', 'aaron', 'lied', 'heartbroken', 'nov', 'acquired', 'ban', 'postseason', 'lazylionsnft', 'chose', 'centre', 'sephora', 'hyperkin', 'auxgod_', 'stern', 'realunogame', 'xboxsweepstakes', 'mitchell', 'eneskanter', 'kanter', 'taeyong', 'recognizes', 'malcolmjackso20', 'bred', 'rtfktstudios', 'ene', 'thunder', 'plane', 'enduyghurforcedlabor', 'uninterrupted', 'forum', 'prevention', 'boredapeyc', 'generation', 'brooklynsown90', 'garcons', 'impossibleisnothing', 'scctradingcards', 'topps', 'duffel', 'virgil', 'meta', 'psg_english', 'ingloriousguido', 'elizabeth_that', 'eth', 'graduation', 'statefarm', 'rodgers', 'richsignorelli', 'techinsider', \"abloh's\", 'adidasfballus', 'easportsfifa', 'beyonce', 'nikeland', 'nba_newyork', 'teamcanada', 'runningshoes', 'sen', 'pre-day', 'enesfreedom', 'throne', 'rtfkt', 'invsblefriends', 'thetraeyoung', 'footydotcom_', \"footy's\", 'zaptio', 'clonex', 'benitopagotto', 'spacerunnersnft', 'ronwyden', 'funneled', 'septe']\n",
      "neighbors of lululemon: ['thank', 'bill', 'happy', 'friday', 'let', 'week', 'strong', 'great', 'weekend', 'jacket', 'nice', 'running', 'thanks', 'early', 'video', 'job', 'top', 'beautiful', 'follow', 'twitter', 'congratulation', 'feel', 'please', 'make', 'supply', 'brand', 'fashion', 'hey', 'order', 'paid', 'shipping', 'seen', 'stuff', 'biggest', 'life', 'new', 'person', 'event', 'got', 'good', 'friend', 'put', 'yall', 'everyone', 'kind', 'took', 'long', 'old', 'school', 'appreciate', 'love', 'awesome', 'need', 'give', 'place', 'card', 'buy', 'day', 'read', 'try', 'wish', 'store', 'get', 'ask', 'know', 'best', 'thing', 'ordered', 'question', 'customer', 'service', 'said', 'run', 'size', 'huge', 'return', 'pair', 'perfect', 'fit', 'tho', 'one', 'find', 'something', 'super', 'club', 'sponsor', 'product', 'leading', 'way', 'app', 'keep', 'lol', 'course', 'hold', 'wearing', 'mind', 'business', 'see', 'stay', 'collection', 'made', 'sound', 'uniform', 'time', 'season', \"i'm\", 'guy', 'think', 'bright', 'hear', 'ya', \"i've\", 'god', 'shop', 'chance', 'collaboration', 'im', 'pack', 'favorite', 'underarmour', 'deal', 'shirt', 'space', 'force', 'pant', 'look', 'discount', 'worker', 'show', 'anyone', 'received', 'package', 'say', 'delivered', 'help', 'yeah', 'sign', 'given', 'kid', 'country', 'tell', 'mean', 'meeting', 'smh', 'set', 'hell', 'many', 'employee', 'short', 'fix', 'home', 'work', 'take', 'shoe', 'coming', 'commercial', 'thought', 'hate', 'rock', 'come', 'gear', 'hit', 'people', 'man', 'speak', 'student', 'team', 'wow', 'site', 'last', 'night', 'oh', 'going', 'right', 'fan', 'told', 'money', 'wrong', 'line', 'point', 'real', 'better', 'website', 'lot', 'year', 'abc', 'want', 'boy', 'stop', 'join', 'community', 'tweet', 'hoodie', 'style', 'miss', 'university', 'saw', 'getting', 'morning', 'hope', 'talk', 'family', 'global', 'free', 'wanna', 'send', 'clothes', 'pink', 'go', 'tomorrow', 'boston', 'call', 'move', 'teamcanada', 'amazing', 'fam', 'share', 'world', 'today', 'high', 'bag', 'check', \"we're\", 'hour', 'experience', 'kick', 'working', 'fire', 'others', 'change', 'present', 'moment', 'remember', 'yellow', 'like', 'purchase', 'item', 'full', 'refund', 'le', 'minute', 'reason', 'woman', 'history', 'classic', 'create', 'list', 'respect', 'wear', 'company', 'fact', 'first', 'become', 'clothing', 'wait', 'bit', 'gift', 'ship', 'next', 'excited', 'speaking', 'ticket', 'part', 'using', 'found', 'including', 'recent', 'release', 'sell', 'sold', 'sent', 'nothing', 'anything', 'gonna', 'giveaway', 'special', 'use', 'official', 'code', 'opportunity', 'different', 'news', 'campaign', 'action', 'mine', 'calling', 'name', 'comment', 'fast', 'hard', 'start', 'ok', 'online', 'waiting', 'someone', 'phone', 'trying', 'support', 'message', 'custom', 'came', 'done', 'decided', 'email', 'went', 'end', 'connected', 'human', 'price', 'sure', 'saying', 'china', 'bought', 'game', 'blazer', \"women's\", 'white', 'black', 'grey', 'ready', 'nikegolf', 'word', 'match', 'clean', 'partnership', 'apparel', 'much', 'birthday', 'turn', 'heard', 'target', 'announced', 'yesterday', 'credit', 'partner', 'color', 'cool', 'social', 'changing', 'missing', 'edition', 'guess', 'starting', 'believe', 'type', 'opening', 'virtual', 'fun', 'helped', 'live', 'started', 'adidas', 'jumpman23', 'girl', 'ad', 'account', 'looking', 'nike', 'taking', 'converse', 'analyst', 'number', 'damn', 'yo', 'model', \"that's\", 'canada', 'athlete', 'raffle', 'prize', 'imagine', 'thinking', 'care', 'shame', 'report', 'buying', 'little', 'able', 'supporting', 'back', 'big', 'shit', 'gold', 'open', 'adidasoriginals', 'puma', 'corporation', 'sport', 'proud', 'leader', 'update', 'dropped', 'dollar', 'fedex', 'stand', 'luck', 'selling', 'hi', 'james', 'continue', 'welcome', 'month', 'head', 'act', 'available', 'asking', 'pay', 'happen', 'launch', 'agree', 'ig', 'blue', 'making', 'photo', 'story', 'used', 'matter', 'giving', 'date', 'sorry', 'inspired', 'issue', 'coach', 'inspire', 'centre', 'bad', 'link', 'total', 'side', 'potus', 'dark', 'state', 'baby', 'boycott', 'as', 'quality', 'celebrate', 'dear', 'woke', 'least', 'true', 'collab', 'city', 'understand', 'hand', 'american', 'fly', 'stock', 'book', 'pick', 'fresh', 'face', 'incredible', 'dude', 'men', 'foot', 'tried', 'respond', 'safe', 'pic', 'drop', 'problem', 'yes', 'delivery', 'win', 'congrats', 'loved', 'play', 'watch', 'wtf', 'talking', 'enter', 'dm', 'goal', 'court', 'marketing', 'program', 'suck', 'access', 'instagram', 'post', 'worst', 'cause', 'sock', 'actual', 'trash', 'hero', 'member', 'retail', 'sale', 'called', 'exclusive', 'manager', 'apple', 'future', 'child', 'shot', 'dress', 'step', 'dropping', 'fucking', 'glad', 'market', \"they're\", 'everything', 'original', 'crazy', 'bring', 'entire', 'knight', \"he's\", 'york', 'project', 'chicago', 'owner', 'royal', 'bet', 'idea', 'medium', 'fuck', 'nba', 'nfl', 'wnba', 'espn', 'netflix', 'amazon', 'cocacola', 'mcdonalds', 'nft', 'nfts', 'lazylionsnft', 'logo', 'tech', 'king', 'street', 'nov', 'design', 'dream', 'wanted', 'second', 'left', 'law', 'wild', 'public', 'holy', 'impossible', 'lmao', 'wonder', 'grow', 'seeing', 'celebrating', 'christmas', 'green', 'pathetic', 'ball', 'seems', 'lost', 'listen', 'plane', 'triple', 'navy', 'nikestore', 'add', 'olympics', 'jordan', 'rest', 'box', 'player', 'slavery', 'lime', 'tonight', 'major', 'ban', 'air', \"can't\", 'final', 'solesavy', 'exec', 'retro', 'boot', 'nicekicks', 'ups', 'fuel', 'whole', 'cop', 'detail', 'ht', 'relationship', 'holiday', 'backpack', 'low', 'bro', 'walmart', 'corporate', 'america', 'example', 'red', 'meet', 'kit', 'brilliant', 'introducing', 'jersey', 'modern', 'content', 'playing', 'http', 'chinese', 'contract', 'image', 'sick', 'celtic', 'footlocker', 'neutral', 'latest', 'justintrudeau', 'summit', 'innovation', 'gotta', 'profit', 'soccer', 'stealing', 'several', 'bot', 'acquired', 'vp', 'blocked', 'deep', 'happened', 'sephora', 'uyghur', 'forced', 'david', 'son', 'digital', 'light', 'ice', 'dawn', 'newbalance', 'december', 'web', 'forum', 'labor', 'sneaker', 'nypdnews', 'prevention', 'alert', \"let's\", 'signed', 'steal', 'max', 'la', 'rule', 'em', 'college', 'usmc', 'young', 'mid', 'basketball', 'xinjiang', 'football', 'peo', 'tv', 'metaverse', 'dick', 'academy', 'midnight', 'shut', 'generation', 'november', 'kanyewest', 'festive', 'nikenyc', 'orange', 'charger', 'de', 'atmos_usa', 'sneakeradmirals', 'snkr_twitr', 'gatorade', 'ebay', 'adidasus', 'baseball', 'turtlepace5', 'adidasrunning', 'jack', 'recognizes', 'purple', 'exploring', 'thee']\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking number of nodes and edges in directional graph\n",
    "graph_summary_stats(G = Semantic_Graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a146cd",
   "metadata": {
    "papermill": {
     "duration": 0.038622,
     "end_time": "2024-06-10T15:50:33.462746",
     "exception": false,
     "start_time": "2024-06-10T15:50:33.424124",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Above, we can see the full semantic network graph. Right away, we can get the idea that this graph is too complex. After all, there are roughly 900 nodes and over 200,000 edges. As with the mentions graphs, we will work to create subgraphs that are much more interpretable. Furthermore, we will use a cleaning function (defined below), which will filter out edges based upon a set weight threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9ca9e526",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T15:50:33.542134Z",
     "iopub.status.busy": "2024-06-10T15:50:33.541661Z",
     "iopub.status.idle": "2024-06-10T15:50:33.551180Z",
     "shell.execute_reply": "2024-06-10T15:50:33.550051Z"
    },
    "papermill": {
     "duration": 0.051713,
     "end_time": "2024-06-10T15:50:33.553768",
     "exception": false,
     "start_time": "2024-06-10T15:50:33.502055",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating helper function 'focus_edges' to clean up graph\n",
    "def focus_edges(G, brand_nodes = None, weight_min = None, weight_max = None):\n",
    "  # Filter based on a list of brand nodes to focus\n",
    "  if brand_nodes != None:\n",
    "    # Filter edges based on the weight threshold\n",
    "    filtered_edges = [(u, v) for u, v in G.edges() if u in brand_nodes or v in brand_nodes]\n",
    "    # Create a subgraph based on the filtered edges\n",
    "    G = G.edge_subgraph(filtered_edges)\n",
    "  # Filter based on weight threshold\n",
    "  if weight_min != None:\n",
    "    # Filter edges based on the weight threshold\n",
    "    filtered_edges = [(u, v) for u, v, d in G.edges(data=True) if d['weight'] >= weight_min]\n",
    "    # Create a subgraph based on the filtered edges\n",
    "    G = G.edge_subgraph(filtered_edges)\n",
    "  if weight_max != None:\n",
    "    # Filter edges based on the weight threshold\n",
    "    filtered_edges = [(u, v) for u, v, d in G.edges(data=True) if d['weight'] <= weight_max]\n",
    "    # Create a subgraph based on the filtered edges\n",
    "    G = G.edge_subgraph(filtered_edges)\n",
    "  # Return the filtered subgraph\n",
    "  return G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23aa7b88",
   "metadata": {
    "papermill": {
     "duration": 0.043598,
     "end_time": "2024-06-10T15:50:33.635934",
     "exception": false,
     "start_time": "2024-06-10T15:50:33.592336",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The function above will help reduce the number of edges in all future semantic network graphs. This will be accomplished by filtering edges based upon a set weight threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7b2d4903",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T15:50:33.720807Z",
     "iopub.status.busy": "2024-06-10T15:50:33.718809Z",
     "iopub.status.idle": "2024-06-10T15:50:34.085294Z",
     "shell.execute_reply": "2024-06-10T15:50:34.084045Z"
    },
    "papermill": {
     "duration": 0.411804,
     "end_time": "2024-06-10T15:50:34.088252",
     "exception": false,
     "start_time": "2024-06-10T15:50:33.676448",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cleaning up the semantic graph with 'focus_edges' function\n",
    "Semantic_Graph_cleaned = focus_edges(G = Semantic_Graph, weight_min = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "066350f8",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-06-10T15:50:34.167078Z",
     "iopub.status.busy": "2024-06-10T15:50:34.166585Z",
     "iopub.status.idle": "2024-06-10T15:50:34.741378Z",
     "shell.execute_reply": "2024-06-10T15:50:34.740092Z"
    },
    "papermill": {
     "duration": 0.617885,
     "end_time": "2024-06-10T15:50:34.745202",
     "exception": false,
     "start_time": "2024-06-10T15:50:34.127317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "##### Graph Summary #####\n",
      "number of nodes: 908\n",
      "number of edges: 39967\n",
      "\n",
      "nodes: ['nike', \"women's\", 'air', 'uptempo', 'white', 'yellow', 'available', 'footlocker', 'sneakerscouts', 'adidas', 'lasership', 'stealing', 'work', 'home', 'alert', 'next', 'collab', 'dropping', 'ad', 'space', 'low', 'snipes_usa', 'snkrs', 'get', 'nikebasketball', 'puma', 'stock', 'partnership', 'helped', 'grow', 'etnow', 'team', 'release', 'jordan', 'real', 'support', 'friend', 'family', 'sick', 'kaya_alexander5', 'nikestore', 'sneakeradmirals', 'awesome', 'pair', 'lot', 'wait', 'dress', 'game', 'jumpman23', 'usnikefootball', 'usarmy', 'britisharmy', 'cnn', 'wheeloffortune', 'bloombergradio', 'wbpictures', 'disneystudios', 'wgci', 'hot97', 'v103', 'tmz', 'harveylevintmz', 'instagram', 'ebay', 'amazon', 'abc', 'abc7chicago', 'chicago_police', 'nypdnews', 'usairforce', 'usmc', 'usnavy', 'royalnavy', 'royalairforce', 'jack', 'finkd', 'priscillachanz', 'business', 'fbi', 'beccadiamondxx', 'potus', 'vp', 'drbiden44', 'bet', 'max', 'japan', 'ht', 'dunk', 'beautiful', 'fuck', 'working', 'sizeofficial', 'adidasus', 'adidasoriginals', 'look', 'problem', 'wearing', 'say', 'drop', 'time', 'vapormax', 'flyknit', 'royal', 'finishline', 'young', 'thetraeyoung', 'much', 'people', 'like', 'nicekicks', 'snkr_twitr', 'iamtmcii', 'good', 'edition', 'global', 'sport', 'week', 'take', 'place', 'black', 'lol', 'apple', 'think', 'corporation', 'care', 'latest', 'underarmour', 'deal', 'run', 'company', 'jersey', 'win', 'guy', 'see', 'way', 'keep', 'play', 'top', 'going', 'year', 'thank', 'great', 'getting', 'started', 'moment', 'connected', 'show', 'understand', \"they're\", 'making', 'tweet', 'want', 'others', 'word', 'kingjames', 'start', 'dude', 'blazer', 'suede', 'red', 'need', 'sneaker', 'day', 'color', 'style', 'code', 'date', 'price', 'sneakerhead', 'fashion', 'yes', 'know', 'free', 'used', 'celebrating', 'birthday', 'discount', 'item', 'thanks', 'given', 'something', 'trying', 'seen', 'issue', 'hand', 'let', 'make', 'sure', 'love', 'school', 'student', 'buy', 'quality', 'everyone', 'job', 'theestallion', 'thee', 'app', 'morning', 'history', 'step', 'go', 'shoe', 'part', 'facility', 'forum', 'mid', 'bot', 'sold', 'summit', 'bright', 'wolf', 'grey', 'hey', 'cool', 'pay', 'dropped', 'exclusive', 'access', 'long', 'member', 'got', 'account', 'brand', 'new', 'customer', 'bought', 'blue', 'month', 'light', 'cocacola', 'turn', 'join', 'please', 'city', 'lime', 'glow', 'kdtrey5', 'coach', 'sponsor', 'use', 'steal', 'package', 'delivery', 'put', 'left', 'le', 'minute', 'delivered', 'lululemon', 'bill', 'happy', 'friday', 'strong', 'weekend', 'jacket', 'deep', \"we're\", 'giving', 'store', 'head', 'wanted', 'hi', 'running', 'force', 'email', 'thing', \"i'm\", 'woman', 'god', 'service', 'idea', 'night', 'gonna', 'waiting', 'old', 'son', 'first', 'tv', 'espn', 'appreciate', 'wnba', 'help', 'change', 'move', 'kicksonfire', 'bro', 'best', 'custom', 'candace_parker', 'leader', 'human', 'right', 'order', 'congrats', 'manager', 'justdoit', 'pixel', 'se', 'gear', 'partner', 'wild', 'retail', 'http', 'second', 'season', 'yeah', 'bad', 'one', 'ice', 'soccer', 'ask', 'thought', 'kit', 'lfc', 'line', 'match', 'sock', 'york', 'looking', 'man', 'many', 'size', 'last', 'chance', 'said', 'wow', 'club', 'shop', 'shirt', 'law', 'tell', \"he's\", 'remember', 'decided', 'campaign', 'link', 'read', 'story', 'today', 'nikenyc', 'girl', 'point', 'smh', 'ni', 'continue', 'stay', 'boot', 'camp', 'swoosh', 'medium', 'em', 'hit', 'nice', 'add', 'tried', 'different', 'product', 'stop', 'adidasrunning', 'adidasfballus', 'adidashoops', 'htt', 'talking', 'shit', 'several', 'money', 'actual', 'virtual', 'event', 'fly', 'israel', 'glad', 'snkrfrkrmag', 'atmos_usa', 'theshoegame', 'ticket', 'sale', 'la', 'tomorrow', 'gatorade', 'ordered', 'came', 'site', 'thesocialstatus', \"that's\", 'starting', 'dick', 'detail', 'yall', 'big', 'wtf', 'incredible', 'hippie', 'foot', 'fontanka', 'pink', 'find', 'kotd', 'yoursneakersaredope', 'kind', 'safe', 'exploring', 'lebron', 'james', 'college', 'wish', 'athlete', 'early', 'video', 'neutral', 'olive', 'travis', 'kick', 'fix', 'fan', 'wear', 'total', 'classic', 'green', 'selling', 'special', 'check', 'icon', 'able', 'follow', 'twitter', 'sb', 'box', 'football', 'believe', 'done', 'community', 'chicago', 'try', 'amazing', 'luck', 'life', 'short', 'holiday', 'face', 'talk', 'buying', 'pant', 'boy', 'pushed', 'send', 'ig', 'academy', 'woke', 'hoodie', 'favorite', 'orange', 'congratulation', 'feel', 'social', 'message', 'supply', 'crazy', 'tho', 'corporate', 'become', 'model', 'rest', 'world', 'pepsi', 'online', 'paid', 'shipping', 'stuff', 'found', 'bring', 'back', 'christmas', 'uniform', 'report', 'high', 'forced', 'chinese', 'shut', 'guess', 'fire', 'watch', 'nikegolf', 'ball', 'nikefootball', 'pathetic', 'made', 'court', 'retro', 'modern', 'purchase', 'seeing', 'program', 'phil', 'knight', 'street', 'university', 'name', 'image', 'rule', 'official', 'clean', 'chicksinkicks2', 'shot', 'hour', 'soleguru', 'welcome', 'converse', 'slave', 'labor', 'clothing', \"can't\", 'number', 'gift', 'suck', 'biggest', 'person', 'innovation', 'fam', 'beiberlove69', 'stand', 'mean', 'wrong', 'type', 'question', 'original', 'come', 'wonder', 'speak', 'update', 'cause', 'website', 'triple', 'better', 'sell', 'im', 'purple', 'dawn', 'fedex', 'worst', 'fit', 'profit', 'called', 'consumersfirst', 'dm', 'took', 'design', 'example', 'kanyewest', 'cop', 'collection', 'content', 'super', 'dope', 'lost', 'ups', 'ship', 'walmart', 'refund', 'mind', 'logo', 'pack', 'heard', 'give', 'card', 'fun', 'true', 'mine', \"i've\", 'hope', 'meet', 'pick', 'gotta', 'dollar', 'american', 'fresh', 'state', 'boost', 'tech', 'side', 'griffey', 'future', 'controlled', 'return', 'netflix', 'present', 'nikeservice', 'saying', 'draw', 'passing', 'post', 'someone', 'lexxdaturtle', 'charger', 'thinking', 'commercial', 'vadriano2000', 'full', 'happen', 'damn', 'wanna', 'whole', 'anyone', 'baby', 'kid', 'hard', 'huge', 'perfect', 'hell', 'prize', 'taking', 'sorry', 'unlvfootball', 'uniswag', 'marketing', 'asking', 'nfl', 'chose', 'fact', 'nothing', 'raffle', 'leading', 'player', \"let's\", 'men', 'dear', 'clothes', 'missing', 'little', 'course', 'opening', 'ok', 'public', 'photo', 'end', 'child', 'hold', 'seems', 'sound', 'using', 'jamarrobrown', 'book', 'navy', 'nikesb', 'saw', 'loved', 'hear', 'ya', 'dream', 'as', 'nft', 'gold', 'collaboration', 'anything', 'sent', 'hate', 'playing', 'exec', 'call', 'reason', 'celebrate', 'newbalance', 'supporting', 'boston', 'least', 'share', 'mcdonalds', 'trash', 'went', 'yo', 'live', 'set', 'coming', 'solefed', 'apparel', 'anniversary', 'excited', 'midnight', 'china', 'employee', 'told', 'pic', 'jadendaly', 'target', 'fucking', 'worker', 'received', 'opportunity', 'boycott', 'tonight', 'listen', 'bit', 'matter', 'michael', 'trvisxx', 'nba', 'celtic', 'sign', 'country', 'meeting', 'proud', 'including', 'goal', 'canada', 'final', 'happened', 'lsoshipping', 'yesterday', 'agree', 'launch', 'basketball', 'ready', 'dirtydetty9381', 'nfts', 'phone', 'yeezy', 'list', 'impossibleisnothing', 'rock', 'speaking', 'donaldjtrumpjr', 'comment', 'maniere_usa', 'fast', 'kaepernick7', 'joshuajhan', 'xbox', 'december', 'bag', 'oh', 'dark', 'cosmic', 'unity', 'everything', 'solelinks', 'miss', 'introducing', 'experience', 'announced', 'act', 'foamposite', 'calling', '_talkswithtj', 'adizero', 'runningshoesgur', 'runningshoes', 'giveaway', 'festive', 'signed', 'contract', 'fuel', 'credit', 'major', 'holy', 'inspire', 'reebok', 'teamcanada', 'impossible', 'slavery', 'inspired', 'open', 'project', 'olympics', 'create', 'spidadmitchell', 'respect', 'lmao', 'league', 'patta_nl', 'news', 'recent', 'america', 'jdsports', 'culture', 'enter', 'owner', 'market', 'eliudkipchoge', 'grail', 'boardroom', 'action', 'brilliant', 'blocked', 'djbluiz', 'changing', 'baseball', 'weareivypark', 'turtlepace5', 'shame', 'november', 'uno', 'region', 'deezle148', '210gotkickz', 'voice', 'on-court', 'repjimbanks', 'joebiden', 'billboard', 'digital', 'copping', 'xinjiang', 'david', 'xbox20', 'metaverse', 'analyst', 'entire', 'aaron', 'justintrudeau', 'relationship', 'king', 'de', 'dashiexp', 'yooo', 'imagine', 'import', 'banned', 'web', 'twotimesprime', 'uyghur', 'mindless_bmd', 'ayeeeee', 'postseason', 'statefarm', 'respond', 'ban', 'sneakerhd84', 'senator', 'centre', 'entering', 'beyonce', 'nov', 'backpack', 'solesavy', 'giannis_an34', 'twelve', 'thunder', 'hero', 'roblox', 'crater', 'donovan', 'mitchell', 'austinekeler', 'peo', 'michael_fabiano', 'slam', 'yardrunner', 'generation', 'easportsfifa', 'lied', 'heartbroken', 'acquired', 'duffel', 'taeyong', 'lazylionsnft', 'sephora', 'xboxsweepstakes', 'hyperkin', 'auxgod_', 'stern', 'bred', 'realunogame', 'plane', 'ingloriousguido', 'eneskanter', 'kanter', 'malcolmjackso20', 'recognizes', 'rtfktstudios', 'ene', 'meta', 'enduyghurforcedlabor', 'eth', 'uninterrupted', 'prevention', 'boredapeyc', 'garcons', 'brooklynsown90', 'graduation', 'scctradingcards', 'topps', 'virgil', 'lagalaxy', 'psg_english', 'elizabeth_that', 'rodgers', 'richsignorelli', 'techinsider', \"abloh's\", 'sen', 'nikeland', 'funneled', 'throne', 'nba_newyork', 'pre-day', 'enesfreedom', 'clonex', 'rtfkt', 'invsblefriends', 'footydotcom_', \"footy's\", 'zaptio', 'benitopagotto', 'spacerunnersnft', 'ronwyden', 'septe', 'maybes']\n",
      "\n",
      "neighbors of adidas: ['available', 'puma', 'stock', 'partnership', 'helped', 'grow', 'nike', 'real', 'get', 'support', 'friend', 'family', 'adidasus', 'adidasoriginals', 'look', 'problem', 'wearing', 'say', 'drop', 'young', 'thetraeyoung', 'much', 'people', 'like', 'lol', 'deal', 'black', 'run', 'company', 'thank', 'great', 'team', 'getting', 'started', 'dude', 'red', 'need', 'jersey', 'yes', 'start', 'know', 'free', 'used', 'history', 'step', 'play', 'forum', 'mid', 'bot', 'sold', 'bought', 'pair', 'blue', 'month', 'light', 'please', 'next', 'city', 'long', 'beautiful', 'sneaker', 'love', 'new', 'grey', 'year', 'wanted', 'thanks', 'service', 'shoe', 'keep', 'idea', 'night', 'gonna', 'human', 'right', 'brand', 'make', 'sure', 'gear', 'see', 'got', 'one', 'good', 'day', 'top', 'trying', 'tell', 'let', \"he's\", 'think', 'dropping', 'today', 'way', 'smh', 'snkr_twitr', 'adidasrunning', 'adidasfballus', 'adidashoops', \"we're\", 'glad', 'em', 'want', 'collab', 'tomorrow', 'man', 'show', 'site', 'wish', 'shop', 'underarmour', 'apple', 'awesome', 'first', 'athlete', 'size', 'fix', 'check', 'icon', 'able', 'believe', 'early', 'done', 'home', 'kit', 'follow', 'instagram', 'http', 'morning', 'try', 'amazing', 'luck', 'help', 'life', 'fan', 'buying', 'academy', 'shirt', 'woke', 'order', 'wait', 'online', 'fire', 'work', 'watch', 'boy', 'looking', 'fly', 'court', 'season', 'retro', 'time', 'store', 'purchase', 'official', 'face', 'win', 'guess', 'total', 'shot', 'tweet', 'hour', 'sale', 'price', 'clothing', 'question', 'original', 'program', 'point', 'world', 'better', 'hey', 'going', 'im', 'big', 'thought', 'dm', 'welcome', 'many', 'design', 'example', 'kanyewest', 'mind', 'ordered', 'mine', \"i've\", \"i'm\", 'old', 'head', 'hope', 'meet', 'pick', 'take', 'care', 'gotta', 'find', 'state', 'white', 'boost', 'tech', 'side', 'give', 'future', 'return', 'passing', 'post', 'thinking', 'commercial', 'guy', 'weekend', 'wanna', 'ask', 'girl', 'wild', 'card', 'prize', 'working', 'seen', 'fun', 'collection', \"that's\", 'nice', 'sell', 'college', 'move', 'several', 'style', 'feel', 'sock', 'dress', 'hoodie', \"women's\", 'pant', 'lasership', 'delivered', 'package', 'customer', 'saying', 'money', 'waiting', 'actual', 'cocacola', 'nft', 'favorite', 'sport', 'school', 'hard', 'bring', 'happy', 'birthday', 'last', 'newbalance', 'converse', 'supporting', 'boston', 'understand', 'sorry', 'everyone', 'put', 'dope', 'jacket', 'excited', 'share', 'coach', 'clean', \"let's\", 'buy', 'stop', 'video', 'something', 'thing', 'missing', 'made', 'green', 'player', 'tonight', 'super', 'high', 'talking', 'message', 'code', 'app', 'gold', 'collaboration', 'china', 'campaign', 'ad', 'asking', 'photo', 'ice', 'rule', 'sneakerhead', 'running', 'fit', 'whole', 'color', 'minute', 'saw', 'release', 'bro', 'walmart', 'target', 'set', 'le', 'football', 'short', 'taking', 'stay', 'ready', 'using', 'reason', 'seems', 'nfts', 'pay', 'join', 'kick', 'impossibleisnothing', 'employee', 'best', 'found', 'different', 'jumpman23', 'nfl', 'nba', 'bag', 'full', 'stand', 'said', 'corporation', \"can't\", 'wear', 'product', 'second', 'game', 'ups', 'paid', 'pack', 'phone', 'lost', 'hell', 'website', 'pic', 'send', 'retail', 'kid', 'fucking', 'sick', 'mean', 'perfect', 'happen', 'shipping', 'worst', 'shit', 'suck', 'speak', 'uniform', 'someone', 'crazy', 'announced', 'cool', 'live', 'street', 'classic', 'huge', 'cause', 'adizero', 'runningshoesgur', 'runningshoes', 'giveaway', 'week', 'back', 'part', 'public', 'come', 'opportunity', 'god', 'kind', 'box', 'call', 'matter', 'line', 'bit', 'others', 'twitter', 'inspire', 'reebok', 'gatorade', 'kotd', 'yoursneakersaredope', 'safe', 'took', 'congrats', 'email', 'use', 'lot', 'discount', 'go', 'bad', 'celebrate', 'change', 'impossible', 'nothing', 'remember', 'name', 'anything', 'match', 'orange', 'member', 'experience', 'exclusive', 'went', 'goal', 'congratulation', 'turn', 'amazon', 'proud', 'add', 'number', 'damn', 'true', 'basketball', 'giving', 'sponsor', 'clothes', 'soccer', 'link', 'ok', 'course', 'open', 'nikebasketball', 'woman', 'enter', 'corporate', 'global', 'fedex', 'happened', 'delivery', 'yeezy', 'steal', 'stuff', 'hand', 'refund', 'wtf', 'nikestore', 'owner', 'date', 'detail', 'ig', 'social', 'medium', 'low', 'quality', 'apparel', 'blocked', 'edition', 'making', 'hi', 'marketing', 'business', 'mcdonalds', 'selling', 'foot', 'virtual', 'seeing', 'yo', 'lmao', 'called', 'content', 'dream', 'pink', 'rock', 'everything', 'comment', 'ball', 'november', 'footlocker', 'story', 'partner', 'gift', 'son', 'xbox', 'tho', 'news', 'alert', 'pepsi', 'market', 'recent', 'country', 'tried', 'trash', 'read', 'community', 'club', 'book', 'holiday', 'type', 'little', 'become', 'account', 'as', 'innovation', 'boot', 'special', 'report', 'sneakeradmirals', 'end', 'sound', 'chose', '_talkswithtj', 'told', 'respect', 'item', 'issue', 'came', 'anyone', 'james', 'vp', 'fuck', 'xbox20', 'anniversary', 'ebay', \"they're\", 'celebrating', 'coming', 'cop', 'inspired', 'launch', 'wrong', 'list', 'aaron', 'wow', 'fresh', 'la', 'king', 'ticket', 'signed', 'nicekicks', 'iamtmcii', 'beiberlove69', 'including', 'fact', 'word', 'air', 'event', 'dashiexp', 'yooo', 'sent', 'culture', 'baby', 'yall', 'logo', 'yeah', 'jack', 'fam', 'candace_parker', 'least', 'model', 'hold', 'holy', 'credit', 'america', 'left', 'jordan', 'force', 'american', 'changing', 'fashion', 'wnba', 'michael', 'latest', 'hit', 'job', 'sizeofficial', 'received', 'men', 'slave', 'labor', 'wonder', 'miss', 'person', 'appreciate', 'dropped', 'dear', 'netflix', 'navy', 'espn', 'royal', 'draw', 'space', 'ship', 'chance', 'child', 'place', 'given', 'worker', 'playing', 'starting', 'weareivypark', 'beyonce', 'supply', 'sign', 'entering', 'bet', 'oh', 'introducing', 'heard', 'contract', 'baseball', 'dick', 'update', 'yesterday', 'nov', 'continue', 'decided', 'solelinks', 'league', 'moment', 'talk', 'final', 'stealing', 'major', 'statefarm', 'christmas', 'student', 'fast', 'snkrs', 'image', 'digital', 'dark', 'hate', 'create', 'rest', 'dunk', 'chicago', 'triple', 'spidadmitchell', 'dollar', 'relationship', 'ya', 'pathetic', 'leader', 'hear', 'raffle', 'access', 'agree', 'university', 'action', 'donovan', 'mitchell', 'friday', 'austinekeler', 'respond', 'peo', 'charger', 'michael_fabiano', 'shut', 'atmos_usa', 'leading', 'chinese', 'kicksonfire', 'strong', 'solefed', 'imagine', 'incredible', 'generation', 'easportsfifa', 'kingjames', 'entire', 'biggest', 'custom', 'present', 'opening', 'project', 'taeyong', 'hero', 'xboxsweepstakes', 'hyperkin', 'act', 'travis', 'roblox', 'bright', 'turtlepace5', 'grail', 'boycott', 'shame', 'loved', 'lexxdaturtle', 'deezle148', 'finishline', 'ht', 'slam', 'jdsports', 'manager', 'htt', 'eneskanter', 'nikesb', 'nikeservice', 'giannis_an34', 'solesavy', 'sephora', 'lfc', 'lagalaxy', 'topps', 'profit', 'listen', 'boredapeyc', 'york', 'pixel', 'december', 'fbi', 'rodgers', 'yellow', 'meta', 'richsignorelli', 'lied', 'analyst', 'web', 'metaverse', 'eth', 'rtfktstudios', 'kaya_alexander5', 'facility', 'jadendaly', 'abc', 'senator', 'potus', 'olive', 'lazylionsnft', 'de', 'boardroom', 'snkrfrkrmag', 'rtfkt', 'maybes']\n",
      "neighbors of nike: [\"women's\", 'air', 'uptempo', 'white', 'yellow', 'available', 'footlocker', 'sneakerscouts', 'lasership', 'stealing', 'work', 'home', 'alert', 'next', 'collab', 'dropping', 'snkrs', 'get', 'nikebasketball', 'puma', 'stock', 'partnership', 'helped', 'grow', 'adidas', 'etnow', 'team', 'release', 'jordan', 'sick', 'lot', 'wait', 'dress', 'usarmy', 'britisharmy', 'cnn', 'wheeloffortune', 'bloombergradio', 'wbpictures', 'disneystudios', 'wgci', 'hot97', 'v103', 'tmz', 'harveylevintmz', 'instagram', 'ebay', 'amazon', 'abc', 'abc7chicago', 'chicago_police', 'nypdnews', 'usairforce', 'usmc', 'usnavy', 'royalnavy', 'royalairforce', 'jack', 'finkd', 'priscillachanz', 'business', 'fbi', 'beccadiamondxx', 'potus', 'vp', 'drbiden44', 'bet', 'ad', 'max', 'japan', 'snipes_usa', 'ht', 'dunk', 'beautiful', 'sizeofficial', 'time', 'vapormax', 'flyknit', 'royal', 'finishline', 'jumpman23', 'sneakeradmirals', 'nicekicks', 'snkr_twitr', 'iamtmcii', 'look', 'good', 'black', 'apple', 'think', 'people', 'corporation', 'care', 'latest', 'underarmour', 'working', 'win', 'guy', 'see', 'way', 'keep', 'play', 'say', 'top', 'going', 'year', 'moment', 'connected', 'show', 'understand', \"they're\", 'making', 'tweet', 'want', 'others', 'space', 'low', 'kingjames', 'edition', 'start', 'blazer', 'suede', 'getting', 'celebrating', 'birthday', 'discount', 'item', 'thanks', 'given', 'something', 'trying', 'seen', 'issue', 'hand', 'go', 'shoe', 'part', 'facility', 'support', 'summit', 'bright', 'wolf', 'grey', 'cocacola', 'turn', 'join', 'problem', 'lime', 'glow', 'kdtrey5', 'new', 'coach', 'sponsor', 'use', 'company', 'steal', 'package', 'delivery', 'put', 'left', 'le', 'minute', 'delivered', 'week', 'deep', 'know', 'thing', \"i'm\", 'woman', 'day', 'god', 'waiting', 'red', 'let', 'order', 'congrats', 'customer', 'manager', 'justdoit', 'force', 'pixel', 'se', 'happy', 'partner', 'wild', 'retail', 'http', 'second', 'season', 'yeah', 'bad', 'lol', 'ice', 'soccer', 'store', 'ask', 'thought', 'kit', 'lfc', 'real', 'line', 'match', 'sock', 'york', 'game', 'custom', 'jersey', 'man', 'many', 'pair', 'size', 'last', 'chance', 'said', 'sold', 'wow', 'club', 'shop', 'shirt', 'please', 'remember', 'app', 'nikenyc', 'girl', 'point', 'blue', 'night', 'ni', 'love', 'swoosh', 'htt', 'today', 'several', 'job', 'make', 'money', 'actual', 'virtual', 'event', 'take', 'link', 'israel', 'right', 'snkrfrkrmag', 'atmos_usa', 'theshoegame', 'great', 'nikestore', 'gatorade', 'dick', 'sure', 'detail', 'incredible', 'global', 'brand', 'hippie', 'light', 'fontanka', 'pink', 'need', 'medium', 'city', 'exploring', 'neutral', 'olive', 'stop', 'giving', 'travis', 'kick', 'buy', 'total', 'classic', 'green', 'college', 'started', 'sport', 'story', 'check', 'sb', 'box', 'sneaker', 'thank', 'pant', 'boy', 'got', 'pushed', 'send', 'hoodie', 'favorite', 'color', 'orange', 'product', 'corporate', 'become', 'model', 'rest', 'world', 'pepsi', 'shipping', 'found', 'bring', 'back', 'christmas', 'uniform', 'report', 'high', 'price', 'forced', 'chinese', 'shut', 'woke', 'best', 'nikegolf', 'ball', 'nikefootball', 'pathetic', 'made', 'big', 'able', 'seeing', 'program', 'purchase', 'school', 'luck', 'phil', 'knight', 'street', 'university', 'athlete', 'name', 'image', 'rule', 'chicksinkicks2', 'much', 'kaya_alexander5', 'soleguru', 'welcome', 'converse', 'holiday', 'slave', 'labor', \"can't\", 'number', 'gift', 'suck', 'innovation', 'fam', 'beiberlove69', 'quality', 'stand', 'mean', 'wrong', 'wish', 'type', 'come', 'em', 'wonder', 'cause', 'website', 'selling', 'lebron', 'triple', 'sell', 'run', 'purple', 'dawn', 'fan', 'fedex', 'worst', 'help', 'foot', 'fit', 'profit', 'called', 'cop', 'believe', 'collection', 'content', 'super', 'dope', 'bro', 'lost', 'ups', 'service', 'ship', 'walmart', 'refund', 'awesome', 'logo', 'pack', 'fun', 'true', 'watch', 'looking', 'give', 'fuck', 'dollar', 'american', 'buying', 'shit', 'fresh', 'griffey', 'tell', 'netflix', 'campaign', 'present', 'add', 'draw', 'lexxdaturtle', 'gonna', 'charger', 'vadriano2000', 'full', 'community', 'one', 'ordered', 'happen', 'ig', 'nice', 'damn', 'paid', 'whole', 'appreciate', 'free', 'anyone', 'word', 'strong', 'baby', 'amazing', 'hell', 'mine', 'weekend', 'tried', 'unlvfootball', 'usnikefootball', 'uniswag', 'hi', 'marketing', 'month', 'done', 'theestallion', 'raffle', 'long', 'player', 'im', 'clothes', 'nothing', 'missing', 'speak', 'retro', 'little', 'fashion', 'sale', 'online', 'photo', 'follow', 'end', 'child', 'question', 'gear', 'nfl', 'first', 'place', 'seems', 'fire', 'icon', 'yes', 'nikeservice', 'book', 'navy', 'cool', 'nikesb', 'saw', 'loved', 'old', 'bought', 'talk', 'social', 'glad', 'pay', 'hard', 'try', 'fix', 'gold', 'wearing', 'anything', 'sent', 'hate', 'playing', 'call', 'yall', 'gotta', 'video', 'feel', 'deal', \"that's\", 'newbalance', 'took', 'hit', 'least', \"we're\", 'life', 'share', 'design', 'hey', 'mcdonalds', 'trash', 'dude', 'wear', 'went', 'yo', 'face', 'person', 'used', 'talking', 'better', 'reason', 'stuff', 'friday', 'set', 'coming', 'solefed', 'kicksonfire', 'son', 'apparel', 'anniversary', 'kid', 'excited', 'change', 'live', 'midnight', 'everyone', 'yoursneakersaredope', 'kotd', 'tomorrow', 'head', 'wanted', 'china', 'employee', 'told', 'crazy', 'came', 'shot', 'dream', 'clean', 'jamarrobrown', 'jadendaly', 'target', 'bill', 'fucking', 'find', 'style', 'opportunity', 'boycott', 'tv', 'commercial', 'running', 'state', 'fact', 'modern', 'football', 'listen', 'saying', 'michael', 'trvisxx', 'nba', 'celtic', 'including', 'meet', 'goal', 'sneakerhead', 'canada', 'someone', 'worker', 'move', 'young', 'final', 'opening', 'dropped', 'country', 'hope', 'site', 'men', 'agree', 'original', 'future', 'jacket', 'different', 'launch', 'exclusive', 'sign', 'course', 'twitter', 'basketball', 'academy', 'family', 'bot', 'taking', 'dirtydetty9381', 'ok', 'post', 'sorry', 'as', 'phone', 'member', 'thinking', 'guess', 'list', 'using', 'hour', 'supporting', 'drop', 'yesterday', 'donaldjtrumpjr', 'comment', 'maniere_usa', 'chicago', 'fast', 'kaepernick7', 'morning', 'ready', 'idea', 'joshuajhan', 'xbox', 'tho', 'adidasus', 'perfect', 'hear', 'december', 'kind', 'example', 'return', 'huge', 'dear', 'dark', 'wanna', 'asking', 'friend', 'cosmic', 'unity', 'official', 'miss', 'stay', 'special', 'dm', 'introducing', 'experience', 'thee', 'decided', 'short', 'access', 'everything', 'act', \"i've\", 'foamposite', 'date', 'calling', '_talkswithtj', 'like', 'festive', 'continue', 'read', 'fuel', 'credit', 'oh', 'hold', 'major', 'card', 'message', 'congratulation', 'holy', 'inspire', 'reebok', 'james', 'boston', 'student', 'email', 'espn', 'safe', 'biggest', 'nfts', \"he's\", 'camp', 'prize', 'la', 'inspired', \"let's\", 'wnba', 'open', 'court', 'project', 'olympics', 'pick', 'contract', 'leading', 'clothing', 'league', 'patta_nl', 'news', 'ya', 'early', 'meeting', 'boot', 'step', 'america', 'jdsports', 'culture', 'mid', 'side', 'create', 'announced', 'owner', 'market', 'wtf', 'eliudkipchoge', 'grail', 'received', 'solelinks', 'pic', 'tech', 'collaboration', 'brilliant', 'respect', 'adidasoriginals', 'update', 'djbluiz', 'changing', 'giveaway', 'smh', 'baseball', 'bit', 'turtlepace5', 'account', 'celebrate', 'sound', 'mind', 'shame', 'bag', 'november', 'matter', 'region', 'happened', 'nft', 'uno', 'starting', 'blocked', 'recent', 'lmao', 'heard', 'signed', 'voice', 'on-court', 'repjimbanks', 'supply', 'joebiden', 'proud', 'billboard', 'lsoshipping', 'rock', 'boardroom', '210gotkickz', 'xinjiang', 'david', 'fly', 'controlled', 'history', 'lululemon', 'law', 'enter', 'deezle148', 'justintrudeau', 'digital', 'relationship', 'de', 'action', 'import', 'human', 'web', 'banned', 'twotimesprime', 'king', 'entire', 'tonight', 'kanyewest', 'uyghur', 'mindless_bmd', 'ayeeeee', 'boost', 'respond', 'copping', 'imagine', 'passing', 'consumersfirst', 'thesocialstatus', 'code', 'slavery', 'senator', 'entering', 'speaking', 'public', 'sneakerhd84', 'impossible', 'leader', 'backpack', 'candace_parker', 'solesavy', 'exec', 'giannis_an34', 'twelve', 'ticket', 'hero', 'metaverse', 'roblox', 'yeezy', 'crater', 'adidashoops', 'slam', 'yardrunner', 'aaron', 'lied', 'heartbroken', 'nov', 'acquired', 'ban', 'postseason', 'lazylionsnft', 'chose', 'centre', 'sephora', 'auxgod_', 'realunogame', 'eneskanter', 'kanter', 'taeyong', 'recognizes', 'malcolmjackso20', 'bred', 'rtfktstudios', 'ene', 'thunder', 'plane', 'enduyghurforcedlabor', 'uninterrupted', 'forum', 'prevention', 'boredapeyc', 'generation', 'brooklynsown90', 'garcons', 'scctradingcards', 'topps', 'duffel', 'virgil', 'meta', 'psg_english', 'ingloriousguido', 'elizabeth_that', 'eth', 'graduation', 'statefarm', 'rodgers', 'richsignorelli', 'techinsider', \"abloh's\", 'beyonce', 'nikeland', 'nba_newyork', 'sen', 'pre-day', 'enesfreedom', 'throne', 'rtfkt', 'invsblefriends', 'footydotcom_', \"footy's\", 'zaptio', 'clonex', 'benitopagotto', 'spacerunnersnft', 'ronwyden', 'funneled', 'septe']\n",
      "neighbors of lululemon: ['thank', 'bill', 'happy', 'friday', 'let', 'week', 'strong', 'great', 'weekend', 'jacket', 'nice', 'running', 'thanks', 'early', 'video', 'job', 'top', 'beautiful', 'follow', 'twitter', 'congratulation', 'feel', 'please', 'make', 'supply', 'brand', 'fashion', 'hey', 'order', 'paid', 'shipping', 'seen', 'stuff', 'life', 'new', 'person', 'event', 'got', 'good', 'friend', 'put', 'everyone', 'kind', 'took', 'long', 'old', 'school', 'appreciate', 'love', 'awesome', 'need', 'give', 'place', 'card', 'buy', 'day', 'read', 'try', 'wish', 'store', 'get', 'ask', 'know', 'best', 'thing', 'ordered', 'question', 'customer', 'service', 'said', 'run', 'size', 'huge', 'return', 'pair', 'perfect', 'fit', 'one', 'find', 'something', 'super', 'club', 'sponsor', 'product', 'way', 'keep', 'lol', 'hold', 'wearing', 'mind', 'business', 'see', 'stay', 'collection', 'made', 'sound', 'uniform', 'time', 'season', \"i'm\", 'guy', 'think', 'hear', \"i've\", 'god', 'shop', 'chance', 'im', 'pack', 'favorite', 'underarmour', 'deal', 'shirt', 'space', 'pant', 'look', 'discount', 'worker', 'show', 'anyone', 'received', 'package', 'say', 'delivered', 'help', 'yeah', 'sign', 'given', 'kid', 'country', 'tell', 'mean', 'smh', 'set', 'hell', 'many', 'employee', 'short', 'fix', 'home', 'work', 'take', 'shoe', 'coming', 'thought', 'hate', 'come', 'gear', 'hit', 'people', 'man', 'speak', 'team', 'wow', 'site', 'last', 'night', 'oh', 'going', 'right', 'fan', 'told', 'money', 'wrong', 'line', 'point', 'real', 'better', 'website', 'lot', 'year', 'abc', 'want', 'stop', 'join', 'community', 'tweet', 'hoodie', 'style', 'miss', 'saw', 'getting', 'morning', 'hope', 'talk', 'family', 'global', 'free', 'send', 'clothes', 'go', 'tomorrow', 'call', 'move', 'teamcanada', 'amazing', 'fam', 'share', 'world', 'today', 'high', 'bag', 'check', \"we're\", 'hour', 'experience', 'working', 'fire', 'others', 'change', 'present', 'moment', 'like', 'purchase', 'item', 'full', 'refund', 'le', 'minute', 'reason', 'woman', 'create', 'list', 'respect', 'wear', 'company', 'fact', 'first', 'clothing', 'wait', 'bit', 'gift', 'ship', 'next', 'excited', 'part', 'using', 'found', 'including', 'sell', 'sold', 'sent', 'nothing', 'anything', 'gonna', 'special', 'use', 'official', 'code', 'opportunity', 'different', 'news', 'action', 'mine', 'calling', 'name', 'comment', 'fast', 'hard', 'start', 'ok', 'online', 'waiting', 'someone', 'phone', 'trying', 'support', 'message', 'came', 'done', 'decided', 'email', 'went', 'end', 'human', 'price', 'sure', 'saying', 'china', 'bought', 'game', 'white', 'black', 'ready', 'word', 'clean', 'partnership', 'apparel', 'much', 'birthday', 'turn', 'heard', 'target', 'yesterday', 'credit', 'partner', 'color', 'cool', 'social', 'changing', 'missing', 'guess', 'starting', 'believe', 'type', 'opening', 'virtual', 'fun', 'helped', 'live', 'started', 'jumpman23', 'girl', 'ad', 'account', 'looking', 'nike', 'taking', 'number', 'damn', 'yo', 'model', \"that's\", 'canada', 'athlete', 'imagine', 'thinking', 'care', 'shame', 'report', 'buying', 'little', 'able', 'supporting', 'back', 'big', 'shit', 'open', 'puma', 'sport', 'proud', 'update', 'dropped', 'dollar', 'fedex', 'stand', 'luck', 'hi', 'continue', 'welcome', 'month', 'head', 'act', 'available', 'asking', 'pay', 'happen', 'launch', 'agree', 'making', 'photo', 'story', 'used', 'matter', 'giving', 'sorry', 'issue', 'coach', 'bad', 'link', 'side', 'state', 'as', 'quality', 'dear', 'least', 'true', 'city', 'understand', 'hand', 'stock', 'book', 'pick', 'fresh', 'face', 'incredible', 'dude', 'men', 'tried', 'respond', 'pic', 'drop', 'problem', 'yes', 'delivery', 'win', 'congrats', 'loved', 'watch', 'wtf', 'talking', 'dm', 'goal', 'marketing', 'program', 'post', 'worst', 'cause', 'hero', 'member', 'retail', 'sale', 'called', 'apple', 'future', 'dress', 'step', 'dropping', 'fucking', 'market', \"they're\", 'everything', 'crazy', 'bring', 'entire', 'york', 'project', 'bet', 'idea', 'nba', 'nfl', 'wnba', 'espn', 'netflix', 'amazon', 'mcdonalds', 'nft', 'logo', 'tech', 'street', 'design', 'wanted', 'second', 'left', 'public', 'grow', 'seeing', 'christmas', 'ball', 'seems', 'lost', 'plane', 'add', 'olympics', 'rest', 'tonight', \"can't\", 'final', 'whole', 'holiday', 'low', 'bro', 'walmart', 'red', 'meet', 'kit', 'gotta', 'happened', 'academy', 'generation']\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking number of nodes and edges in cleaned graph\n",
    "graph_summary_stats(G = Semantic_Graph_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7f290d",
   "metadata": {
    "papermill": {
     "duration": 0.038773,
     "end_time": "2024-06-10T15:50:34.825084",
     "exception": false,
     "start_time": "2024-06-10T15:50:34.786311",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "With the help of the cleaning function, we have reduced the total number of edges to roughly 40,000! This is a substantial reduction. This 'cleaned' graph will be used to create subgraphs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66180157",
   "metadata": {
    "papermill": {
     "duration": 0.039466,
     "end_time": "2024-06-10T15:50:34.903600",
     "exception": false,
     "start_time": "2024-06-10T15:50:34.864134",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"5d\"></a>\n",
    "## **Creating Subgraphs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "369046c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T15:50:34.986970Z",
     "iopub.status.busy": "2024-06-10T15:50:34.986084Z",
     "iopub.status.idle": "2024-06-10T15:50:35.023632Z",
     "shell.execute_reply": "2024-06-10T15:50:35.022344Z"
    },
    "papermill": {
     "duration": 0.082912,
     "end_time": "2024-06-10T15:50:35.026558",
     "exception": false,
     "start_time": "2024-06-10T15:50:34.943646",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating and defining node interactions and connections\n",
    "# Kudos to 'Chiuchiyin'! The following code chunk has been adapted from their work\n",
    "\n",
    "# Defining key nodes\n",
    "key_nodes_all = ['nike', 'lululemon', 'adidas']\n",
    "key_nodes_nl = ['nike', 'lululemon']\n",
    "key_nodes_na = ['nike', 'adidas']\n",
    "key_nodes_al = ['adidas', 'lululemon']\n",
    "\n",
    "# Finding neighbors of the key nodes by themselves\n",
    "neighbors_sets_n = set(nx.all_neighbors(Semantic_Graph_cleaned, 'nike'))\n",
    "neighbors_sets_a = set(nx.all_neighbors(Semantic_Graph_cleaned, 'adidas'))\n",
    "neighbors_sets_l = set(nx.all_neighbors(Semantic_Graph_cleaned, 'lululemon'))\n",
    "\n",
    "# Finding neighbors of the key nodes\n",
    "neighbors_sets_all = [set(nx.all_neighbors(Semantic_Graph_cleaned, node)) for node in key_nodes_all]\n",
    "neighbors_sets_nl = [set(nx.all_neighbors(Semantic_Graph_cleaned, node)) for node in key_nodes_nl]\n",
    "neighbors_sets_na = [set(nx.all_neighbors(Semantic_Graph_cleaned, node)) for node in key_nodes_na]\n",
    "neighbors_sets_al = [set(nx.all_neighbors(Semantic_Graph_cleaned, node)) for node in key_nodes_al]\n",
    "\n",
    "# Intersecting the sets to get nodes connected to all key nodes\n",
    "common_neighbors_all = set.intersection(*neighbors_sets_all)\n",
    "\n",
    "# Intersecting the sets to get nodes connected to only 2 key nodes\n",
    "common_neighbors_nl = set.intersection(*neighbors_sets_nl) - common_neighbors_all - set(key_nodes_al)\n",
    "common_neighbors_na = set.intersection(*neighbors_sets_na) - common_neighbors_all - set(key_nodes_nl)\n",
    "common_neighbors_al = set.intersection(*neighbors_sets_al) - common_neighbors_all - set(key_nodes_na)\n",
    "\n",
    "# Getting nodes connected to any one of the key nodes but not all of them\n",
    "union_neighbors = set.union(*neighbors_sets_all)\n",
    "\n",
    "# Getting nodes connected to only 1 brand\n",
    "exclusive_neighbors = (union_neighbors - common_neighbors_all\n",
    "                       - common_neighbors_nl - common_neighbors_na - common_neighbors_al)\n",
    "\n",
    "# Getting nodes connected to each specific brand\n",
    "exclusive_neighbors_n = neighbors_sets_n - common_neighbors_all - common_neighbors_nl - common_neighbors_na - set(key_nodes_al)\n",
    "exclusive_neighbors_a = neighbors_sets_a - common_neighbors_all - common_neighbors_na - common_neighbors_nl - set(key_nodes_nl)\n",
    "exclusive_neighbors_l = neighbors_sets_l - common_neighbors_all - common_neighbors_nl - common_neighbors_al - set(key_nodes_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fc00e716",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T15:50:35.105358Z",
     "iopub.status.busy": "2024-06-10T15:50:35.104910Z",
     "iopub.status.idle": "2024-06-10T15:50:35.868013Z",
     "shell.execute_reply": "2024-06-10T15:50:35.866750Z"
    },
    "papermill": {
     "duration": 0.805934,
     "end_time": "2024-06-10T15:50:35.870965",
     "exception": false,
     "start_time": "2024-06-10T15:50:35.065031",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating subgraphs\n",
    "\n",
    "# Creating subgraph of bridges between all brand nodes\n",
    "nodes_to_keep_all = list(common_neighbors_all) + key_nodes_all\n",
    "Semantic_Graph_bridge_all = focus_edges(Semantic_Graph_cleaned, weight_min=125).subgraph(nodes_to_keep_all)\n",
    "\n",
    "# Creating subgraph of bridges between Nike & Adidas\n",
    "nodes_to_keep_na = list(common_neighbors_na) + key_nodes_na\n",
    "Semantic_Graph_bridge_na = Semantic_Graph_cleaned.subgraph(nodes_to_keep_na)\n",
    "\n",
    "# Creating subgraph of bridges between Nike & Lululemon\n",
    "nodes_to_keep_nl = list(common_neighbors_nl) + key_nodes_nl\n",
    "Semantic_Graph_bridge_nl = Semantic_Graph_cleaned.subgraph(nodes_to_keep_nl)\n",
    "\n",
    "# Creating subgraph of bridges between Adidas & Lululemon\n",
    "nodes_to_keep_al = list(common_neighbors_al) + key_nodes_al\n",
    "Semantic_Graph_bridge_al = Semantic_Graph_cleaned.subgraph(nodes_to_keep_al)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "32c26049",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-06-10T15:50:35.950619Z",
     "iopub.status.busy": "2024-06-10T15:50:35.950200Z",
     "iopub.status.idle": "2024-06-10T15:50:36.489826Z",
     "shell.execute_reply": "2024-06-10T15:50:36.488355Z"
    },
    "papermill": {
     "duration": 0.583408,
     "end_time": "2024-06-10T15:50:36.493288",
     "exception": false,
     "start_time": "2024-06-10T15:50:35.909880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "##### Bridges Between All Brand Nodes #####\n",
      "number of nodes: 453\n",
      "number of edges: 1220\n",
      "\n",
      "nodes: ['continue', 'news', 'wish', 'fast', 'wear', 'mean', 'strong', 'fun', 'wrong', 'program', 'country', 'going', 'anything', 'tech', 'site', 'family', 'found', 'ask', 'model', 'sorry', 'worst', 'gotta', 'seen', 'target', 'partnership', 'stand', 'big', 'perfect', 'everyone', 'best', 'goal', 'go', 'photo', 'black', 'everything', 'collection', 'color', 'clothes', \"i'm\", 'congrats', 'super', 'ball', 'sold', 'stuff', 'entire', 'place', 'update', 'home', 'public', 'jumpman23', 'nfl', 'get', 'side', 'tweet', 'year', 'kid', 'problem', 'shirt', 'email', 'shoe', 'word', 'member', 'hell', 'keep', 'supporting', 'men', 'adidas', 'make', 'code', 'called', 'tried', 'present', 'running', 'school', 'gift', 'available', 'told', 'cool', 'number', 'saw', 'tonight', 'im', 'stay', 'final', 'package', 'hold', 'hero', 'fucking', 'fashion', 'come', 'including', 'check', 'hard', 'used', 'price', 'went', 'right', 'walmart', 'tell', 'worker', 'post', 'started', 'report', 'win', 'man', 'follow', 'fedex', 'nba', 'create', 'making', 'read', 'good', 'nothing', 'high', 'look', 'waiting', 'great', 'favorite', 'take', 'coach', 'future', 'least', 'fresh', 'rest', 'amazon', 'coming', 'dollar', 'whole', 'pick', 'purchase', 'launch', 'little', 'different', 'dude', 'wearing', 'hope', 'paid', 'new', 'miss', 'end', 'love', 'let', 'minute', 'decided', 'real', 'credit', 'bad', 'netflix', 'virtual', 'hoodie', 'loved', 'put', 'fan', 'company', 'shop', 'way', \"that's\", 'meet', 'mine', \"we're\", 'luck', 'mind', 'wait', 'bought', 'night', 'community', 'send', 'find', 'short', 'talking', 'seeing', 'sure', 'looking', 'trying', 'social', 'kind', 'damn', 'welcome', 'working', 'espn', 'thought', 'item', 'month', 'happened', 'able', 'person', 'apparel', 'world', 'season', 'apple', 'york', 'dm', 'yeah', 'white', 'order', 'space', 'something', 'holiday', 'lost', 'dress', 'appreciate', 'generation', 'gonna', 'yo', 'sign', 'hey', 'give', 'use', 'woman', 'made', 'list', 'happy', 'grow', 'return', 'link', 'shame', 'dropped', 'hear', 'size', 'like', 'amazing', 'video', 'human', 'asking', 'saying', 'opening', 'clothing', 'guy', 'got', 'talk', 'line', 'yesterday', 'free', 'many', 'better', 'chance', 'work', 'dear', 'pay', 'sell', 'act', 'someone', 'wow', 'delivery', 'friend', 'stock', 'others', 'day', 'le', 'customer', 'today', 'pic', 'kit', 'congratulation', 'ordered', 'shipping', 'employee', 'yes', 'fix', 'help', 'fact', 'agree', \"can't\", 'partner', 'back', 'nft', 'abc', 'market', 'one', 'seems', 'open', 'long', 'think', 'twitter', 'change', 'feel', 'early', \"they're\", 'clean', 'sent', 'came', 'set', 'said', 'move', 'red', 'athlete', 'online', 'nike', 'business', 'run', 'reason', 'call', 'experience', 'thanks', 'need', 'retail', 'hit', 'message', 'excited', 'birthday', 'puma', 'huge', 'imagine', 'shit', 'fire', 'point', 'marketing', 'believe', 'live', 'try', 'store', 'top', 'god', 'getting', 'giving', 'sport', 'care', \"i've\", 'phone', 'sponsor', 'respond', 'website', 'incredible', 'deal', 'underarmour', 'took', 'lot', 'book', 'awesome', 'design', 'street', 'using', 'fam', 'tomorrow', 'time', 'matter', 'received', 'full', 'old', 'special', 'lululemon', 'refund', 'card', 'hour', 'speak', 'job', 'moment', 'crazy', 'issue', 'first', 'sale', 'ship', 'thank', 'mcdonalds', 'life', 'missing', 'name', 'lol', 'delivered', 'share', 'ok', 'done', 'money', 'gear', 'cause', 'oh', 'idea', 'club', 'nice', 'hand', 'buy', 'opportunity', 'people', 'hate', 'china', 'taking', 'buying', 'understand', 'global', 'uniform', 'face', 'head', 'type', 'project', 'story', 'beautiful', 'bring', 'want', 'know', 'see', 'next', 'pair', 'bet', 'proud', 'game', 'academy', 'add', 'please', 'girl', 'ready', 'last', 'christmas', 'happen', 'product', 'official', 'quality', 'discount', 'city', 'question', 'thing', 'anyone', 'show', 'helped', 'left', 'true', 'wtf', 'as', 'pant', 'guess', 'team', 'thinking', 'supply', 'start', 'low', 'turn', 'watch', 'say', 'respect', 'sound', 'given', 'drop', 'pack', 'wanted', 'smh', 'brand', 'state', 'second', 'morning', 'support', 'stop', 'friday', 'style', 'much', 'join', 'logo', 'week', 'part', 'service', 'fit', 'ad', 'wnba', 'bro', 'account']\n",
      "\n",
      "neighbors of adidas: ['available', 'puma', 'partnership', 'nike', 'real', 'get', 'support', 'friend', 'family', 'look', 'wearing', 'say', 'drop', 'much', 'people', 'lol', 'deal', 'black', 'run', 'company', 'thank', 'great', 'team', 'getting', 'need', 'start', 'know', 'free', 'pair', 'month', 'please', 'next', 'long', 'love', 'new', 'year', 'thanks', 'service', 'shoe', 'keep', 'gonna', 'right', 'brand', 'make', 'sure', 'gear', 'see', 'got', 'one', 'good', 'day', 'top', 'trying', 'let', 'think', 'today', 'way', 'want', 'man', 'show', 'underarmour', 'awesome', 'first', 'size', 'check', 'kit', 'follow', 'amazing', 'help', 'life', 'fan', 'shirt', 'order', 'wait', 'online', 'work', 'looking', 'season', 'time', 'store', 'purchase', 'win', 'program', 'point', 'world', 'better', 'hey', 'going', 'big', 'many', 'design', \"i'm\", 'hope', 'take', 'find', 'white', 'tech', 'give', 'thinking', 'guy', 'card', 'collection', 'nice', 'style', 'hoodie', 'package', 'customer', 'money', 'nft', 'sport', 'happy', 'last', 'everyone', 'put', 'excited', 'coach', 'buy', 'stop', 'something', 'thing', 'made', 'ad', 'running', 'saw', 'walmart', 'stay', 'best', 'jumpman23', 'nfl', 'nba', \"can't\", 'wear', 'product', 'game', 'website', 'send', 'shit', 'crazy', 'cool', 'week', 'come', 'call', 'twitter', 'congrats', 'use', 'go', 'nothing', 'anything', 'went', 'amazon', 'number', 'sponsor', 'global', 'fedex', 'making', 'ball', 'news', 'community', 'including', 'sent', 'fashion', 'chance', 'christmas', 'respond', 'generation', 'opening', 'project']\n",
      "neighbors of nike: ['white', 'available', 'work', 'home', 'next', 'get', 'puma', 'stock', 'partnership', 'helped', 'grow', 'adidas', 'team', 'lot', 'wait', 'amazon', 'abc', 'business', 'bet', 'ad', 'beautiful', 'time', 'jumpman23', 'look', 'good', 'black', 'apple', 'think', 'people', 'care', 'underarmour', 'working', 'win', 'guy', 'see', 'way', 'keep', 'say', 'top', 'going', 'year', 'moment', 'show', 'understand', \"they're\", 'making', 'tweet', 'want', 'others', 'space', 'low', 'start', 'getting', 'birthday', 'item', 'thanks', 'given', 'something', 'trying', 'seen', 'issue', 'hand', 'go', 'shoe', 'part', 'support', 'turn', 'join', 'problem', 'new', 'coach', 'sponsor', 'use', 'company', 'package', 'delivery', 'put', 'left', 'le', 'minute', 'delivered', 'week', 'know', 'thing', \"i'm\", 'woman', 'day', 'god', 'waiting', 'red', 'let', 'order', 'congrats', 'customer', 'happy', 'partner', 'retail', 'second', 'season', 'yeah', 'bad', 'lol', 'store', 'ask', 'thought', 'real', 'line', 'york', 'game', 'man', 'many', 'pair', 'size', 'last', 'chance', 'said', 'sold', 'wow', 'club', 'shop', 'shirt', 'please', 'girl', 'point', 'night', 'love', 'today', 'job', 'make', 'money', 'virtual', 'take', 'link', 'right', 'great', 'sure', 'incredible', 'global', 'brand', 'need', 'city', 'stop', 'giving', 'buy', 'started', 'sport', 'story', 'check', 'thank', 'got', 'send', 'hoodie', 'favorite', 'color', 'product', 'model', 'rest', 'world', 'shipping', 'found', 'bring', 'back', 'christmas', 'uniform', 'report', 'high', 'price', 'best', 'ball', 'made', 'big', 'able', 'seeing', 'program', 'purchase', 'school', 'luck', 'street', 'athlete', 'name', 'much', 'welcome', 'holiday', \"can't\", 'number', 'gift', 'fam', 'quality', 'stand', 'mean', 'wrong', 'wish', 'type', 'come', 'cause', 'website', 'sell', 'run', 'fan', 'fedex', 'worst', 'help', 'fit', 'called', 'believe', 'collection', 'super', 'bro', 'lost', 'service', 'ship', 'walmart', 'refund', 'awesome', 'logo', 'pack', 'fun', 'true', 'watch', 'looking', 'give', 'dollar', 'buying', 'shit', 'fresh', 'tell', 'netflix', 'present', 'add', 'gonna', 'full', 'community', 'one', 'ordered', 'happen', 'nice', 'damn', 'paid', 'whole', 'appreciate', 'free', 'anyone', 'word', 'amazing', 'hell', 'mine', 'tried', 'marketing', 'month', 'done', 'long', 'im', 'clothes', 'nothing', 'missing', 'speak', 'little', 'fashion', 'sale', 'online', 'photo', 'follow', 'end', 'question', 'gear', 'nfl', 'first', 'place', 'seems', 'fire', 'yes', 'book', 'cool', 'saw', 'old', 'bought', 'talk', 'social', 'pay', 'hard', 'try', 'fix', 'wearing', 'anything', 'sent', 'hate', 'call', 'gotta', 'video', 'feel', 'deal', \"that's\", 'took', 'hit', 'least', \"we're\", 'life', 'share', 'design', 'hey', 'mcdonalds', 'dude', 'wear', 'went', 'yo', 'face', 'person', 'used', 'talking', 'better', 'reason', 'stuff', 'friday', 'set', 'coming', 'apparel', 'kid', 'excited', 'change', 'live', 'everyone', 'tomorrow', 'head', 'wanted', 'china', 'employee', 'told', 'crazy', 'came', 'clean', 'target', 'fucking', 'find', 'style', 'opportunity', 'running', 'state', 'fact', 'saying', 'nba', 'including', 'goal', 'someone', 'worker', 'move', 'final', 'dropped', 'country', 'hope', 'site', 'men', 'agree', 'future', 'different', 'launch', 'sign', 'twitter', 'family', 'taking', 'ok', 'post', 'sorry', 'as', 'phone', 'member', 'thinking', 'guess', 'list', 'using', 'hour', 'supporting', 'drop', 'yesterday', 'fast', 'morning', 'ready', 'idea', 'perfect', 'hear', 'kind', 'return', 'huge', 'dear', 'asking', 'friend', 'official', 'miss', 'stay', 'special', 'dm', 'experience', 'short', 'everything', 'act', \"i've\", 'like', 'continue', 'read', 'credit', 'oh', 'hold', 'card', 'message', 'congratulation', 'email', 'espn', 'wnba', 'open', 'project', 'pick', 'clothing', 'news', 'early', 'side', 'create', 'market', 'wtf', 'received', 'pic', 'tech', 'respect', 'update', 'smh', 'account', 'sound', 'mind', 'matter', 'happened', 'nft', 'supply', 'proud', 'human', 'entire', 'tonight', 'imagine', 'public', 'hero', 'generation']\n",
      "neighbors of lululemon: ['thank', 'great', 'thanks', 'please', 'make', 'brand', 'order', 'new', 'got', 'good', 'love', 'need', 'day', 'store', 'get', 'know', 'best', 'customer', 'service', 'see', 'time', 'pant', 'look', 'discount', 'worker', 'work', 'take', 'team', 'line', 'year', 'want', 'go', 'today', 'hour', 'company', 'first', 'excited', 'fast']\n",
      "----------------------------------------\n",
      "\n",
      "----------------------------------------\n",
      "##### Nike & Adidas Bridges #####\n",
      "number of nodes: 247\n",
      "number of edges: 2202\n",
      "\n",
      "nodes: ['facility', 'sneaker', 'inspired', 'celebrating', 'lazylionsnft', 'fly', 'baby', 'orange', 'pepsi', 'green', 'kicksonfire', 'rodgers', 'trash', 'student', 'eth', 'culture', 'shut', 'lied', 'bright', 'remember', 'grey', 'nikesb', 'nikeservice', 'slam', 'cocacola', 'dark', 'xbox', 'manager', 'solesavy', 'metaverse', 'travis', 'beyonce', 'holy', 'adidashoops', 'dick', 'corporation', 'kotd', 'baseball', 'deezle148', \"let's\", 'sizeofficial', 'potus', 'cop', 'web', 'senator', 'adidasus', 'newbalance', 'reebok', 'playing', 'fuck', 'digital', 'major', 'relationship', 'date', 'contract', 'jersey', 'history', 'mid', 'boot', 'candace_parker', 'impossible', 'entering', 'league', '_talkswithtj', 'college', 'jack', 'announced', 'solefed', 'chinese', 'navy', 'introducing', 'shot', 'box', 'yellow', 'listen', 'meta', 'edition', 'pixel', 'leader', 'enter', 'custom', 'http', 'young', 'olive', 'james', 'iamtmcii', 'steal', 'ya', 'blue', 'campaign', 'slave', 'leading', 'alert', 'king', 'dream', 'dope', 'wonder', 'retro', 'michael', 'jordan', 'eneskanter', 'adidasoriginals', 'air', 'example', 'wanna', 'tho', 'finishline', 'access', 'woke', 'nike', 'dunk', 'commercial', 'rule', 'university', 'em', 'blocked', 'nikestore', 'beiberlove69', 'la', 'safe', 'celebrate', 'total', 'fbi', 'actual', 'suck', 'selling', 'match', 'boardroom', 'course', 'adidas', 'content', 'instagram', 'richsignorelli', 'football', 'detail', 'kingjames', 'profit', 'force', 'snkrfrkrmag', 'boston', 'ebay', 'nov', 'december', 'sephora', 'aaron', 'footlocker', 'boost', 'american', 'soccer', 'roblox', 'medium', 'pink', 'anniversary', 'nikebasketball', 'player', 'royal', 'ice', 'taeyong', 'solelinks', 'original', 'topps', 'classic', 'prize', 'atmos_usa', 'yeezy', 'inspire', 'icon', 'lmao', 'rock', 'converse', 'rtfktstudios', 'giveaway', 'ht', 'jadendaly', 'chicago', \"women's\", 'gold', 'release', 'several', 'sneakerhead', 'yall', 'become', 'lexxdaturtle', 'november', 'play', 'sneakeradmirals', 'yoursneakersaredope', 'statefarm', 'snkr_twitr', 'court', 'passing', 'htt', 'gatorade', 'nfts', 'innovation', 'pathetic', 'snkrs', 'boycott', 'chose', 'foot', 'child', 'ig', 'latest', 'labor', 'signed', 'de', 'app', 'light', 'exclusive', 'image', 'kick', 'corporate', 'forum', 'jdsports', 'america', 'lasership', 'collaboration', 'draw', 'charger', 'rtfkt', 'giannis_an34', 'vp', 'basketball', 'boredapeyc', 'sock', 'collab', 'turtlepace5', 'grail', 'glad', \"he's\", 'lfc', 'son', 'kaya_alexander5', 'wild', 'ticket', 'raffle', 'triple', 'biggest', 'nicekicks', 'recent', 'boy', 'kanyewest', 'bot', 'stealing', 'sick', 'ups', 'owner']\n",
      "\n",
      "neighbors of adidas: ['nike', 'adidasus', 'adidasoriginals', 'young', 'jersey', 'history', 'play', 'forum', 'mid', 'bot', 'blue', 'light', 'sneaker', 'grey', \"he's\", 'snkr_twitr', 'adidashoops', 'glad', 'em', 'collab', 'icon', 'instagram', 'http', 'woke', 'boy', 'fly', 'court', 'retro', 'total', 'shot', 'original', 'example', 'kanyewest', 'boost', 'passing', 'commercial', 'wanna', 'wild', 'prize', 'college', 'several', 'sock', \"women's\", 'lasership', 'actual', 'cocacola', 'newbalance', 'converse', 'boston', 'dope', \"let's\", 'green', 'player', 'app', 'gold', 'collaboration', 'campaign', 'ice', 'rule', 'sneakerhead', 'release', 'football', 'nfts', 'kick', 'corporation', 'ups', 'sick', 'suck', 'announced', 'classic', 'giveaway', 'box', 'inspire', 'reebok', 'gatorade', 'kotd', 'yoursneakersaredope', 'safe', 'celebrate', 'impossible', 'remember', 'match', 'orange', 'exclusive', 'basketball', 'soccer', 'course', 'nikebasketball', 'enter', 'corporate', 'yeezy', 'steal', 'nikestore', 'owner', 'date', 'detail', 'ig', 'medium', 'blocked', 'edition', 'selling', 'foot', 'lmao', 'content', 'dream', 'pink', 'rock', 'november', 'footlocker', 'son', 'xbox', 'tho', 'alert', 'pepsi', 'recent', 'trash', 'become', 'innovation', 'boot', 'sneakeradmirals', 'chose', '_talkswithtj', 'james', 'vp', 'fuck', 'anniversary', 'ebay', 'celebrating', 'cop', 'inspired', 'aaron', 'la', 'king', 'ticket', 'signed', 'nicekicks', 'iamtmcii', 'beiberlove69', 'air', 'culture', 'baby', 'yall', 'jack', 'candace_parker', 'holy', 'america', 'jordan', 'force', 'american', 'michael', 'latest', 'sizeofficial', 'slave', 'labor', 'wonder', 'navy', 'royal', 'draw', 'child', 'playing', 'beyonce', 'entering', 'introducing', 'contract', 'baseball', 'dick', 'nov', 'solelinks', 'league', 'stealing', 'major', 'statefarm', 'student', 'snkrs', 'image', 'digital', 'dark', 'dunk', 'chicago', 'triple', 'relationship', 'ya', 'pathetic', 'leader', 'raffle', 'access', 'university', 'charger', 'shut', 'atmos_usa', 'leading', 'chinese', 'kicksonfire', 'solefed', 'kingjames', 'biggest', 'custom', 'taeyong', 'travis', 'roblox', 'bright', 'turtlepace5', 'grail', 'boycott', 'lexxdaturtle', 'deezle148', 'finishline', 'ht', 'slam', 'jdsports', 'manager', 'htt', 'eneskanter', 'nikesb', 'nikeservice', 'giannis_an34', 'solesavy', 'sephora', 'lfc', 'topps', 'profit', 'listen', 'boredapeyc', 'pixel', 'december', 'fbi', 'rodgers', 'yellow', 'meta', 'richsignorelli', 'lied', 'web', 'metaverse', 'eth', 'rtfktstudios', 'kaya_alexander5', 'facility', 'jadendaly', 'senator', 'potus', 'olive', 'lazylionsnft', 'de', 'boardroom', 'snkrfrkrmag', 'rtfkt']\n",
      "neighbors of nike: [\"women's\", 'air', 'yellow', 'footlocker', 'lasership', 'stealing', 'alert', 'collab', 'snkrs', 'nikebasketball', 'adidas', 'release', 'jordan', 'sick', 'instagram', 'ebay', 'jack', 'fbi', 'potus', 'vp', 'ht', 'dunk', 'sizeofficial', 'royal', 'finishline', 'sneakeradmirals', 'nicekicks', 'snkr_twitr', 'iamtmcii', 'corporation', 'latest', 'play', 'kingjames', 'edition', 'celebrating', 'facility', 'bright', 'grey', 'cocacola', 'steal', 'manager', 'force', 'pixel', 'wild', 'http', 'ice', 'soccer', 'lfc', 'match', 'sock', 'custom', 'jersey', 'remember', 'app', 'blue', 'htt', 'several', 'actual', 'snkrfrkrmag', 'atmos_usa', 'nikestore', 'gatorade', 'dick', 'detail', 'light', 'pink', 'medium', 'olive', 'travis', 'kick', 'total', 'classic', 'green', 'college', 'box', 'sneaker', 'boy', 'orange', 'corporate', 'become', 'pepsi', 'chinese', 'shut', 'woke', 'pathetic', 'university', 'image', 'rule', 'kaya_alexander5', 'converse', 'slave', 'labor', 'suck', 'innovation', 'beiberlove69', 'em', 'wonder', 'selling', 'triple', 'foot', 'profit', 'cop', 'content', 'dope', 'ups', 'fuck', 'american', 'campaign', 'draw', 'lexxdaturtle', 'charger', 'ig', 'baby', 'raffle', 'player', 'retro', 'child', 'icon', 'nikeservice', 'navy', 'nikesb', 'glad', 'gold', 'playing', 'yall', 'newbalance', 'trash', 'solefed', 'kicksonfire', 'son', 'anniversary', 'yoursneakersaredope', 'kotd', 'shot', 'dream', 'jadendaly', 'boycott', 'commercial', 'football', 'listen', 'michael', 'sneakerhead', 'young', 'original', 'exclusive', 'course', 'basketball', 'bot', 'chicago', 'xbox', 'tho', 'adidasus', 'december', 'example', 'dark', 'wanna', 'introducing', 'access', 'date', '_talkswithtj', 'major', 'holy', 'inspire', 'reebok', 'james', 'boston', 'student', 'safe', 'biggest', 'nfts', \"he's\", 'prize', 'la', 'inspired', \"let's\", 'court', 'contract', 'leading', 'league', 'ya', 'boot', 'america', 'jdsports', 'culture', 'mid', 'announced', 'owner', 'grail', 'solelinks', 'collaboration', 'adidasoriginals', 'giveaway', 'baseball', 'turtlepace5', 'celebrate', 'november', 'blocked', 'recent', 'lmao', 'signed', 'rock', 'boardroom', 'fly', 'history', 'enter', 'deezle148', 'digital', 'relationship', 'de', 'web', 'king', 'kanyewest', 'boost', 'passing', 'senator', 'entering', 'impossible', 'leader', 'candace_parker', 'solesavy', 'giannis_an34', 'ticket', 'metaverse', 'roblox', 'yeezy', 'adidashoops', 'slam', 'aaron', 'lied', 'nov', 'lazylionsnft', 'chose', 'sephora', 'eneskanter', 'taeyong', 'rtfktstudios', 'forum', 'boredapeyc', 'topps', 'meta', 'eth', 'statefarm', 'rodgers', 'richsignorelli', 'beyonce', 'rtfkt']\n",
      "----------------------------------------\n",
      "\n",
      "----------------------------------------\n",
      "##### Nike & Lululemon Bridges #####\n",
      "number of nodes: 7\n",
      "number of edges: 12\n",
      "\n",
      "nodes: ['nike', 'calling', 'olympics', 'plane', 'lululemon', 'canada', 'bill']\n",
      "\n",
      "neighbors of nike: ['bill', 'canada', 'calling', 'olympics', 'lululemon', 'plane']\n",
      "neighbors of lululemon: ['bill', 'calling', 'nike', 'canada', 'plane', 'olympics']\n",
      "----------------------------------------\n",
      "\n",
      "----------------------------------------\n",
      "##### Adidas & Lululemon Bridges #####\n",
      "number of nodes: 2\n",
      "number of edges: 0\n",
      "\n",
      "nodes: ['adidas', 'lululemon']\n",
      "\n",
      "neighbors of adidas: []\n",
      "neighbors of lululemon: []\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking the summaries of the new subgraphs\n",
    "graph_summary_stats(G = Semantic_Graph_bridge_all, title='Bridges Between All Brand Nodes')\n",
    "graph_summary_stats(G = Semantic_Graph_bridge_na, title='Nike & Adidas Bridges')\n",
    "graph_summary_stats(G = Semantic_Graph_bridge_nl, title='Nike & Lululemon Bridges')\n",
    "graph_summary_stats(G = Semantic_Graph_bridge_al, title='Adidas & Lululemon Bridges')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a742915d",
   "metadata": {
    "papermill": {
     "duration": 0.038879,
     "end_time": "2024-06-10T15:50:36.571813",
     "exception": false,
     "start_time": "2024-06-10T15:50:36.532934",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As with the mentions network graphs, we have created much smaller subgraphs. At a glance, these subgraphs are much less complex than the original. Again, some of these graphs will be analyzed and shown in the conclusion & analysis section. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17050be1",
   "metadata": {
    "papermill": {
     "duration": 0.039211,
     "end_time": "2024-06-10T15:50:36.650578",
     "exception": false,
     "start_time": "2024-06-10T15:50:36.611367",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"5e\"></a>\n",
    "## **Saving and Plotting the Graphs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a08e92cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-10T15:50:36.732343Z",
     "iopub.status.busy": "2024-06-10T15:50:36.731887Z",
     "iopub.status.idle": "2024-06-10T15:50:36.738240Z",
     "shell.execute_reply": "2024-06-10T15:50:36.736952Z"
    },
    "papermill": {
     "duration": 0.049807,
     "end_time": "2024-06-10T15:50:36.741073",
     "exception": false,
     "start_time": "2024-06-10T15:50:36.691266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This code chunk saves the graphs to the set Google Drive filepath\n",
    "\n",
    "# Saving and plotting the full graph\n",
    "# plot_graph(G = Semantic_Graph_cleaned, file_path='semantic_network', use_edge_weight = False, plot_size='large')\n",
    "\n",
    "# Saving and plotting the subgraphs\n",
    "# plot_graph(G = Semantic_Graph_bridge_all, file_path='semantic_network_bridge_all', use_edge_weight = False, plot_size='small')\n",
    "# plot_graph(G = Semantic_Graph_bridge_na, file_path='semantic_network_bridge_nike_adidas', use_edge_weight = False, plot_size='small')\n",
    "# plot_graph(G = Semantic_Graph_bridge_nl, file_path='semantic_network_bridge_nike_lululemon', use_edge_weight = False, plot_size='small')\n",
    "# plot_graph(G = Semantic_Graph_bridge_al, file_path='semantic_network_bridge_adidas_lululemon', use_edge_weight = False, plot_size='small')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49fba4f",
   "metadata": {
    "papermill": {
     "duration": 0.044576,
     "end_time": "2024-06-10T15:50:36.826812",
     "exception": false,
     "start_time": "2024-06-10T15:50:36.782236",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<a id=\"toc\"></a>\n",
    "\n",
    "<a href=\"#toc\" style=\"background-color:blue; color:white; padding: 7px 10px; text-decoration: none; border-radius: 50px;\">Back to top</a><a id=\"toc\"></a>\n",
    "\n",
    "<a id=\"toc\"></a>\n",
    "\n",
    "<a href=\"#r\" style=\"background-color:blue; color:white; padding: 7px 10px; text-decoration: none; border-radius: 50px;\">Next Section</a><a id=\"toc\"></a>\n",
    "<a id=\"6\"></a>\n",
    "\n",
    "---\n",
    "# **Conclusion/Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e73eee9",
   "metadata": {
    "papermill": {
     "duration": 0.046136,
     "end_time": "2024-06-10T15:50:36.919189",
     "exception": false,
     "start_time": "2024-06-10T15:50:36.873053",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"6a\"></a>\n",
    "## **Twitter Mentions Network**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16e969c",
   "metadata": {
    "papermill": {
     "duration": 0.039381,
     "end_time": "2024-06-10T15:50:37.000614",
     "exception": false,
     "start_time": "2024-06-10T15:50:36.961233",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Out of the 175,077 tweets in the dataset, there were 131,663 unique users. This implies that many of the users engaged at least once with one or more of the major brands. To filter down this large number of users, only users with 2 or more tweets and at least 100,000 followers were selected. This resulted in a group of only 198 users. This concentrated group of users represents the individuals who are most active and who have the largest following on Twitter. Overall, the reduction of users resulted in a user group of 0.15% of the initial population."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba51f18a",
   "metadata": {
    "papermill": {
     "duration": 0.040296,
     "end_time": "2024-06-10T15:50:37.080667",
     "exception": false,
     "start_time": "2024-06-10T15:50:37.040371",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "By pulling the userid's of the three major brands, we can gain quick insight into which brands have the most mention activity. Nike had the vast majority of interactions, with 120,125 mentions. Following this, Adidas had 36,654 and Lululemon had only 6,294. From this, one could definitely postulate that Nike appears to be the most popular brand amongst the Twitter users. Alternatively, Nike could be the most active/engaging on this particular social media platform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c1d52f",
   "metadata": {
    "papermill": {
     "duration": 0.040094,
     "end_time": "2024-06-10T15:50:37.161655",
     "exception": false,
     "start_time": "2024-06-10T15:50:37.121561",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "**Mentions Network: Bridge - All**\n",
    "\n",
    "![mentions_bridge_all](https://lh3.googleusercontent.com/drive-viewer/AKGpihbC1SBTjc6vlKqRAMxiF8ICkX6Hayq_2EERVE6x-qU0ig9f8XChttW--lLqZ_GoEE23CN3F9KqZBsvzh3OYZnv7G_r9_Tb8VFI=s1600-rw-v1)\n",
    "In the network graph above, we can see the users who interact with each of the three brands. Due to the fact that these users mention all three brands, they can be labeled as bridgers (or bridge users). These users are @deezefi, @wwd, and @uniwatch.\n",
    "\n",
    "The first user, @deezefi, goes by the name DeeZe. This user has multiple Twitter accounts, with the main account having over 250k followers. DeeZe appears to be a social media influencer, who is involved in art, podcasting, and NFTs. It is not immediately obvious as to why this user would be interacting with all three brands.\n",
    "\n",
    "The next user, @wwd (Women's Wear Daily), is a media and news company with over 2.7 million followers on Twitter. This company appears to involve fashion, and thus it is fairly obvious that they would interact with all three apparel brands.\n",
    "\n",
    "The last user, @uniwatch (Uni Watch), is another type of media company that has over 140k followers. Upon brief inspection, this company is a media group that discusses and documents sports fashion, i.e. uniforms and logos. As all three brands sell athletic apparel, it is clear as to why this particular user would be interacting with all brands."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206f6e3c",
   "metadata": {
    "papermill": {
     "duration": 0.042824,
     "end_time": "2024-06-10T15:50:37.244312",
     "exception": false,
     "start_time": "2024-06-10T15:50:37.201488",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "**Mentions Network: Bridge - Nike & Adidas**\n",
    "\n",
    "![mentions_network_bridge_nike_adidas.png](https://lh3.googleusercontent.com/drive-viewer/AKGpihYbmyRNWhKLMA7W3QZdTQQ2FTn-Y4c-DCSdBat5DJWorAnGvCginDhhVRQv_YjAHkeMSUKH0zkF3BXiDwLTw8v0OxKeSZcZvVA=s1600-rw-v1)\n",
    "The network graph above shows the connections between Nike and Adidas. While both brands are clearly the recipients of a lot of engagement, it appears as though Nike has more frequent mentions. This is represented by the thickness of the edges (lines) in the graph. We can also confirm this with prior knowledge, as an earlier portion of this project showed that Nike has the vast majority of mentions (120,125) compared to Adidas (36,654).\n",
    "\n",
    "Many of the connections between Nike and Adidas appear to be companies or organizations, such as @burgerking, @wnba, and @reebok. It is likely that these entities have some sort of partnership or sponsorship with the two brands. For example, it is likely that the WNBA has teams that are sponsored by either Nike or Adidas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e731d85",
   "metadata": {
    "papermill": {
     "duration": 0.039817,
     "end_time": "2024-06-10T15:50:37.323128",
     "exception": false,
     "start_time": "2024-06-10T15:50:37.283311",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "**Mentions Network: Bridge - Nike & Lululemon**\n",
    "\n",
    "![mentions_network_bridge_nike_lululemon.png](https://lh3.googleusercontent.com/drive-viewer/AKGpihYMhiga0O11MoOFd2FBY5kycOsU5pTOx66FMht9hb81DzR6TLczAwPAhAXlDxg7NDLZwtcoh-Vk3jg5_rGtJjoKjQs_OEDqbYc=s1600-rw-v1)\n",
    "The network graph above shows the bridges between Nike and Lululemon. At a glance, this graph has far fewer nodes and connections compared to the previous graph (Nike & Adidas). This may be explained by the fact that Lululemon had the fewest total mentions (6,294).\n",
    "\n",
    "Similar to before, many of the connecting users are companies/organizations, such as @brooksrunning and @khou (a Houston news station). Other connections appear to be individual users, such as @evankirstel and @realrclark25. The latter user, Ryan Clark, is a former NFL player and current sports analyst. Thus, it makes sense that his tweets would involve athletic apparel brands such as Nike and Lululemon. The former user, Evan Kirstel, is a tech influencer and content creator. As with @deezefi (a bridge user identified earlier), it is not immediately clear as to why this user is interacting with these brands. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f50a51",
   "metadata": {
    "papermill": {
     "duration": 0.039977,
     "end_time": "2024-06-10T15:50:37.402734",
     "exception": false,
     "start_time": "2024-06-10T15:50:37.362757",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "**Mentions Network: Bridge - Adidas & Lululemon**\n",
    "\n",
    "![mentions_network_bridge_adidas_lululemon.png](https://lh3.googleusercontent.com/drive-viewer/AKGpihY4bqM0xc0UsGWVfxrysYifkpxgSBAkJ5J1gchxJj3uBk1bBw1o7s-ZC-rzpi4srFV8QTBWCCuadGgNk98h-Msn4O5ENvGquMs=s1600-rw-v1)\n",
    "The last mentions network graph shows the connections between Adidas and Lululemon. This graph has only three connecting users, which may be explained by the fact that Adidas and Lululemon had the fewest total mentions.\n",
    "\n",
    "The three connecting users appear to be companies/organizations. The first, @iamwellandgood, is a media and news company focused on wellness. The second, @adweek, is another media and news company. And the third, @predsnhl, is a professional hockey team."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9206808",
   "metadata": {
    "papermill": {
     "duration": 0.040553,
     "end_time": "2024-06-10T15:50:37.483098",
     "exception": false,
     "start_time": "2024-06-10T15:50:37.442545",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"6b\"></a>\n",
    "## **Semantic Network**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b6678a",
   "metadata": {
    "papermill": {
     "duration": 0.038655,
     "end_time": "2024-06-10T15:50:37.561543",
     "exception": false,
     "start_time": "2024-06-10T15:50:37.522888",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Out of the 175,077 tweets in the dataset, there were 77,718 unique words. The top 10 most frequently used words were:\n",
    "- 'nike'\n",
    "- 'adidas'\n",
    "- 'sneakerscouts'\n",
    "- 'eneskanter'\n",
    "- 'xbox'\n",
    "- 'available'\n",
    "- 'day'\n",
    "- 'air'\n",
    "- 'china'\n",
    "- 'kingjames'\n",
    "\n",
    "For the sake of reducing the large number of unique words, only the words that appeared more than 250 times were included. This resulted in 908 unique words, which makes up only ~1.16% of the original number of unique words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb387586",
   "metadata": {
    "papermill": {
     "duration": 0.039325,
     "end_time": "2024-06-10T15:50:37.640650",
     "exception": false,
     "start_time": "2024-06-10T15:50:37.601325",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The full, initial graph contained 908 nodes (reflecting the number of unique words) and 207,401 edges. To reduce the impact on this notebook's RAM, a function was used to only include edges of a minimum weight. As a result, the cleaned semantic network graph contained only 39,967 edges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9041949a",
   "metadata": {
    "papermill": {
     "duration": 0.039515,
     "end_time": "2024-06-10T15:50:37.719461",
     "exception": false,
     "start_time": "2024-06-10T15:50:37.679946",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "**Semantic Network: Bridge - Nike & Adidas**\n",
    "\n",
    "![semantic_network_bridge_nike_adidas.png](https://lh3.googleusercontent.com/drive-viewer/AKGpihZePbBycXmSlc5v98oxtGQP_LlyFHGxNP60XHGOo9uV4leiTJKHZsAekJeW_55_cypit-_nGrmUUF-bG47yuHDDCuaZaTeMHw=s1600-rw-v1)\n",
    "The network graph above shows the words most commonly used in tweets involving both Nike and Adidas. I have selected to show this particular graph, as it is relatively easier to interpret when compared to the full semantic network graph.\n",
    "\n",
    "Here, we can see several interesting combinations of words. For example, the Nike node is most closely surrounded by multiple color words, such as 'blue', 'grey', 'orange', 'gold', and 'pink'. This suggests that many Twitter users are discussing the color of Nike products. These products are most likely shoes, as words like 'sneaker', 'snkrs', 'nicekicks', and 'sneakeradmirals' are also very close to the Nike node.\n",
    "\n",
    "Another intriguing pairing of words close to the Nike node is 'slave' and 'labor'. This suggests that many Twitter users are discussing the controversial employment practices and labor conditions that Nike has been exposed for.\n",
    "\n",
    "For the Adidas node, the closest words are 'mid', 'dope', 'retro', 'icon', 'access', 'exclusive', and 'instagram'. There is somewhat of a pattern here, as these words appear to reference the quality of Adidas products, i.e. with 'mid', 'dope', and 'retro'. Moreover, the combination of 'access' and 'exclusive' hints at the potential exclusivity of Adidas products."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8558a080",
   "metadata": {
    "papermill": {
     "duration": 0.04004,
     "end_time": "2024-06-10T15:50:37.799157",
     "exception": false,
     "start_time": "2024-06-10T15:50:37.759117",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "**Semantic Network: Bridge - Nike & Lululemon**\n",
    "\n",
    "![semantic_network_bridge_nike_lululemon.png](https://lh3.googleusercontent.com/drive-viewer/AKGpihZC64b79sRJY00_UsIMS4nV_iidk1PHaxlivRkevThXCxw72A6KYQYyUQHTvMB1Qs4N9c_P1SYim8QpP3UfDHZsnKvyG1B4buA=s1600-rw-v1)\n",
    "The network graph above shows the word commonalities between Nike and Lululemon. Immediately, we can see that there is a significant difference between this network and the previous one. In fact, this network shows only 5 common words used in tweets regarding both Nike and Lululemon.\n",
    "\n",
    "However, this network is not entirely useless. The words 'canada' and 'olympics' suggest that Canada's Olympic team may be sponsored by these two brands. In fact, a quick Google search shows that Lululemon is sponsoring the team through 2028."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e845105c",
   "metadata": {
    "papermill": {
     "duration": 0.040965,
     "end_time": "2024-06-10T15:50:37.880297",
     "exception": false,
     "start_time": "2024-06-10T15:50:37.839332",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Thank You!**\n",
    "\n",
    "Thank you for taking the time to view my notebook and project. If you enjoyed this or learned something new, please feel free to comment on or like the notebook! Furthermore, if you have any constructive criticism/feedback, please let me know! I am always eager to learn something new.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6031de29",
   "metadata": {
    "papermill": {
     "duration": 0.041136,
     "end_time": "2024-06-10T15:50:37.961131",
     "exception": false,
     "start_time": "2024-06-10T15:50:37.919995",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<a id=\"toc\"></a>\n",
    "\n",
    "<a href=\"#toc\" style=\"background-color:blue; color:white; padding: 7px 10px; text-decoration: none; border-radius: 50px;\">Back to top</a><a id=\"toc\"></a>\n",
    "\n",
    "<a id=\"toc\"></a>\n",
    "\n",
    "<a id=\"r\"></a>\n",
    "\n",
    "---\n",
    "# **References**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797e7292",
   "metadata": {
    "papermill": {
     "duration": 0.038569,
     "end_time": "2024-06-10T15:50:38.038741",
     "exception": false,
     "start_time": "2024-06-10T15:50:38.000172",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- https://drive.google.com/drive/folders/1rrRiAegl6A-P6BVP3oRLgpZQmyP8J-jR?usp=sharing (Original Work)\n",
    "- https://networkx.org/ (NetworkX)\n",
    "- https://developer.x.com/en/docs/twitter-api/v1/tweets/search/api-reference/get-search-tweets (Twitter: Standard search API)\n",
    "- https://github.com/Chiuchiyin/marketing-network-analysis/blob/main/marketing_network_analysis_with_twitter_data.ipynb (Reference to a code chunk by Chiuchiyin)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5185642,
     "sourceId": 8656158,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 834.873846,
   "end_time": "2024-06-10T15:50:39.406499",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-10T15:36:44.532653",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
